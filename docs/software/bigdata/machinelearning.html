<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>1.Overview | è®¡ç®—æœºåŸºç¡€æ•™ç¨‹</title>
    <meta name="description" content="è½¯ä»¶å¼€å‘æ•™ç¨‹ï¼Œç™½å¸½é»‘å®¢å…¥é—¨æ•™ç¨‹ï¼ŒåŒºå—é“¾å…¥é—¨æ•™ç¨‹ï¼Œç‰©è”ç½‘ï¼Œå¤§æ•°æ®">
    
    
    <link rel="preload" href="/docs/assets/css/0.styles.89d6372f.css" as="style"><link rel="preload" href="/docs/assets/js/app.62086776.js" as="script"><link rel="preload" href="/docs/assets/js/2.dc5756d7.js" as="script"><link rel="preload" href="/docs/assets/js/67.7ded8a09.js" as="script"><link rel="prefetch" href="/docs/assets/js/10.4d226ed6.js"><link rel="prefetch" href="/docs/assets/js/100.e30f4a25.js"><link rel="prefetch" href="/docs/assets/js/101.2ded5471.js"><link rel="prefetch" href="/docs/assets/js/102.fcb255ae.js"><link rel="prefetch" href="/docs/assets/js/103.2c9bbc47.js"><link rel="prefetch" href="/docs/assets/js/104.3bd6d6f1.js"><link rel="prefetch" href="/docs/assets/js/105.7edcbe4a.js"><link rel="prefetch" href="/docs/assets/js/106.a272932f.js"><link rel="prefetch" href="/docs/assets/js/107.d0584c6d.js"><link rel="prefetch" href="/docs/assets/js/108.00e7a88b.js"><link rel="prefetch" href="/docs/assets/js/109.069feab3.js"><link rel="prefetch" href="/docs/assets/js/11.7543d77a.js"><link rel="prefetch" href="/docs/assets/js/110.1584b92a.js"><link rel="prefetch" href="/docs/assets/js/111.82c1714f.js"><link rel="prefetch" href="/docs/assets/js/112.ec647b77.js"><link rel="prefetch" href="/docs/assets/js/113.d496dd59.js"><link rel="prefetch" href="/docs/assets/js/114.84c6cb54.js"><link rel="prefetch" href="/docs/assets/js/115.9f666400.js"><link rel="prefetch" href="/docs/assets/js/116.884aa941.js"><link rel="prefetch" href="/docs/assets/js/117.1b593fdc.js"><link rel="prefetch" href="/docs/assets/js/118.475f43ab.js"><link rel="prefetch" href="/docs/assets/js/119.4bd2df6e.js"><link rel="prefetch" href="/docs/assets/js/12.85900eee.js"><link rel="prefetch" href="/docs/assets/js/120.4206c2bf.js"><link rel="prefetch" href="/docs/assets/js/121.39ec161b.js"><link rel="prefetch" href="/docs/assets/js/122.128b1400.js"><link rel="prefetch" href="/docs/assets/js/123.c777127e.js"><link rel="prefetch" href="/docs/assets/js/124.585d96b5.js"><link rel="prefetch" href="/docs/assets/js/125.dc954237.js"><link rel="prefetch" href="/docs/assets/js/126.dd096bc5.js"><link rel="prefetch" href="/docs/assets/js/127.851a126f.js"><link rel="prefetch" href="/docs/assets/js/128.46b7a5b8.js"><link rel="prefetch" href="/docs/assets/js/129.aac4d132.js"><link rel="prefetch" href="/docs/assets/js/13.ddd25e6a.js"><link rel="prefetch" href="/docs/assets/js/130.c19476d5.js"><link rel="prefetch" href="/docs/assets/js/131.9c6861fc.js"><link rel="prefetch" href="/docs/assets/js/132.a19d6a43.js"><link rel="prefetch" href="/docs/assets/js/133.e21c151e.js"><link rel="prefetch" href="/docs/assets/js/134.0b08cde7.js"><link rel="prefetch" href="/docs/assets/js/135.94475558.js"><link rel="prefetch" href="/docs/assets/js/136.b808a519.js"><link rel="prefetch" href="/docs/assets/js/137.be0de4e5.js"><link rel="prefetch" href="/docs/assets/js/138.952e48a9.js"><link rel="prefetch" href="/docs/assets/js/139.27a1858e.js"><link rel="prefetch" href="/docs/assets/js/14.3619c6cd.js"><link rel="prefetch" href="/docs/assets/js/140.664e0cf3.js"><link rel="prefetch" href="/docs/assets/js/141.54b44bb0.js"><link rel="prefetch" href="/docs/assets/js/142.e4cd4ce3.js"><link rel="prefetch" href="/docs/assets/js/143.2320be36.js"><link rel="prefetch" href="/docs/assets/js/144.11b8fce5.js"><link rel="prefetch" href="/docs/assets/js/145.dbfe4ab3.js"><link rel="prefetch" href="/docs/assets/js/146.418153f4.js"><link rel="prefetch" href="/docs/assets/js/147.e9b1885d.js"><link rel="prefetch" href="/docs/assets/js/148.5b345881.js"><link rel="prefetch" href="/docs/assets/js/149.dcc9bde2.js"><link rel="prefetch" href="/docs/assets/js/15.8ab2e168.js"><link rel="prefetch" href="/docs/assets/js/150.12701ee4.js"><link rel="prefetch" href="/docs/assets/js/151.5f0b3247.js"><link rel="prefetch" href="/docs/assets/js/152.2664db99.js"><link rel="prefetch" href="/docs/assets/js/153.d4d105ac.js"><link rel="prefetch" href="/docs/assets/js/154.7bbc1cb9.js"><link rel="prefetch" href="/docs/assets/js/155.b6deab4a.js"><link rel="prefetch" href="/docs/assets/js/156.2bc76ba0.js"><link rel="prefetch" href="/docs/assets/js/157.2d5683e1.js"><link rel="prefetch" href="/docs/assets/js/158.c454ff79.js"><link rel="prefetch" href="/docs/assets/js/159.19f35230.js"><link rel="prefetch" href="/docs/assets/js/16.8e3e454a.js"><link rel="prefetch" href="/docs/assets/js/160.5c3012aa.js"><link rel="prefetch" href="/docs/assets/js/161.197c427c.js"><link rel="prefetch" href="/docs/assets/js/162.5d3f9194.js"><link rel="prefetch" href="/docs/assets/js/17.5603d183.js"><link rel="prefetch" href="/docs/assets/js/18.e8ad1c31.js"><link rel="prefetch" href="/docs/assets/js/19.cb7475da.js"><link rel="prefetch" href="/docs/assets/js/20.d2cc80df.js"><link rel="prefetch" href="/docs/assets/js/21.567748c4.js"><link rel="prefetch" href="/docs/assets/js/22.115ce153.js"><link rel="prefetch" href="/docs/assets/js/23.3aa7e609.js"><link rel="prefetch" href="/docs/assets/js/24.ead9c542.js"><link rel="prefetch" href="/docs/assets/js/25.0fd63cee.js"><link rel="prefetch" href="/docs/assets/js/26.c554e17c.js"><link rel="prefetch" href="/docs/assets/js/27.abe42d4e.js"><link rel="prefetch" href="/docs/assets/js/28.f6fe18b4.js"><link rel="prefetch" href="/docs/assets/js/29.a482a4c6.js"><link rel="prefetch" href="/docs/assets/js/3.b8cbfa33.js"><link rel="prefetch" href="/docs/assets/js/30.95126d00.js"><link rel="prefetch" href="/docs/assets/js/31.2df53cfb.js"><link rel="prefetch" href="/docs/assets/js/32.f556ecbd.js"><link rel="prefetch" href="/docs/assets/js/33.a863a884.js"><link rel="prefetch" href="/docs/assets/js/34.b48323ca.js"><link rel="prefetch" href="/docs/assets/js/35.34dcb847.js"><link rel="prefetch" href="/docs/assets/js/36.d952ffab.js"><link rel="prefetch" href="/docs/assets/js/37.18aeadea.js"><link rel="prefetch" href="/docs/assets/js/38.6fdae7cc.js"><link rel="prefetch" href="/docs/assets/js/39.ac58eab3.js"><link rel="prefetch" href="/docs/assets/js/4.7d1e56e0.js"><link rel="prefetch" href="/docs/assets/js/40.e12c2f06.js"><link rel="prefetch" href="/docs/assets/js/41.fdab0fba.js"><link rel="prefetch" href="/docs/assets/js/42.f49db8d6.js"><link rel="prefetch" href="/docs/assets/js/43.d9973e0a.js"><link rel="prefetch" href="/docs/assets/js/44.0f754825.js"><link rel="prefetch" href="/docs/assets/js/45.daefe8f5.js"><link rel="prefetch" href="/docs/assets/js/46.2623fe2d.js"><link rel="prefetch" href="/docs/assets/js/47.0f947901.js"><link rel="prefetch" href="/docs/assets/js/48.28528919.js"><link rel="prefetch" href="/docs/assets/js/49.51293cca.js"><link rel="prefetch" href="/docs/assets/js/5.c2840be0.js"><link rel="prefetch" href="/docs/assets/js/50.15c5b177.js"><link rel="prefetch" href="/docs/assets/js/51.4610e6fd.js"><link rel="prefetch" href="/docs/assets/js/52.4e010d38.js"><link rel="prefetch" href="/docs/assets/js/53.0008ea69.js"><link rel="prefetch" href="/docs/assets/js/54.3c17f552.js"><link rel="prefetch" href="/docs/assets/js/55.ba33519b.js"><link rel="prefetch" href="/docs/assets/js/56.cf5ad342.js"><link rel="prefetch" href="/docs/assets/js/57.ee67e86b.js"><link rel="prefetch" href="/docs/assets/js/58.e8d7c0db.js"><link rel="prefetch" href="/docs/assets/js/59.0b2a7cab.js"><link rel="prefetch" href="/docs/assets/js/6.f59a9763.js"><link rel="prefetch" href="/docs/assets/js/60.b17e5029.js"><link rel="prefetch" href="/docs/assets/js/61.d30c4cab.js"><link rel="prefetch" href="/docs/assets/js/62.2c2f9f41.js"><link rel="prefetch" href="/docs/assets/js/63.6acfe33c.js"><link rel="prefetch" href="/docs/assets/js/64.8919994b.js"><link rel="prefetch" href="/docs/assets/js/65.69c8e4b6.js"><link rel="prefetch" href="/docs/assets/js/66.82611c2f.js"><link rel="prefetch" href="/docs/assets/js/68.fb25792a.js"><link rel="prefetch" href="/docs/assets/js/69.c2e43917.js"><link rel="prefetch" href="/docs/assets/js/7.0c0e690a.js"><link rel="prefetch" href="/docs/assets/js/70.6febe695.js"><link rel="prefetch" href="/docs/assets/js/71.552faae1.js"><link rel="prefetch" href="/docs/assets/js/72.ff978c9d.js"><link rel="prefetch" href="/docs/assets/js/73.7c90f6a0.js"><link rel="prefetch" href="/docs/assets/js/74.06d795a9.js"><link rel="prefetch" href="/docs/assets/js/75.ecd90803.js"><link rel="prefetch" href="/docs/assets/js/76.6b0dc92a.js"><link rel="prefetch" href="/docs/assets/js/77.d03bd56c.js"><link rel="prefetch" href="/docs/assets/js/78.28dfee4f.js"><link rel="prefetch" href="/docs/assets/js/79.c2df039a.js"><link rel="prefetch" href="/docs/assets/js/8.57e9ad53.js"><link rel="prefetch" href="/docs/assets/js/80.af9e69f3.js"><link rel="prefetch" href="/docs/assets/js/81.2ed688fd.js"><link rel="prefetch" href="/docs/assets/js/82.1475de48.js"><link rel="prefetch" href="/docs/assets/js/83.9a1c8bab.js"><link rel="prefetch" href="/docs/assets/js/84.867e40fb.js"><link rel="prefetch" href="/docs/assets/js/85.fe009ed1.js"><link rel="prefetch" href="/docs/assets/js/86.25eb3c3b.js"><link rel="prefetch" href="/docs/assets/js/87.adcd0c58.js"><link rel="prefetch" href="/docs/assets/js/88.7a258334.js"><link rel="prefetch" href="/docs/assets/js/89.6cb359c4.js"><link rel="prefetch" href="/docs/assets/js/9.e602ccff.js"><link rel="prefetch" href="/docs/assets/js/90.2861a17c.js"><link rel="prefetch" href="/docs/assets/js/91.e9d0f202.js"><link rel="prefetch" href="/docs/assets/js/92.3fc2b4b6.js"><link rel="prefetch" href="/docs/assets/js/93.dbc2798c.js"><link rel="prefetch" href="/docs/assets/js/94.c894326d.js"><link rel="prefetch" href="/docs/assets/js/95.cd126106.js"><link rel="prefetch" href="/docs/assets/js/96.4719ac8b.js"><link rel="prefetch" href="/docs/assets/js/97.a4a65d5b.js"><link rel="prefetch" href="/docs/assets/js/98.399b09f2.js"><link rel="prefetch" href="/docs/assets/js/99.2c1db3f9.js">
    <link rel="stylesheet" href="/docs/assets/css/0.styles.89d6372f.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/docs/" class="home-link router-link-active"><!----> <span class="site-name">è®¡ç®—æœºåŸºç¡€æ•™ç¨‹</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/docs/" class="nav-link">æœºå™¨æŒ‡ä»¤</a></div><div class="nav-item"><a href="/docs/software/" class="nav-link router-link-active">è½¯ä»¶åŸºç¡€</a></div><div class="nav-item"><a href="/docs/coder2hacker/" class="nav-link">ç™½å¸½é»‘å®¢</a></div><div class="nav-item"><a href="/docs/blockchain/" class="nav-link">åŒºå—é“¾</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/docs/" class="nav-link">æœºå™¨æŒ‡ä»¤</a></div><div class="nav-item"><a href="/docs/software/" class="nav-link router-link-active">è½¯ä»¶åŸºç¡€</a></div><div class="nav-item"><a href="/docs/coder2hacker/" class="nav-link">ç™½å¸½é»‘å®¢</a></div><div class="nav-item"><a href="/docs/blockchain/" class="nav-link">åŒºå—é“¾</a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/docs/software/" class="sidebar-link">è½¯ä»¶åŸºç¡€</a></li><li><a href="/docs/coder2hacker/" class="sidebar-link">é»‘å®¢å…¥é—¨</a></li><li><a href="/docs/blockchain/" class="sidebar-link">åŒºå—é“¾å…¥é—¨</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="_1-overview"><a href="#_1-overview" class="header-anchor">#</a> 1.Overview</h2> <p>Supervise training(give the right answer first-training set examples)
Unsupervise training
Asymptotes
Superscript</p> <p>Regression problem
Classification problem</p> <h2 id="_2-linear-regression"><a href="#_2-linear-regression" class="header-anchor">#</a> 2.Linear regression</h2> <p>æœ€å°äºŒä¹˜æ³•ï¼ˆleast squares methodï¼‰
https://www.youtube.com/watch?v=MC7l96tW8V8
æœ€å°äºŒä¹˜æ³•ï¼ˆLeast Squareï¼‰å’Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡
Least squares cost function
æœ€å°äºŒä¹˜æ³•çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ https://www.zhihu.com/question/37031188
åœ¨è¿›è¡Œçº¿æ€§å›å½’æ—¶ï¼Œä¸ºä»€ä¹ˆæœ€å°äºŒä¹˜æ³•æ˜¯æœ€ä¼˜æ–¹æ³• https://www.zhihu.com/question/24095027</p> <p>ä»€ä¹ˆæ—¶å€™æœ€å°äºŒä¹˜å‚æ•°ä¼°è®¡å’Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡ç»“æœç›¸åŒï¼Ÿ
https://www.jiqizhixin.com/articles/2018-01-09-6
æœ€å°äºŒä¹˜æ³•æ˜¯å¦ä¸€ç§å¸¸ç”¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹å‚æ•°ä¼°è®¡æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå½“æ¨¡å‹å‘ä¸Šè¿°ä¾‹å­ä¸­ä¸€æ ·è¢«å‡è®¾ä¸ºé«˜æ–¯åˆ†å¸ƒæ—¶ï¼ŒMLE çš„ä¼°è®¡ç­‰ä»·äºæœ€å°äºŒä¹˜æ³•ã€‚
ç›´è§‰ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç†è§£ä¸¤ç§æ–¹æ³•çš„ç›®çš„æ¥è§£é‡Šè¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„è”ç³»ã€‚å¯¹äºæœ€å°äºŒä¹˜å‚æ•°ä¼°è®¡ï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾åˆ°æœ€å°åŒ–æ•°æ®ç‚¹å’Œå›å½’çº¿ä¹‹é—´è·ç¦»å¹³æ–¹ä¹‹å’Œçš„ç›´çº¿ï¼ˆè§ä¸‹å›¾ï¼‰ã€‚åœ¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–æ•°æ®åŒæ—¶å‡ºç°çš„æ€»æ¦‚ç‡ã€‚å½“å¾…æ±‚åˆ†å¸ƒè¢«å‡è®¾ä¸ºé«˜æ–¯åˆ†å¸ƒæ—¶ï¼Œæœ€å¤§æ¦‚ç‡ä¼šåœ¨æ•°æ®ç‚¹æ¥è¿‘å¹³å‡å€¼æ—¶æ‰¾åˆ°ã€‚ç”±äºé«˜æ–¯åˆ†å¸ƒæ˜¯å¯¹ç§°çš„ï¼Œè¿™ç­‰ä»·äºæœ€å°åŒ–æ•°æ®ç‚¹ä¸å¹³å‡å€¼ä¹‹é—´çš„è·ç¦»ã€‚</p> <h3 id="_2-1-univariate-linear-regression-linear-regression-with-one-variable"><a href="#_2-1-univariate-linear-regression-linear-regression-with-one-variable" class="header-anchor">#</a> 2.1 Univariate linear regression / Linear regression with one variable</h3> <p>ä¸¤ç§æ–¹æ³•æ±‚ç³»æ•°</p> <p>1.ç›´æ¥æ•°å­¦æŠ€å·§</p> <p>âˆ‘y = na + bâˆ‘x</p> <p>âˆ‘xy = âˆ‘xa + bâˆ‘xÂ²</p> <p>=&gt;</p> <p>b = nâˆ‘xy â€“ (âˆ‘x)(âˆ‘y)   nâˆ‘xÂ² â€“ (âˆ‘x)Â²</p> <p>a = âˆ‘y â€“ bâˆ‘x   n</p> <p>https://www.accountingverse.com/managerial-accounting/cost-behavior/least-squares-method.html</p> <p>å˜å½¢/å½¢å¼å˜æ¢
https://blog.csdn.net/u011026329/article/details/79183114</p> <p><img src="/docs/docs_image/software/bigdata/machinelearning01.png" alt=""></p> <p>2.æœ€å°äºŒä¹˜æ³•æ±‚å¯¼</p> <p><img src="/docs/docs_image/software/bigdata/machinelearning02.png" alt=""></p> <h3 id="_2-2-linear-regression-with-multiple-variable"><a href="#_2-2-linear-regression-with-multiple-variable" class="header-anchor">#</a> 2.2 Linear regression with multiple variable</h3> <p>å› ä¸ºç³»æ•°è¿‡å¤šæ— æ³•åƒä¸Šé¢ç›´æ¥æ±‚è§£ï¼Œå¦å¤–å¦‚æœç›´æ¥æ±‚è§£æ— æ³•ç»§ç»­ç”¨regularlizationæ¥æƒ©ç½šç³»æ•°ä»è€Œæ§åˆ¶overfittingï¼Œæ‰€ä»¥ç›´æ¥æ ¹æ®æœ€å°äºŒä¹˜æ³•æ„é€ cost function
<img src="/docs/docs_image/software/bigdata/machinelearning03.png" alt=""></p> <p>http://www.fanyeong.com/2017/03/29/machine-learning-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88linear-regression%EF%BC%89/
ä¸Šé¢å›¾ä¸­æåˆ°çš„é™¤mæ›´å¥½æ˜¯ç”¨<s>å¤§æ•°å®šç†</s>ä¸­å¿ƒæé™å®šç†æ¥è§£é‡Šï¼Œå³å½“æ ·æœ¬è¶³å¤Ÿå¤§ï¼Œå‡å€¼ç­‰äºæ•°å­¦æœŸæœ›ï¼ˆç®—æœ¯å¹³å‡å€¼ï¼‰ï¼Œæ­¤å¤„å³è¯¯å·®çš„å‡å€¼ç­‰äºæ•°å­¦æœŸæœ›
<img src="/docs/docs_image/software/bigdata/machinelearning04.png" alt="">
https://stats.stackexchange.com/questions/155580/cost-function-in-ols-linear-regression</p> <p>æ¢¯åº¦ä¸‹é™
<img src="/docs/docs_image/software/bigdata/machinelearning05.png" alt=""></p> <h2 id="_3-logistic-regression"><a href="#_3-logistic-regression" class="header-anchor">#</a> 3. Logistic regression</h2> <p>Sigmoid function / logistic function
ä¸ºä»€ä¹ˆé‡‡ç”¨sigmoid functionï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ä½œä¸ºcost function
è·Ÿ æœ€å¤§ä¼¼ç„¶ä¼°è®¡æœ‰å…³
http://www.hanlongfei.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/08/05/mle/</p> <p><img src="/docs/docs_image/software/bigdata/machinelearning06.png" alt=""></p> <p>regularization
<img src="/docs/docs_image/software/bigdata/machinelearning07.png" alt=""></p> <h2 id="quiz"><a href="#quiz" class="header-anchor">#</a> Quiz</h2> <p>Linear Regression with One Variable</p> <ol><li><p>Consider the problem of predicting how well a student does in her second year of college/university, given how well she did in her first year.
Specifically, let x be equal to the number of &quot;A&quot; grades (including A-. A and A+ grades) that a student receives in their first year of college (freshmen year). We would like to predict the value of y, which we define as the number of &quot;A&quot; grades they get in their second year (sophomore year).
Here each row is one training example. Recall that in linear regression, our hypothesis is hÎ¸(x)=Î¸0+Î¸1x, and we use m to denote the number of training examples.
| x | y |
|---|---|
| 5 | 4 |
| 3 | 4 |
| 0 | 1 |
| 4 | 3 |
For the training set given above (note that this training set may also be referenced in other questions in this quiz), what is the value of m? In the box below, please enter your answer (which should be a number between 0 and 10).</p></li> <li><p>For this question, assume that we are
using the training set from Q1. Recall our definition of the
cost function was J(Î¸0,Î¸1)=12mâˆ‘mi=1(hÎ¸(x(i))âˆ’y(i))2.
What is J(0,1)? In the box below,
please enter your answer (Simplify fractions to decimals when entering answer, and '.' as the decimal delimiter e.g., 1.5).</p></li> <li><p>Suppose we set Î¸0=âˆ’1,Î¸1=2 in the linear regression hypothesis from Q1. What is hÎ¸(6)?</p></li> <li><p>Let f be some function so that
f(Î¸0,Î¸1) outputs a number. For this problem,
f is some arbitrary/unknown smooth function (not necessarily the
cost function of linear regression, so f may have local optima).
Suppose we use gradient descent to try to minimize f(Î¸0,Î¸1)
as a function of Î¸0 and Î¸1. Which of the
following statements are true? (Check all that apply.)
If Î¸0 and Î¸1 are initialized so that Î¸0=Î¸1, then by symmetry (because we do simultaneous updates to the two parameters), after one iteration of gradient descent, we will still have Î¸0=Î¸1.
If the learning rate is too small, then gradient descent may take a very long
time to converge.
If Î¸0 and Î¸1 are initialized at
a local minimum, then one iteration will not change their values.
Even if the learning rate Î± is very large, every iteration of
gradient descent will decrease the value of f(Î¸0,Î¸1).</p></li> <li><p>Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some Î¸0, Î¸1 such that J(Î¸0,Î¸1)=0.
Which of the statements below must then be true? (Check all that apply.)
Our training set can be fit perfectly by a straight line,
i.e., all of our training examples lie perfectly on some straight line.
For this to be true, we must have y(i)=0 for every value of i=1,2,â€¦,m.
For this to be true, we must have Î¸0=0 and Î¸1=0
so that hÎ¸(x)=0
Gradient descent is likely to get stuck at a local minimum and fail to find the global minimum.</p></li> <li><p>Many substances that can burn (such as gasoline and alcohol) have a chemical structure based on carbon atoms; for this reason they are called hydrocarbons. A chemist wants to understand how the number of carbon atoms in a molecule affects how much energy is released when that molecule combusts (meaning that it is burned). The chemist obtains the dataset below. In the column on the right, â€œkJ/molâ€ is the unit measuring the amount of energy released.
| Name of molecule | Number of hydrocarbons in molecule(x) | Heat release when burned(kJ/mol)(y)|
|---|---|---|
| methane | 1 | -890 |
| ethene | 2 | -1411 |
| ethane | 2 | -1560 |
| propane | 3 | -2220 |
| cyclopropane | 3 | -2091 |
| butane | 4 | -2878 |
| pentane | 5 | -3537 |
| benzene | 6 | -3268 |
| cycloexane | 6 | -3920 |
| hexane | 6 | -4163 |
| octane | 8 | -5471 |
| napthalene | 10 | -5157 |</p></li></ol> <p>You would like to use linear regression (hÎ¸(x)=Î¸0+Î¸1x) to estimate the amount of energy released (y) as a function of the number of carbon atoms (x). Which of the following do you think will be the values you obtain for Î¸0 and Î¸1? You should be able to select the right answer without actually implementing linear regression.</p> <p>Linear Regression with Multiple Variables</p> <ol><li>Suppose m=4 students have taken some class, and the class had a midterm exam and a final exam. You have collected a dataset of their scores on the two exams, which is as follows:</li></ol> <table><thead><tr><th>midterm exam</th> <th>(midterm exam)2</th> <th>final exam</th></tr></thead> <tbody><tr><td>89</td> <td>7921</td> <td>96</td></tr> <tr><td>72</td> <td>5184</td> <td>74</td></tr> <tr><td>94</td> <td>8836</td> <td>87</td></tr> <tr><td>69</td> <td>4761</td> <td>78</td></tr></tbody></table> <p>You'd like to use polynomial regression to predict a student's final exam score from their midterm exam score. Concretely, suppose you want to fit a model of the form hÎ¸(x)=Î¸0+Î¸1x1+Î¸2x2, where x1 is the midterm score and x2 is (midterm score)2. Further, you plan to use both feature scaling (dividing by the &quot;max-min&quot;, or range, of a feature) and mean normalization.What is the normalized feature x(3)1? (Hint: midterm = 94, final = 87 is training example 3.) Please round off your answer to two decimal places and enter in the text box below.</p> <ol start="2"><li><p>You run gradient descent for 15 iterationsï¼Œwith Î±=0.3 and compute,J(Î¸) after each iteration. You find that the value of J(Î¸) decreases slowly and is still decreasing after 15 iterations. Based on this, which of the following conclusions seems most plausible? Rather than use the current value of Î±, it'd be more promising to try a smaller value of Î± (say Î±=0.1). Î±=0.3 is an effective choice of learning rate.Rather than use the current value of Î±, it'd be more promising to try a larger value of Î± (say Î±=1.0).</p></li> <li><p>Suppose you have m=23 training examples with n=5 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is Î¸=(XTX)âˆ’1XTy. For the given values of m and n, what are the dimensions of Î¸, X, and y in this equation?
X is 23Ã—6, y is 23Ã—6, Î¸ is 6Ã—6
X is 23Ã—5, y is 23Ã—1, Î¸ is 5Ã—1
X is 23Ã—6, y is 23Ã—1, Î¸ is 6Ã—1
X is 23Ã—5, y is 23Ã—1, Î¸ is 5Ã—5</p></li> <li><p>Suppose you have a dataset with m=50 examples and n=15 features for each example. You want to use multivariate linear regression to fit the parameters Î¸ to our data. Should you prefer gradient descent or the normal equation? Gradient descent, since it will always converge to the optimal Î¸. The normal equation, since it provides an efficient way to directly find the solution. Gradient descent, since (XTX)âˆ’1 will be very slow to compute in the normal equation. The normal equation, since gradient descent might be unable to find the optimal Î¸.</p></li> <li><p>Which of the following are reasons for using feature scaling?
It speeds up gradient descent by making each iteration of gradient descent less expensive to compute.
It speeds up gradient descent by making it require fewer iterations to get to a good solution.
It prevents the matrix XTX (used in the normal equation) from being non-invertable (singular/degenerate).
It is necessary to prevent the normal equation from getting stuck in local optima.</p></li></ol> <p>æœºå™¨å­¦ä¹ é”™é¢˜é›†</p> <ol><li><p>Some of the problems below are best addressed using a supervised learning algorithm, and the others with an unsupervised learning algorithm. Which of the following would you apply supervised learning to? (Select all that apply.) In each case,assume some appropriate dataset is available for your algorithm to learn from. ã€A,Cã€‘
A. Given historical data of childrens' ages and heights, predict children's height as a function of their age.
ã€è§£æã€‘This is a supervised learning, regression problem, where we can learn from a training set to predict height.
B.  Examine a large collection of emails that are known to be spam email, to discover if there are sub-types of spam mail.
ã€è§£æã€‘This can addressed using a clustering (unsupervised learning) algorithm, to cluster spam mail into sub-types.
C.  Examine the statistics of two football teams, and predicting which team will win tomorrow's match (given historical data of teams' wins/losses to learn from).
ã€è§£æã€‘This can be addressed using supervised learning, in which we learn from historical records to make win/loss predictions.
D.  Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatements.
ã€è§£æã€‘This can be addressed using an unsupervised learning, clustering, algorithm, in which we group patients into different clusters.</p></li> <li><p>Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some Î¸0, Î¸1 such that J(Î¸0,Î¸1)=0. Which of the statements below must then be true?  ã€Aã€‘
A. For these values of Î¸0 and Î¸1 that satisfy J(Î¸0,Î¸1)=0, we have that hÎ¸(x(i))=y(i) for every training example (x(i),y(i))
ã€è§£æã€‘J(Î¸0,Î¸1)=0, that means the line defined by the equation &quot;y=Î¸0+Î¸1x&quot; perfectly fits all of our data.
B. For this to be true, we must have y(i)=0 for every value of i=1,2,â€¦,m.
ã€è§£æã€‘So long as all of our training examples lie on a straight line, we will be able to find Î¸0 and Î¸1 so that J(Î¸0,Î¸1)=0. It is not necessary that y(i)=0 for all of our examples.
C. Gradient descent is likely to get stuck at a local minimum and fail to find the global minimum.
ã€è§£æã€‘The cost function J(Î¸0,Î¸1) for linear regression has no local optima (other than the global minimum), so gradient descent will not get stuck at a bad local minimum.
D.We can perfectly predict the value of y even for new examples that we have not yet seen. (e.g., we can perfectly predict prices of even new houses that we have not yet seen.)
ã€è§£æã€‘Even though we can fit our training set perfectly, this does not mean that we'll always make perfect predictions on houses in the future/on houses that we have not yet seen.</p></li> <li><p>Which of the following are reasons for using feature scaling?
It speeds up gradient descent by making it require fewer iterations to get to a good solution.
ã€è§£æã€‘Feature scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features take on much larger values than the rest.
The cost function J(Î¸) for linear regression has no local optima.
The magnitude of the feature values are insignificant in terms of computational cost.</p></li> <li><p>You run gradient descent for 15 iterations with Î±=0.3 and compute J(Î¸) aftereach iteration. You find that the value of J(Î¸) decreases quickly then levels off. Based on this, which of the following conclusions seems most plausible?
A smaller learning rate will only decrease the rate of convergence to the cost function's minimum, thus increasing the number of iterations needed.</p></li> <li><p>You are training a classification model with logistic regression. Which of the following statements are true? Check all that apply.ã€Dã€‘
A. Introducing regularization to the model always results in equal or better performance on the training set.
ã€è§£æã€‘If we introduce too much regularization, we can underfit the training set and have worse performance on the training set.
B.Adding many new features to the model helps prevent overfitting on the training set.
ã€è§£æã€‘Adding many new features gives us more expressive models which are able to better fit our training set. If too many new features are added, this can lead to overfitting of the training set.
C. Adding a new feature to the model always results in equal or better performance on examples not in the training set.
ã€è§£æã€‘Adding  more features might result in a model that overfits the training set, and thus can lead to worse performs for examples which are not in the training set.
D.Adding a new feature to the model always results in equal or better performance on the training set.
ã€è§£æã€‘By adding a new feature, our model must be more (or just as) expressive, thus allowing it learn more complex hypotheses to fit the training set.</p></li> <li><p>Which of the following statements about regularization are true? Check all that apply.ã€Dã€‘
A.Because regularization causes J(Î¸) to no longer be convex, gradient descent may not always converge to the global minimum (when Î»&gt;0, and when using an appropriate learning rate Î±).
ã€è§£æã€‘Regularized logistic regression and regularized linear regression are both convex, and thus gradient descent will still converge to the global minimum.
B.Using too large a value of Î» can cause your hypothesis to overfit the data; this can be avoided by reducing Î».
ã€è§£æã€‘Using a very large value of Î» can lead to underfitting of the training set.
C.Because logistic regression outputs values 0â‰¤hÎ¸(x)â‰¤1, it's range of output values can only be &quot;shrunk&quot; slightly by regularization anyway, so regularization is generally not helpful for it.
ã€è§£æã€‘Regularization affects the parameters Î¸ and is also helpful for logistic regression.
D.Consider a classification problem. Adding regularization may cause your classifier to incorrectly classify some training examples (which it had correctly classified when not using regularization, i.e. when Î»=0).
ã€è§£æã€‘Regularization penalizes complex models (with large values of Î¸).They can lead to a simpler models, which misclassifies more training examples.</p></li> <li><p>Which of the following statements about regularization are true? Check all that apply.ã€A,B,C,Dã€‘
A.For computational efficiency, after we have performed gradient checking to verify that our backpropagation code is correct, we usually disable gradient checking before using backpropagation to train the network.
ã€è§£æã€‘Checking the gradient numerically is a debugging tool: it helps ensure a correct implementation,
but it is too slow to use as a method for actually computing gradients.
B.If our neural network overfits the training set, one reasonable step to take is to increase the regularization parameter Î».
ã€è§£æã€‘Just as with logistic regression, a large value of Î» will penalize large parameter values, thereby reducing the changes of overfitting the training set.
C.Suppose you are training a neural network using gradient descent. Depending on your random initialization, your algorithm may converge to different local optima (i.e., if you run the algorithm twice with different random initializations, gradient descent may converge to two different solutions).
ã€è§£æã€‘The cost function for a neural network is non-convex, so it may have multiple minima. Which minimum you find with gradient descent depends on the initialization.
D.Suppose we have a correct implementation of backpropagation, and are training a neural network using gradient descent. Suppose we plot J(Î˜) as a function of the number of iterations, and find that it is increasing rather than decreasing. One possible cause of this is that the learning rate Î± is too large.
ã€è§£æã€‘If the learning rate is too large, the cost function can diverge during gradient descent. Thus, you should select a smaller value of Î±.
E.Suppose that the parameter Î˜(1) is a square matrix (meaning the number of rows equals the number of columns). If we replace Î˜(1) with its transpose (Î˜(1))T, then we have not changed the function that the network is computing.
ã€è§£æã€‘Î˜(1) can be an arbitrary matrix, so when you compute a(2)=g(Î˜(1)a(1)), replacing Î˜(1) with its transpose will compute a different value.
F.Suppose we are using gradient descent with learning rate Î±. For logistic regression and linear regression, J(Î¸) was a convex optimization problem and thus we did not want to choose a learning rate Î± that is too large. For a neural network however, J(Î˜) may not be convex, and thus choosing a very large value of Î± can only speed up convergence.
ã€è§£æã€‘Even when J(Î˜) is not convex, a learning rate that is too large can prevent gradient descent from converging.
G.Using a large value of Î» cannot hurt the performance of your neural network; the only reason we do not set Î» to be too large is to avoid numerical problems.
ã€è§£æã€‘A large value of Î» can be quite detrimental. If you set it too high, then the network will be underfit to the training data and give poor predictions on both training data and new, unseen test data.
H.Gradient checking is useful if we are using gradient descent as our optimization algorithm. However, it serves little purpose if we are using one of the advanced optimization methods (such as in fminunc).
ã€è§£æã€‘Gradient checking will still be useful with advanced optimization methods, as they depend on computing the gradient at given parameter settings. The difference is they use the gradient values in more sophisticated ways than gradient descent.</p></li></ol> <p>exercise</p> <p>This post contains links to all of the programming exercise tutorials.
After clicking on a link, you may need to scroll down to find the highlighted post.
--- Note: Additional test cases can be found<a href="https://www.coursera.org/learn/machine-learning/discussions/0SxufTSrEeWPACIACw4G5w" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>ex1
computeCost() tutorial - also applies to computeCostMulti().
gradientDescent() - also applies to gradientDescentMulti() - includes test cases.
featureNormalize() tutorial
Note: if you use OS X and the contour plot doesn't display correctly, see the Course Wiki for additional tips.</p> <hr> <p>ex1 Tutorial for computeCost
Tom MosherMentorWeek 2 Â· 2 years ago Â· Edited
This is a step-by-step tutorial for how to complete the computeCost() function portion of ex1. You will still have to do some thinking, because I'll describe the implementation, but you have to turn it into Octave script commands.
All the programming exercises in this course follow the same procedure; you are provided a starter code template for a function that you need to complete. You never have to start a new script file from scratch.
This is a vectorized implementation. You're only going to write a few simple lines of code.
With a text editor (NOT a word processor), open up the computeCost.m file. Scroll down until you find the &quot;====== YOUR CODE HERE =====&quot; section. Below this section is where you're going to add your lines of code. Just skip over the lines that start with the '%' sign - those are instructive comments.
We'll write these three lines of code by inspecting the equation on Page 5 of ex1.pdf
The first line of code will compute a vector 'h' containing all of the hypothesis values - one for each training example (i.e. for each row of X).
The hypothesis (also called the prediction) is simply the product of X and theta. So your first line of code isâ€¦
h = {multiply X and theta, in the proper order that the inner    dimensions match}
Since X is size (m x n) and theta is size (n x 1), you arrange the order of operators so the result is size (m x 1).
The second line of code will compute the difference between the hypothesis and y - that's the error for each training example. Difference means subtract.
error = {the difference between h and y}
The third line of code will compute the square of each of those error terms (using element-wise exponentiation),
An example of using element-wise exponentiation - try this in your workspace command line so you see how it works.
v = [-2 3]
v_sqr = v.^2
So, now you should compute the squares of the error terms:
error_sqr = {use what you have learned}
Next, here's an example of how the sum function works (try this from your command line)
q = sum([1 2 3])
Now, we'll finish the last two steps all in one line of code. You need to compute the sum of the error_sqr vector, and scale the result (multiply) by 1/(2<em>m). That completed sum is the cost value J.
J = {multiply 1/(2</em>m) times the sum of the error_sqr vector}
That's it. If you run the ex1.m script (by entering the command &quot;ex1&quot; in the console), you should have the correct value for J. Then you should test further by running the additional Test Cases (available via the Resources menu).
Important Note: You cannot test your computeCost() function by simply entering &quot;computeCost&quot; or &quot;computeCost()&quot; in the console. The function requires that you pass it three data parameters (X, y, and theta). The &quot;ex1&quot; script does this for you.
Then you can run the &quot;submit&quot; script, and hopefully it will pass.
Note: Be sure that every line of code ends with a semicolon. That will suppress the output of any values to the workspace. Leaving out the semicolons will surely make the grader unhappy.</p> <p>============</p> <p>ex2
Note: If you are using MATLAB version R2015a or later, the fminunc() function has been changed in this version. The function works better, but does not give the expected result for Figure 5 in ex2.pdf, and it throws some warning messages (about a local minimum) when you run ex2_reg.m. This is normal, and you should still be able to submit your work to the grader.
Note: If your installation has trouble with the GradObj option, see this thread: <link>
Note: If you are using a linux-derived operating system, you may need to remove the attribute &quot;MarkerFaceColor&quot; from the plot() function call in plotData.m.</p> <hr> <p>sigmoid() tutorial
costFunction() cost tutorial - also good for costFunctionReg()
costFunction() gradient tutorial - also good for costFunctionReg()
predict() - tutorial for logistic regression prediction
Discussion of plotDecisionBoundary() <link></p> <hr> <p>ex3
Note: a change to displayData.m for MacOS users: (link)
Note: if your images are upside-down, use flipud() to reverse the data. This is due to a change in gnuplot()'s defaults.
lrCostFunction() - This function is identical to your costFunctionReg() from ex2. Do not remove the line &quot;grad = grad(ğŸ˜ƒ&quot; from the end of the lrCostFunction.m script template. This line guarantees that the grad value is returned as a column vector.
oneVsAll() tutorial
predictOneVsAll() tutorial (updated)
predict() tutorial (for the NN forward propagation - updated)</p> <hr> <p>ex4
nnCostFunction() - forward propagation and cost w/ regularization
nnCostFunction() - tutorial for backpropagation
Tutorial on using matrix multiplication to compute the cost value 'J'</p> <hr> <p>ex5
linearRegCostFunction() tutorial
polyFeatures() - tutorial
learningCurve() tutorial (really just a set of tips)
validationCurve() tips</p> <hr> <p>ex6
Note: Update to ex6.m: At line 69/70, change &quot;sigma = 0.5&quot; to &quot;sigma = %0.5f&quot;
and change the list of output variables from &quot;sim&quot; to &quot;sigma, sim&quot;.
(note: As of Jan 2017, this issue is already included in the zip file)
Note: Error in visualizeBoundary.m. Change the call to contour() like this:
contour(X1, X2, vals, [1 1], 'b');
(This change removes the attribute 'Color', and changes the contour interval. Note that [0.5 0.5] also works and is more logical, since &quot;vals&quot; has range [0..1])
This issue can cause either the &quot;hggroup&quot; error message, or the decision boundaries to not be displayed, or possibly cause Octave 3.8.x to crash when running ex6.m.
All ex6 tutorials (link)</p> <hr> <p>ex7
findClosestCentroids() tutorial
computeCentroids() tutorial
Tutorials for ex7_pca functions - pca(), projectData(), recoverData()</p> <hr> <p>ex8
selectThreshold() - use the tips in the function script template, and the bulleted list on page 6 of ex8.pdf, to compute each of the tp, fp, and fn values.
Note: error in ex8_cofi.m (click this link)
Tip for estimateGaussian(): Compute the mean using &quot;mean()&quot;. You can compute sigma2 using the equation in ex8.pdf, or you can use &quot;var()&quot; if you set the OPT parameter so it normalizes over the entire sample size.
cofiCostFunc() tutorial</p> <p>--</p> <p>ref:</p> <p>Coursera
https://www.coursera.org/learn/machine-learning/home/welcome</p> <p>MIT
Artificial Intelligence https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/</p> <p>Courseraä¸Šæœºå™¨å­¦ä¹ è¯¾ç¨‹ï¼ˆå…¬å¼€è¯¾ï¼‰æ±‡æ€»æ¨è
http://blog.coursegraph.com/tag/æœºå™¨å­¦ä¹ 
æ–¯å¦ç¦æœºå™¨å­¦ä¹ ç¬”è®° https://legacy.gitbook.com/book/yoyoyohamapi/mit-ml/details
http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex7/ex7.html</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/docs/assets/js/app.62086776.js" defer></script><script src="/docs/assets/js/2.dc5756d7.js" defer></script><script src="/docs/assets/js/67.7ded8a09.js" defer></script>
  </body>
</html>
