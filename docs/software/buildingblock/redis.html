<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>计算机基础教程</title>
    <meta name="generator" content="VuePress 1.9.7">
    <script async="true" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9742852210287449" crossorigin="anonymous"></script>
    <script async="true" src="https://www.googletagmanager.com/gtag/js?id=AW-748196294"></script>
    <script>
          window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());
		  
		  gtag('config', 'AW-748196294');
        </script>
    <script>
			(function() { // DON'T EDIT BELOW THIS LINE
			
			})();
		 </script>
    <meta name="description" content="软件开发教程，白帽黑客入门教程，区块链入门教程，物联网，大数据">
    
    <link rel="preload" href="/docs/assets/css/0.styles.d4eb5ad8.css" as="style"><link rel="preload" href="/docs/assets/js/app.650a987d.js" as="script"><link rel="preload" href="/docs/assets/js/2.d450803a.js" as="script"><link rel="preload" href="/docs/assets/js/193.04942d8f.js" as="script"><link rel="preload" href="/docs/assets/js/7.be017326.js" as="script"><link rel="prefetch" href="/docs/assets/js/10.6280ea0f.js"><link rel="prefetch" href="/docs/assets/js/100.78a1e34e.js"><link rel="prefetch" href="/docs/assets/js/101.d462fae9.js"><link rel="prefetch" href="/docs/assets/js/102.77970bea.js"><link rel="prefetch" href="/docs/assets/js/103.ef970172.js"><link rel="prefetch" href="/docs/assets/js/104.ce8eb710.js"><link rel="prefetch" href="/docs/assets/js/105.36a71e55.js"><link rel="prefetch" href="/docs/assets/js/106.b2d3b318.js"><link rel="prefetch" href="/docs/assets/js/107.bd236459.js"><link rel="prefetch" href="/docs/assets/js/108.62b2c756.js"><link rel="prefetch" href="/docs/assets/js/109.200d1c9e.js"><link rel="prefetch" href="/docs/assets/js/11.4f1296fa.js"><link rel="prefetch" href="/docs/assets/js/110.fb8c7fe0.js"><link rel="prefetch" href="/docs/assets/js/111.0f75c6ac.js"><link rel="prefetch" href="/docs/assets/js/112.0b136230.js"><link rel="prefetch" href="/docs/assets/js/113.1d671039.js"><link rel="prefetch" href="/docs/assets/js/114.7a842863.js"><link rel="prefetch" href="/docs/assets/js/115.7828b562.js"><link rel="prefetch" href="/docs/assets/js/116.51bf1151.js"><link rel="prefetch" href="/docs/assets/js/117.e459d63c.js"><link rel="prefetch" href="/docs/assets/js/118.7322910d.js"><link rel="prefetch" href="/docs/assets/js/119.0b0b9504.js"><link rel="prefetch" href="/docs/assets/js/12.25f970c0.js"><link rel="prefetch" href="/docs/assets/js/120.9376b1d8.js"><link rel="prefetch" href="/docs/assets/js/121.20883507.js"><link rel="prefetch" href="/docs/assets/js/122.63f4c7cf.js"><link rel="prefetch" href="/docs/assets/js/123.c7764c61.js"><link rel="prefetch" href="/docs/assets/js/124.62a5fce9.js"><link rel="prefetch" href="/docs/assets/js/125.0ffc653e.js"><link rel="prefetch" href="/docs/assets/js/126.54d37b72.js"><link rel="prefetch" href="/docs/assets/js/127.c2c22d99.js"><link rel="prefetch" href="/docs/assets/js/128.be15ff04.js"><link rel="prefetch" href="/docs/assets/js/129.289dba17.js"><link rel="prefetch" href="/docs/assets/js/13.0a356446.js"><link rel="prefetch" href="/docs/assets/js/130.53c551b0.js"><link rel="prefetch" href="/docs/assets/js/131.26f3119f.js"><link rel="prefetch" href="/docs/assets/js/132.2da5a809.js"><link rel="prefetch" href="/docs/assets/js/133.805f36ba.js"><link rel="prefetch" href="/docs/assets/js/134.ae98d9c6.js"><link rel="prefetch" href="/docs/assets/js/135.93655658.js"><link rel="prefetch" href="/docs/assets/js/136.c8db6196.js"><link rel="prefetch" href="/docs/assets/js/137.c1483793.js"><link rel="prefetch" href="/docs/assets/js/138.3b990d46.js"><link rel="prefetch" href="/docs/assets/js/139.ddc2131e.js"><link rel="prefetch" href="/docs/assets/js/14.f8cbf4a0.js"><link rel="prefetch" href="/docs/assets/js/140.edb31f2f.js"><link rel="prefetch" href="/docs/assets/js/141.93bde59f.js"><link rel="prefetch" href="/docs/assets/js/142.6da407a1.js"><link rel="prefetch" href="/docs/assets/js/143.6b73dadb.js"><link rel="prefetch" href="/docs/assets/js/144.0e5dec99.js"><link rel="prefetch" href="/docs/assets/js/145.cffa89ce.js"><link rel="prefetch" href="/docs/assets/js/146.eeef3cdb.js"><link rel="prefetch" href="/docs/assets/js/147.f4a64dc7.js"><link rel="prefetch" href="/docs/assets/js/148.eefd5839.js"><link rel="prefetch" href="/docs/assets/js/149.66c85b5d.js"><link rel="prefetch" href="/docs/assets/js/15.1149fa19.js"><link rel="prefetch" href="/docs/assets/js/150.c1620bfe.js"><link rel="prefetch" href="/docs/assets/js/151.e6811790.js"><link rel="prefetch" href="/docs/assets/js/152.ef71743e.js"><link rel="prefetch" href="/docs/assets/js/153.7f93b564.js"><link rel="prefetch" href="/docs/assets/js/154.fc8e099a.js"><link rel="prefetch" href="/docs/assets/js/155.4e6aca0d.js"><link rel="prefetch" href="/docs/assets/js/156.2dbf12a3.js"><link rel="prefetch" href="/docs/assets/js/157.844a0f9b.js"><link rel="prefetch" href="/docs/assets/js/158.5c17e555.js"><link rel="prefetch" href="/docs/assets/js/159.3e66963e.js"><link rel="prefetch" href="/docs/assets/js/16.47e36cc3.js"><link rel="prefetch" href="/docs/assets/js/160.16fb7822.js"><link rel="prefetch" href="/docs/assets/js/161.bbfe2865.js"><link rel="prefetch" href="/docs/assets/js/162.b113f3ae.js"><link rel="prefetch" href="/docs/assets/js/163.067b4227.js"><link rel="prefetch" href="/docs/assets/js/164.82ef6347.js"><link rel="prefetch" href="/docs/assets/js/165.8d08b6fc.js"><link rel="prefetch" href="/docs/assets/js/166.8b399bfe.js"><link rel="prefetch" href="/docs/assets/js/167.1bb5fa28.js"><link rel="prefetch" href="/docs/assets/js/168.fca08777.js"><link rel="prefetch" href="/docs/assets/js/169.8c26c529.js"><link rel="prefetch" href="/docs/assets/js/17.58360e9a.js"><link rel="prefetch" href="/docs/assets/js/170.c67e9069.js"><link rel="prefetch" href="/docs/assets/js/171.6dffe5c5.js"><link rel="prefetch" href="/docs/assets/js/172.c9b3db0f.js"><link rel="prefetch" href="/docs/assets/js/173.8f15d14d.js"><link rel="prefetch" href="/docs/assets/js/174.43af4d3c.js"><link rel="prefetch" href="/docs/assets/js/175.a6e5793f.js"><link rel="prefetch" href="/docs/assets/js/176.ebcef599.js"><link rel="prefetch" href="/docs/assets/js/177.2175c95d.js"><link rel="prefetch" href="/docs/assets/js/178.fc4f8b9d.js"><link rel="prefetch" href="/docs/assets/js/179.9468428c.js"><link rel="prefetch" href="/docs/assets/js/18.cdf43746.js"><link rel="prefetch" href="/docs/assets/js/180.c9386a8f.js"><link rel="prefetch" href="/docs/assets/js/181.f0decc92.js"><link rel="prefetch" href="/docs/assets/js/182.45e1f5d4.js"><link rel="prefetch" href="/docs/assets/js/183.c9544df2.js"><link rel="prefetch" href="/docs/assets/js/184.4288b66c.js"><link rel="prefetch" href="/docs/assets/js/185.a6871a28.js"><link rel="prefetch" href="/docs/assets/js/186.0cae6ad0.js"><link rel="prefetch" href="/docs/assets/js/187.eb0a2d4c.js"><link rel="prefetch" href="/docs/assets/js/188.5428bbcd.js"><link rel="prefetch" href="/docs/assets/js/189.c2023260.js"><link rel="prefetch" href="/docs/assets/js/19.9643a492.js"><link rel="prefetch" href="/docs/assets/js/190.1259fe0c.js"><link rel="prefetch" href="/docs/assets/js/191.8158a666.js"><link rel="prefetch" href="/docs/assets/js/192.a5ae8765.js"><link rel="prefetch" href="/docs/assets/js/194.59a9f2cd.js"><link rel="prefetch" href="/docs/assets/js/195.73cb8b00.js"><link rel="prefetch" href="/docs/assets/js/196.99e13e37.js"><link rel="prefetch" href="/docs/assets/js/197.97b7cbd9.js"><link rel="prefetch" href="/docs/assets/js/198.b39b2e80.js"><link rel="prefetch" href="/docs/assets/js/199.5e6907d6.js"><link rel="prefetch" href="/docs/assets/js/20.6fa24146.js"><link rel="prefetch" href="/docs/assets/js/200.836db031.js"><link rel="prefetch" href="/docs/assets/js/201.f92c0826.js"><link rel="prefetch" href="/docs/assets/js/202.337a2a3a.js"><link rel="prefetch" href="/docs/assets/js/203.45fc8e34.js"><link rel="prefetch" href="/docs/assets/js/204.b7c5c51e.js"><link rel="prefetch" href="/docs/assets/js/205.5818cd77.js"><link rel="prefetch" href="/docs/assets/js/206.a8925824.js"><link rel="prefetch" href="/docs/assets/js/207.f5008095.js"><link rel="prefetch" href="/docs/assets/js/208.8a13c56b.js"><link rel="prefetch" href="/docs/assets/js/209.b21cd054.js"><link rel="prefetch" href="/docs/assets/js/21.0658abb8.js"><link rel="prefetch" href="/docs/assets/js/210.a731fcbd.js"><link rel="prefetch" href="/docs/assets/js/211.b3e2ca64.js"><link rel="prefetch" href="/docs/assets/js/212.4343b22b.js"><link rel="prefetch" href="/docs/assets/js/213.df508ee1.js"><link rel="prefetch" href="/docs/assets/js/214.8f13134f.js"><link rel="prefetch" href="/docs/assets/js/215.ac8a1df9.js"><link rel="prefetch" href="/docs/assets/js/216.3190392d.js"><link rel="prefetch" href="/docs/assets/js/217.9a53a331.js"><link rel="prefetch" href="/docs/assets/js/218.734ed28b.js"><link rel="prefetch" href="/docs/assets/js/219.29558ea8.js"><link rel="prefetch" href="/docs/assets/js/22.54790dac.js"><link rel="prefetch" href="/docs/assets/js/220.e3b54da8.js"><link rel="prefetch" href="/docs/assets/js/221.b859f1f1.js"><link rel="prefetch" href="/docs/assets/js/222.da05c4df.js"><link rel="prefetch" href="/docs/assets/js/223.6e975196.js"><link rel="prefetch" href="/docs/assets/js/224.f966e4f1.js"><link rel="prefetch" href="/docs/assets/js/225.e00384ee.js"><link rel="prefetch" href="/docs/assets/js/226.738f033d.js"><link rel="prefetch" href="/docs/assets/js/227.6457bb60.js"><link rel="prefetch" href="/docs/assets/js/228.98d89bf8.js"><link rel="prefetch" href="/docs/assets/js/229.50325b8a.js"><link rel="prefetch" href="/docs/assets/js/23.dc0c5722.js"><link rel="prefetch" href="/docs/assets/js/230.8305f79c.js"><link rel="prefetch" href="/docs/assets/js/231.235238c1.js"><link rel="prefetch" href="/docs/assets/js/232.e8e39046.js"><link rel="prefetch" href="/docs/assets/js/233.7b3a2c63.js"><link rel="prefetch" href="/docs/assets/js/234.b8719239.js"><link rel="prefetch" href="/docs/assets/js/235.dcacf71e.js"><link rel="prefetch" href="/docs/assets/js/236.09b0b33a.js"><link rel="prefetch" href="/docs/assets/js/237.9aa1fe35.js"><link rel="prefetch" href="/docs/assets/js/238.9aba37ee.js"><link rel="prefetch" href="/docs/assets/js/239.d7f19078.js"><link rel="prefetch" href="/docs/assets/js/24.f2287780.js"><link rel="prefetch" href="/docs/assets/js/240.23e02c5f.js"><link rel="prefetch" href="/docs/assets/js/241.06fd31ed.js"><link rel="prefetch" href="/docs/assets/js/242.d4975f70.js"><link rel="prefetch" href="/docs/assets/js/243.270532cf.js"><link rel="prefetch" href="/docs/assets/js/244.751b73ab.js"><link rel="prefetch" href="/docs/assets/js/245.a58ad64e.js"><link rel="prefetch" href="/docs/assets/js/246.a72ec5f0.js"><link rel="prefetch" href="/docs/assets/js/247.a0f63b0d.js"><link rel="prefetch" href="/docs/assets/js/248.757c56f0.js"><link rel="prefetch" href="/docs/assets/js/249.78cdcccc.js"><link rel="prefetch" href="/docs/assets/js/25.5fed7ab6.js"><link rel="prefetch" href="/docs/assets/js/250.6738035e.js"><link rel="prefetch" href="/docs/assets/js/251.a098be8f.js"><link rel="prefetch" href="/docs/assets/js/252.1f8d0803.js"><link rel="prefetch" href="/docs/assets/js/253.0e255e2d.js"><link rel="prefetch" href="/docs/assets/js/254.7cbcb887.js"><link rel="prefetch" href="/docs/assets/js/255.9e5e9947.js"><link rel="prefetch" href="/docs/assets/js/256.c28cd484.js"><link rel="prefetch" href="/docs/assets/js/257.f210130c.js"><link rel="prefetch" href="/docs/assets/js/258.cdb0f10b.js"><link rel="prefetch" href="/docs/assets/js/259.bc6c670f.js"><link rel="prefetch" href="/docs/assets/js/26.91e32847.js"><link rel="prefetch" href="/docs/assets/js/260.81c5b013.js"><link rel="prefetch" href="/docs/assets/js/261.cd95619f.js"><link rel="prefetch" href="/docs/assets/js/262.ea4dc4c4.js"><link rel="prefetch" href="/docs/assets/js/263.35997384.js"><link rel="prefetch" href="/docs/assets/js/264.8a4c62d0.js"><link rel="prefetch" href="/docs/assets/js/265.a255ac5e.js"><link rel="prefetch" href="/docs/assets/js/266.7aba2bb0.js"><link rel="prefetch" href="/docs/assets/js/267.e002959d.js"><link rel="prefetch" href="/docs/assets/js/268.a8aa8dea.js"><link rel="prefetch" href="/docs/assets/js/269.159e2971.js"><link rel="prefetch" href="/docs/assets/js/27.a5be7aee.js"><link rel="prefetch" href="/docs/assets/js/270.e4d93d87.js"><link rel="prefetch" href="/docs/assets/js/271.30cf523b.js"><link rel="prefetch" href="/docs/assets/js/272.8f7b9e87.js"><link rel="prefetch" href="/docs/assets/js/273.437af003.js"><link rel="prefetch" href="/docs/assets/js/274.2b76000f.js"><link rel="prefetch" href="/docs/assets/js/275.4f6e2ae7.js"><link rel="prefetch" href="/docs/assets/js/276.23a45ec1.js"><link rel="prefetch" href="/docs/assets/js/277.96495a2c.js"><link rel="prefetch" href="/docs/assets/js/278.7681e6b1.js"><link rel="prefetch" href="/docs/assets/js/279.1a1ff129.js"><link rel="prefetch" href="/docs/assets/js/28.72f87125.js"><link rel="prefetch" href="/docs/assets/js/280.5fde0f60.js"><link rel="prefetch" href="/docs/assets/js/281.979d212a.js"><link rel="prefetch" href="/docs/assets/js/282.d4b989d4.js"><link rel="prefetch" href="/docs/assets/js/283.c2c7d3a6.js"><link rel="prefetch" href="/docs/assets/js/284.c67aa032.js"><link rel="prefetch" href="/docs/assets/js/285.b93eafb1.js"><link rel="prefetch" href="/docs/assets/js/286.2972ab24.js"><link rel="prefetch" href="/docs/assets/js/287.40e455fd.js"><link rel="prefetch" href="/docs/assets/js/288.445eed75.js"><link rel="prefetch" href="/docs/assets/js/289.c680b1b5.js"><link rel="prefetch" href="/docs/assets/js/29.aaaf9769.js"><link rel="prefetch" href="/docs/assets/js/290.f4d29ac7.js"><link rel="prefetch" href="/docs/assets/js/291.9cf2bc7b.js"><link rel="prefetch" href="/docs/assets/js/292.af01b9f7.js"><link rel="prefetch" href="/docs/assets/js/293.64eefd6c.js"><link rel="prefetch" href="/docs/assets/js/294.a868ee96.js"><link rel="prefetch" href="/docs/assets/js/295.118b7890.js"><link rel="prefetch" href="/docs/assets/js/296.21a66eb0.js"><link rel="prefetch" href="/docs/assets/js/297.3c4faef1.js"><link rel="prefetch" href="/docs/assets/js/298.998358b2.js"><link rel="prefetch" href="/docs/assets/js/299.af2155a5.js"><link rel="prefetch" href="/docs/assets/js/3.2adef09c.js"><link rel="prefetch" href="/docs/assets/js/30.0ad9d011.js"><link rel="prefetch" href="/docs/assets/js/300.f6ad0a9b.js"><link rel="prefetch" href="/docs/assets/js/301.68e2959f.js"><link rel="prefetch" href="/docs/assets/js/302.2b7a4d95.js"><link rel="prefetch" href="/docs/assets/js/303.876d7a0c.js"><link rel="prefetch" href="/docs/assets/js/304.00098142.js"><link rel="prefetch" href="/docs/assets/js/305.8f5de789.js"><link rel="prefetch" href="/docs/assets/js/306.07bf597a.js"><link rel="prefetch" href="/docs/assets/js/307.c9855286.js"><link rel="prefetch" href="/docs/assets/js/308.f29dd64a.js"><link rel="prefetch" href="/docs/assets/js/309.9a150d03.js"><link rel="prefetch" href="/docs/assets/js/31.910795dd.js"><link rel="prefetch" href="/docs/assets/js/310.4888366e.js"><link rel="prefetch" href="/docs/assets/js/311.5b3c6aed.js"><link rel="prefetch" href="/docs/assets/js/312.7a150454.js"><link rel="prefetch" href="/docs/assets/js/313.1e6d4ee2.js"><link rel="prefetch" href="/docs/assets/js/314.f882be85.js"><link rel="prefetch" href="/docs/assets/js/315.a87b71b2.js"><link rel="prefetch" href="/docs/assets/js/316.24cdc507.js"><link rel="prefetch" href="/docs/assets/js/317.ed7c03fb.js"><link rel="prefetch" href="/docs/assets/js/318.0340a33a.js"><link rel="prefetch" href="/docs/assets/js/319.448ecc60.js"><link rel="prefetch" href="/docs/assets/js/32.2643b16c.js"><link rel="prefetch" href="/docs/assets/js/320.6dc34c63.js"><link rel="prefetch" href="/docs/assets/js/321.ebd6a5c5.js"><link rel="prefetch" href="/docs/assets/js/322.ea9ae054.js"><link rel="prefetch" href="/docs/assets/js/323.d4306d1c.js"><link rel="prefetch" href="/docs/assets/js/324.731fe0fb.js"><link rel="prefetch" href="/docs/assets/js/325.f26cdf8a.js"><link rel="prefetch" href="/docs/assets/js/326.f9a8f865.js"><link rel="prefetch" href="/docs/assets/js/327.c6e9ff14.js"><link rel="prefetch" href="/docs/assets/js/328.a07220ad.js"><link rel="prefetch" href="/docs/assets/js/329.5358218a.js"><link rel="prefetch" href="/docs/assets/js/33.934b3724.js"><link rel="prefetch" href="/docs/assets/js/330.1d3519cc.js"><link rel="prefetch" href="/docs/assets/js/331.58bbbf11.js"><link rel="prefetch" href="/docs/assets/js/332.9cc5fb5e.js"><link rel="prefetch" href="/docs/assets/js/333.90a684e5.js"><link rel="prefetch" href="/docs/assets/js/334.780de00d.js"><link rel="prefetch" href="/docs/assets/js/335.4586387c.js"><link rel="prefetch" href="/docs/assets/js/336.765bb178.js"><link rel="prefetch" href="/docs/assets/js/337.75aeaaca.js"><link rel="prefetch" href="/docs/assets/js/338.f601c8cc.js"><link rel="prefetch" href="/docs/assets/js/339.39b28555.js"><link rel="prefetch" href="/docs/assets/js/34.ce795261.js"><link rel="prefetch" href="/docs/assets/js/340.69cdf746.js"><link rel="prefetch" href="/docs/assets/js/341.4c7b362c.js"><link rel="prefetch" href="/docs/assets/js/342.9df8f39b.js"><link rel="prefetch" href="/docs/assets/js/343.9c3504d0.js"><link rel="prefetch" href="/docs/assets/js/344.450fa488.js"><link rel="prefetch" href="/docs/assets/js/345.d59da24b.js"><link rel="prefetch" href="/docs/assets/js/346.62d130bf.js"><link rel="prefetch" href="/docs/assets/js/347.367bdf4b.js"><link rel="prefetch" href="/docs/assets/js/348.8d611f76.js"><link rel="prefetch" href="/docs/assets/js/349.620f4d88.js"><link rel="prefetch" href="/docs/assets/js/35.b4ca4763.js"><link rel="prefetch" href="/docs/assets/js/350.6e8da176.js"><link rel="prefetch" href="/docs/assets/js/351.174f986a.js"><link rel="prefetch" href="/docs/assets/js/352.cd76f92c.js"><link rel="prefetch" href="/docs/assets/js/353.1f1ed4ea.js"><link rel="prefetch" href="/docs/assets/js/354.8b118a78.js"><link rel="prefetch" href="/docs/assets/js/355.760ab545.js"><link rel="prefetch" href="/docs/assets/js/356.f5701d95.js"><link rel="prefetch" href="/docs/assets/js/357.d68ea3a9.js"><link rel="prefetch" href="/docs/assets/js/358.349be35f.js"><link rel="prefetch" href="/docs/assets/js/359.1dda186b.js"><link rel="prefetch" href="/docs/assets/js/36.23d4fb3f.js"><link rel="prefetch" href="/docs/assets/js/360.ee25bf86.js"><link rel="prefetch" href="/docs/assets/js/361.0d16e528.js"><link rel="prefetch" href="/docs/assets/js/362.ad0dc600.js"><link rel="prefetch" href="/docs/assets/js/363.4622c41e.js"><link rel="prefetch" href="/docs/assets/js/364.6367f1d5.js"><link rel="prefetch" href="/docs/assets/js/365.a4c7f801.js"><link rel="prefetch" href="/docs/assets/js/366.85ce8578.js"><link rel="prefetch" href="/docs/assets/js/367.adb425e9.js"><link rel="prefetch" href="/docs/assets/js/368.788b0abc.js"><link rel="prefetch" href="/docs/assets/js/37.3bebbd80.js"><link rel="prefetch" href="/docs/assets/js/38.9fcd912f.js"><link rel="prefetch" href="/docs/assets/js/39.489dd677.js"><link rel="prefetch" href="/docs/assets/js/4.6d27a4d7.js"><link rel="prefetch" href="/docs/assets/js/40.f792fcd8.js"><link rel="prefetch" href="/docs/assets/js/41.202b7cb1.js"><link rel="prefetch" href="/docs/assets/js/42.831dc71f.js"><link rel="prefetch" href="/docs/assets/js/43.62bdce03.js"><link rel="prefetch" href="/docs/assets/js/44.97fe4437.js"><link rel="prefetch" href="/docs/assets/js/45.0dbebe9e.js"><link rel="prefetch" href="/docs/assets/js/46.6901fc3e.js"><link rel="prefetch" href="/docs/assets/js/47.48417af1.js"><link rel="prefetch" href="/docs/assets/js/48.bfd2d337.js"><link rel="prefetch" href="/docs/assets/js/49.7bffafdb.js"><link rel="prefetch" href="/docs/assets/js/5.3f8add54.js"><link rel="prefetch" href="/docs/assets/js/50.bcddfc42.js"><link rel="prefetch" href="/docs/assets/js/51.b05efe73.js"><link rel="prefetch" href="/docs/assets/js/52.9643828c.js"><link rel="prefetch" href="/docs/assets/js/53.9a1a842f.js"><link rel="prefetch" href="/docs/assets/js/54.6511bd39.js"><link rel="prefetch" href="/docs/assets/js/55.fe79e0ef.js"><link rel="prefetch" href="/docs/assets/js/56.011d602c.js"><link rel="prefetch" href="/docs/assets/js/57.88899fc7.js"><link rel="prefetch" href="/docs/assets/js/58.2d157dba.js"><link rel="prefetch" href="/docs/assets/js/59.73790e3e.js"><link rel="prefetch" href="/docs/assets/js/6.9965f195.js"><link rel="prefetch" href="/docs/assets/js/60.096cf39b.js"><link rel="prefetch" href="/docs/assets/js/61.de254afd.js"><link rel="prefetch" href="/docs/assets/js/62.d62298dd.js"><link rel="prefetch" href="/docs/assets/js/63.726f066a.js"><link rel="prefetch" href="/docs/assets/js/64.708515f3.js"><link rel="prefetch" href="/docs/assets/js/65.c002ad7b.js"><link rel="prefetch" href="/docs/assets/js/66.b601c078.js"><link rel="prefetch" href="/docs/assets/js/67.5ddcc27c.js"><link rel="prefetch" href="/docs/assets/js/68.c0a179f4.js"><link rel="prefetch" href="/docs/assets/js/69.85ec12b9.js"><link rel="prefetch" href="/docs/assets/js/70.340cd6f4.js"><link rel="prefetch" href="/docs/assets/js/71.71686ab3.js"><link rel="prefetch" href="/docs/assets/js/72.a0c091c1.js"><link rel="prefetch" href="/docs/assets/js/73.b9e2c861.js"><link rel="prefetch" href="/docs/assets/js/74.c161aa35.js"><link rel="prefetch" href="/docs/assets/js/75.dacec3db.js"><link rel="prefetch" href="/docs/assets/js/76.1a288b5c.js"><link rel="prefetch" href="/docs/assets/js/77.d259f14b.js"><link rel="prefetch" href="/docs/assets/js/78.a477dba5.js"><link rel="prefetch" href="/docs/assets/js/79.e35c7003.js"><link rel="prefetch" href="/docs/assets/js/8.1fe519c9.js"><link rel="prefetch" href="/docs/assets/js/80.963f1976.js"><link rel="prefetch" href="/docs/assets/js/81.ed2c492e.js"><link rel="prefetch" href="/docs/assets/js/82.92310094.js"><link rel="prefetch" href="/docs/assets/js/83.7192481a.js"><link rel="prefetch" href="/docs/assets/js/84.1b56ffca.js"><link rel="prefetch" href="/docs/assets/js/85.38a3cfc3.js"><link rel="prefetch" href="/docs/assets/js/86.12512fcb.js"><link rel="prefetch" href="/docs/assets/js/87.5cc76acc.js"><link rel="prefetch" href="/docs/assets/js/88.4236f40c.js"><link rel="prefetch" href="/docs/assets/js/89.947fa2b7.js"><link rel="prefetch" href="/docs/assets/js/9.0d3f257e.js"><link rel="prefetch" href="/docs/assets/js/90.95fcf7bb.js"><link rel="prefetch" href="/docs/assets/js/91.4e73014e.js"><link rel="prefetch" href="/docs/assets/js/92.27f33a47.js"><link rel="prefetch" href="/docs/assets/js/93.2e6077ce.js"><link rel="prefetch" href="/docs/assets/js/94.65eaa548.js"><link rel="prefetch" href="/docs/assets/js/95.3868c1ab.js"><link rel="prefetch" href="/docs/assets/js/96.d3644ba5.js"><link rel="prefetch" href="/docs/assets/js/97.663bcd82.js"><link rel="prefetch" href="/docs/assets/js/98.20f3dcd1.js"><link rel="prefetch" href="/docs/assets/js/99.af6df0c8.js">
    <link rel="stylesheet" href="/docs/assets/css/0.styles.d4eb5ad8.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/docs/" class="home-link router-link-active"><!----> <span class="site-name">计算机基础教程</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/docs/" class="nav-link">
  机器指令
</a></div><div class="nav-item"><a href="/docs/software/" class="nav-link router-link-active">
  软件基础
</a></div><div class="nav-item"><a href="/docs/coder2hacker/" class="nav-link">
  白帽黑客
</a></div><div class="nav-item"><a href="/docs/blockchain/" class="nav-link">
  区块链
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/docs/" class="nav-link">
  机器指令
</a></div><div class="nav-item"><a href="/docs/software/" class="nav-link router-link-active">
  软件基础
</a></div><div class="nav-item"><a href="/docs/coder2hacker/" class="nav-link">
  白帽黑客
</a></div><div class="nav-item"><a href="/docs/blockchain/" class="nav-link">
  区块链
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span></span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/software/buildingblock/redis.html#_1-安装使用" class="sidebar-link">1. 安装使用</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_1-1-简单安装" class="sidebar-link">1.1 简单安装</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_1-2-生产环境安装-推荐" class="sidebar-link">1.2 生产环境安装（推荐）</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_1-3-单节点启动-single-node" class="sidebar-link">1.3 单节点启动 Single node</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_1-4-集群启动-cluster-mode" class="sidebar-link">1.4 集群启动 Cluster Mode</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#使用utils脚本" class="sidebar-link" style="padding-left:3rem;">使用utils脚本</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#手动创建" class="sidebar-link" style="padding-left:3rem;">手动创建</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#customize脚本" class="sidebar-link" style="padding-left:3rem;">customize脚本</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_1-5-config" class="sidebar-link">1.5 Config</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#common-config" class="sidebar-link" style="padding-left:3rem;">common config</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#cluster-config" class="sidebar-link" style="padding-left:3rem;">cluster config</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_1-6-commands" class="sidebar-link">1.6 commands</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_1-7-restore-an-rdb-file" class="sidebar-link">1.7 Restore an RDB file</a></li></ul></li><li><a href="/docs/software/buildingblock/redis.html#_2-理论基础-theory" class="sidebar-link">2. 理论基础 Theory</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_2-1-基本" class="sidebar-link">2.1 基本</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#replication" class="sidebar-link" style="padding-left:3rem;">Replication</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#redis-sentinel-vs-redis-cluster" class="sidebar-link" style="padding-left:3rem;">Redis Sentinel vs Redis Cluster</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_2-2-redis-cluster" class="sidebar-link">2.2 Redis Cluster</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#cluster-gossip-protocol" class="sidebar-link" style="padding-left:3rem;">Cluster Gossip Protocol</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#cluster-data-sharding" class="sidebar-link" style="padding-left:3rem;">Cluster DATA SHARDING</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#consitensy-guarantee" class="sidebar-link" style="padding-left:3rem;">Consitensy guarantee</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#currentepoch-configepoch" class="sidebar-link" style="padding-left:3rem;">currentEpoch &amp; configEpoch</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#cluster-failover-strategy-主从切换" class="sidebar-link" style="padding-left:3rem;">Cluster failover strategy 主从切换</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#step-1-failure-detection" class="sidebar-link" style="padding-left:4rem;">Step 1: Failure detection</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#step-2-slave-election-and-promotion" class="sidebar-link" style="padding-left:4rem;">Step 2: Slave election and promotion</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#example" class="sidebar-link" style="padding-left:4rem;">Example</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#case-1-network-partition-短暂的脑裂" class="sidebar-link" style="padding-left:4rem;">Case 1: network partition 短暂的脑裂</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#case-2-master-fail-slave-promote-to-master" class="sidebar-link" style="padding-left:4rem;">Case 2: master fail=&gt;slave promote to master</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#case-3-mater-slave-both-fail-but-slave-fail-first" class="sidebar-link" style="padding-left:4rem;">Case 3: mater &amp; slave both fail, but slave fail first</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#case-4-slave-of-slave-node" class="sidebar-link" style="padding-left:4rem;">Case 4：Slave of Slave node</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#case-5-网络不稳定-频繁主从切换" class="sidebar-link" style="padding-left:4rem;">Case 5：网络不稳定，频繁主从切换</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#case-6-常见现象-master-nodes-aggregate" class="sidebar-link" style="padding-left:4rem;">Case 6: 常见现象：master nodes aggregate</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_2-3-sentinel" class="sidebar-link">2.3 Sentinel</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_2-4-深度探索" class="sidebar-link">2.4 深度探索</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#内存优化" class="sidebar-link" style="padding-left:3rem;">内存优化</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#内存模型" class="sidebar-link" style="padding-left:4rem;">内存模型</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#命令" class="sidebar-link" style="padding-left:4rem;">命令</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#优化思路" class="sidebar-link" style="padding-left:4rem;">优化思路</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#redis的opsforhash带来的内存空间优化" class="sidebar-link" style="padding-left:4rem;">redis的opsForHash带来的内存空间优化</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#数据倾斜" class="sidebar-link" style="padding-left:3rem;">数据倾斜</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#线程安全" class="sidebar-link" style="padding-left:3rem;">线程安全</a></li></ul></li><li><a href="/docs/software/buildingblock/redis.html#_3-cluster-集群管理" class="sidebar-link">3. cluster 集群管理</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_3-1-commands-gui" class="sidebar-link">3.1 Commands&amp;GUI</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_3-2-自动方式管理" class="sidebar-link">3.2 自动方式管理</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#cluster-failover" class="sidebar-link" style="padding-left:3rem;">cluster failover</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#cluster-replica-migration" class="sidebar-link" style="padding-left:3rem;">cluster replica migration</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#migration-between-clusters-多个集群之间的交互" class="sidebar-link" style="padding-left:3rem;">migration between clusters 多个集群之间的交互</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#多个data-center之间的关系" class="sidebar-link" style="padding-left:3rem;">多个data center之间的关系</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_3-3-手动方式管理" class="sidebar-link">3.3 手动方式管理</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_3-4-日常维护" class="sidebar-link">3.4 日常维护</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_3-5-删除集群" class="sidebar-link">3.5 删除集群</a></li></ul></li><li><a href="/docs/software/buildingblock/redis.html#_4-sentinel-管理" class="sidebar-link">4. Sentinel 管理</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/docs/software/buildingblock/redis.html#_5-redis操作和系统集成-integration" class="sidebar-link">5. Redis操作和系统集成 Integration</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_5-0-redis基本数据操作" class="sidebar-link">5.0 Redis基本数据操作</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#data-types" class="sidebar-link" style="padding-left:3rem;">Data types</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#data-operation" class="sidebar-link" style="padding-left:3rem;">Data operation</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#ttl-key" class="sidebar-link" style="padding-left:3rem;">TTL key</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_5-1-stackexchange-redis" class="sidebar-link">5.1 StackExchange.Redis</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_5-2-python" class="sidebar-link">5.2 Python</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#_5-3-java-spring-boot-integration" class="sidebar-link">5.3 Java-Spring boot integration</a></li></ul></li><li><a href="/docs/software/buildingblock/redis.html#_6-security-hardening" class="sidebar-link">6.Security hardening</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/docs/software/buildingblock/redis.html#troubleshooting" class="sidebar-link">Troubleshooting</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#err-crossslot-keys-in-request-don-t-hash-to-the-same-slot" class="sidebar-link">ERR CROSSSLOT Keys in request don't hash to the same slot.</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#redissystemexception" class="sidebar-link">RedisSystemException</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#unable-to-connect-to-redis" class="sidebar-link">&quot;Unable to connect to Redis;</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#err-protocol-error-invalid-bulk-length" class="sidebar-link">ERR Protocol error: invalid bulk length</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#timeout-issue" class="sidebar-link">Timeout issue</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#err-node-xxxx-6379-is-not-empty" class="sidebar-link">[ERR] Node XXXX:6379 is not empty.</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#issue-raised-by-above-cluster-rest-the-node-become-a-standalone-node-forgot-other-nodes" class="sidebar-link">issue raised by above ‘cluster rest’, the node become a standalone node, forgot other nodes!!!!</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#promote-slave-node-to-master" class="sidebar-link">promote slave node to master</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#failed-delete-node" class="sidebar-link">failed delete node</a></li><li class="sidebar-sub-header"><a href="/docs/software/buildingblock/redis.html#redistemplate-can-not-access-the-node" class="sidebar-link">RedisTemplate can not access the node</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><p><a href="/docs/software">回目录</a>  《分布式缓存redis》</p> <h2 id="_1-安装使用"><a href="#_1-安装使用" class="header-anchor">#</a> 1. 安装使用</h2> <p>https://redis.io/topics/quickstart</p> <h3 id="_1-1-简单安装"><a href="#_1-1-简单安装" class="header-anchor">#</a> 1.1 简单安装</h3> <div class="language- extra-class"><pre class="language-text"><code>yum install gcc

$ wget https://download.redis.io/releases/redis-6.2.1.tar.gz
$ tar xzf redis-6.2.1.tar.gz
$ cd redis-6.2.1
$ make

make test
sudo cp src/redis-server /usr/local/bin/
sudo cp src/redis-cli /usr/local/bin/

卸载的时候别忘记
rm /usr/local/bin/redis-server
rm /usr/local/bin/redis-cli
</code></pre></div><h3 id="_1-2-生产环境安装-推荐"><a href="#_1-2-生产环境安装-推荐" class="header-anchor">#</a> 1.2 生产环境安装（推荐）</h3> <blockquote><p>Installing Redis more properly
Running Redis from the command line is fine just to hack a bit with it or for development. However at some point you'll have some actual application to run on a real server. For this kind of usage you have two different choices:</p> <ul><li>Run Redis using screen.</li> <li>Install Redis in your Linux box in a proper way using an init script, so that after a restart everything will start again properly.
A proper install using an init script is strongly suggested. The following instructions can be used to perform a proper installation using the init script shipped with Redis 2.4 in a Debian or Ubuntu based distribution.</li></ul> <p>We assume you already copied redis-server and redis-cli executables under /usr/local/bin.</p> <ul><li>Create a directory in which to store your Redis config files and your data:
sudo mkdir /etc/redis
sudo mkdir /var/redis</li> <li>Copy the init script that you'll find in the Redis distribution under the utils directory into /etc/init.d. We suggest calling it with the name of the port where you are running this instance of Redis. For example:
sudo cp utils/redis_init_script /etc/init.d/redis_6379</li> <li>Edit the init script.
sudo vi /etc/init.d/redis_6379
...
https://redis.io/topics/quickstart</li></ul></blockquote> <h3 id="_1-3-单节点启动-single-node"><a href="#_1-3-单节点启动-single-node" class="header-anchor">#</a> 1.3 单节点启动 Single node</h3> <div class="language- extra-class"><pre class="language-text"><code>redis-server
Or start with config
redis-server redis.conf
</code></pre></div><h3 id="_1-4-集群启动-cluster-mode"><a href="#_1-4-集群启动-cluster-mode" class="header-anchor">#</a> 1.4 集群启动 Cluster Mode</h3> <p>Redis cluster tutorial https://redis.io/topics/cluster-tutorial</p> <p>Config:</p> <div class="language- extra-class"><pre class="language-text"><code>daemonize yes  #后台运行模式
logfile “redis_6379_log”
pidfile /home/web/redis/var/run/redis_6379.pid  #修改pid文件
dir /home/web/redis/data/ #指定本地数据库存放目录
port 6379  #端口
dbfilename dump_6379.rdb #数据文件
appendfilename &quot;appendonly_6379.aof&quot;
cluster-enabled yes #打开注释，启动cluster模式
cluster-config-file nodes-6379.conf #打开注释，启动cluster模式
# Cluster node timeout is the amount of milliseconds a node must be unreachable
# for it to be considered in failure state.
# Most other internal time limits are multiple of the node timeout.
cluster-node-timeout 15000 #打开注释，启动cluster模式
</code></pre></div><p>参考最低配置：https://redis.io/topics/cluster-tutorial
<em>Note that the minimal cluster that works as expected requires to contain at least three master nodes.</em></p> <h4 id="使用utils脚本"><a href="#使用utils脚本" class="header-anchor">#</a> 使用utils脚本</h4> <p>位置：redis/utils/create-cluster/README</p> <p>To create a cluster, follow these steps:</p> <ol><li>Edit create-cluster and change the start / end port, depending on the
number of instances you want to create.</li> <li>Use &quot;./create-cluster start&quot; in order to run the instances.</li> <li>Use &quot;./create-cluster create&quot; in order to execute redis-cli --cluster create, so that
an actual Redis cluster will be created.</li> <li>Now you are ready to play with the cluster. AOF files and logs for each instances are created in the current directory.</li></ol> <p>In order to stop a cluster:</p> <ol><li>Use &quot;./create-cluster stop&quot; to stop all the instances. After you stopped the instances you can use &quot;./create-cluster start&quot; to restart them if you change your mind.</li> <li>Use &quot;./create-cluster clean&quot; to remove all the AOF / log files to restart with a clean environment.</li></ol> <h4 id="手动创建"><a href="#手动创建" class="header-anchor">#</a> 手动创建</h4> <div class="language- extra-class"><pre class="language-text"><code>redis-server conf/redis6379.conf
redis-server conf/redis6380.conf
redis-server conf/redis6381.conf
redis-cli --cluster create &lt;HOSTIP1&gt;:6379 &lt;HOSTIP1&gt;:6380 &lt;HOSTIP1&gt;:6381 \
&lt;HOSTIP2&gt;:6379 &lt;HOSTIP2&gt;:6380 &lt;HOSTIP2&gt;:6381 \
&lt;HOSTIP3&gt;:6379 &lt;HOSTIP3&gt;:6380 &lt;HOSTIP3&gt;:6381 \
--cluster-replicas 2

&gt;&gt;&gt; Performing hash slots allocation on 9 nodes...   
Master[0] -&gt; Slots 0 - 5460                            
Master[1] -&gt; Slots 5461 - 10922                                                                    
Master[2] -&gt; Slots 10923 - 16383            
Adding replica HOSTIP2:6380 to HOSTIP1:6379
Adding replica HOSTIP3:6380 to HOSTIP1:6379
Adding replica HOSTIP1:6381 to HOSTIP2:6379
Adding replica HOSTIP3:6381 to HOSTIP2:6379
Adding replica HOSTIP2:6381 to HOSTIP3:6379        
Adding replica HOSTIP1:6380 to HOSTIP3:6379     
M: afabffee7a9076d42c9640a77ae2db6e6eb52fae HOSTIP1:6379           
   slots:[0-5460] (5461 slots) master                           
S: f24a6554ed2b64b071122bd16c7201aca1b184d0 HOSTIP1:6380                               
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c               
S: bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOSTIP1:6381
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d             
M: b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOSTIP2:6379
   slots:[5461-10922] (5462 slots) master                                   
S: 27c88c277aa82340f5e2f9d73078d59399ed6b87 HOSTIP2:6380 
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae                
S: 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOSTIP2:6381 
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c     
M: 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOSTIP3:6379     
   slots:[10923-16383] (5461 slots) master               
S: 54d6095aca3e1edd27761e080651bb28144e3a81 HOSTIP3:6380
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae
S: 9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOSTIP3:6381
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d
Can I set the above configuration? (type 'yes' to accept): yes
&gt;&gt;&gt; Nodes configuration updated
&gt;&gt;&gt; Assign a different config epoch to each node
&gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
...
&gt;&gt;&gt; Performing Cluster Check (using node HOSTIP1:6379)
M: afabffee7a9076d42c9640a77ae2db6e6eb52fae HOSTIP1:6379
   slots:[0-5460] (5461 slots) master
   2 additional replica(s)
S: bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOSTIP1:6381
   slots: (0 slots) slave
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d
S: 27c88c277aa82340f5e2f9d73078d59399ed6b87 HOSTIP2:6380
   slots: (0 slots) slave
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae
M: b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOSTIP2:6379
   slots:[5461-10922] (5462 slots) master
   2 additional replica(s)
S: 9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOSTIP3:6381
   slots: (0 slots) slave
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d
S: 54d6095aca3e1edd27761e080651bb28144e3a81 HOSTIP3:6380
   slots: (0 slots) slave
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae
S: 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOSTIP2:6381
   slots: (0 slots) slave
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c
S: f24a6554ed2b64b071122bd16c7201aca1b184d0 HOSTIP1:6380
   slots: (0 slots) slave
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c
M: 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOSTIP3:6379
   slots:[10923-16383] (5461 slots) master
   2 additional replica(s)
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.

</code></pre></div><p>重建集群：
要想删除一个集群，首先关闭Redis服务，方法如下：</p> <p>redis-cli -h 127.0.0.1 -p 7000 shutdown</p> <p>关闭所有集群上节点后，进入各个节点文件夹，删除以下文件：</p> <p>appendonly.aof
dump.rdb
nodes-7000.conf
批量删除指令如下：</p> <p><code>rm -f ./*/nodes-*.conf ./*/appendonly.aof ./*/dump.rdb</code></p> <h4 id="customize脚本"><a href="#customize脚本" class="header-anchor">#</a> customize脚本</h4> <div class="language- extra-class"><pre class="language-text"><code>REDIS_HOME=/opt/redis-5.0.5/src
pushd ${REDIS_HOME} &amp;&gt;/dev/null

while [ &quot;${1:0:1}&quot; == &quot;-&quot; ]; do
  case $1 in
    --start)
      echo &quot;Starting redis nodes...&quot;
      redis-server conf/redis6379.conf
      redis-server conf/redis6380.conf
      redis-server conf/redis6381.conf
      ;;
    --create-cluster)
    	redis-cli --cluster create HOST1:6379 HOST1:6380 HOST1:6381 ....HOST2... --cluster-replicas 2
    --kill)
      echo &quot;Stopping redis nodes...&quot;
      redis-cli -p 6379 shutdown
      redis-cli -p 6380 shutdown
      redis-cli -p 6381 shutdown
          ;;
    --clear-cache)
      masterNodes=($(./redis-cli cluster nodes | grep master | awk '{ print $2 }'))
      testArray=(${masterNodes[@]})
      echo ${testArray[0]}
      for item in &quot;${masterNodes[@]}&quot;; do
                host=&quot;$(cut -d':' -f1 &lt;&lt;&lt;$item)&quot;
                tmp=&quot;$(cut -d':' -f2 &lt;&lt;&lt;$item)&quot;
                port=&quot;$(cut -d'@' -f1 &lt;&lt;&lt;$tmp)&quot;
                redis-cli -c -h ${serverIp} -p ${port} flushall
        done
    *)
      echo &quot;usage: --start|--kill&quot;
      exit 1
      ;;
  esac
  shift
done

</code></pre></div><h3 id="_1-5-config"><a href="#_1-5-config" class="header-anchor">#</a> 1.5 Config</h3> <h4 id="common-config"><a href="#common-config" class="header-anchor">#</a> common config</h4> <ul><li>replica-read-only</li></ul> <div class="language- extra-class"><pre class="language-text"><code># Since Redis 2.6 by default replicas are read-only.
#
# Note: read only replicas are not designed to be exposed to untrusted clients
# on the internet. It's just a protection layer against misuse of the instance.
# Still a read only replica exports by default all the administrative commands
# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve
# security of read only replicas using 'rename-command' to shadow all the
# administrative / dangerous commands.
replica-read-only yes
</code></pre></div><ul><li>Allow writes only with N attached replicas</li></ul> <div class="language- extra-class"><pre class="language-text"><code># It is possible for a master to stop accepting writes if there are less than
# N replicas connected, having a lag less or equal than M seconds.
#
# The N replicas need to be in &quot;online&quot; state.
#
# The lag in seconds, that must be &lt;= the specified value, is calculated from
# the last ping received from the replica, that is usually sent every second.
#
# This option does not GUARANTEE that N replicas will accept the write, but
# will limit the window of exposure for lost writes in case not enough replicas
# are available, to the specified number of seconds.
#
# For example to require at least 3 replicas with a lag &lt;= 10 seconds use:
#
# min-replicas-to-write 3
# min-replicas-max-lag 10
#
# Setting one or the other to 0 disables the feature.
#
# By default min-replicas-to-write is set to 0 (feature disabled) and
# min-replicas-max-lag is set to 10.
</code></pre></div><h4 id="cluster-config"><a href="#cluster-config" class="header-anchor">#</a> cluster config</h4> <ul><li><p><strong>cluster-enabled <code>&lt;yes/no&gt;</code></strong>: If yes, enables Redis Cluster support in a specific Redis instance.  Otherwise the instance starts as a stand alone instance as usual.</p></li> <li><p><strong>cluster-config-file <code>&lt;filename&gt;</code></strong>: Note that despite the name of this option, this is not a user editable  configuration file, but the file where a Redis Cluster node  automatically persists the cluster configuration (the state, basically)  every time there is a change, in order to be able to re-read it at  startup. The file lists things like the other nodes in the cluster,  their state, persistent variables, and so forth. Often this file is  rewritten and flushed on disk as a result of some message reception.</p></li> <li><p><strong>cluster-node-timeout <code>&lt;milliseconds&gt;</code></strong>: The maximum amount of time a Redis Cluster node can be unavailable,  without it being considered as failing. If a master node is not  reachable for more than the specified amount of time, it will be failed  over by its slaves. This parameter controls other important things in  Redis Cluster. Notably, every node that can't reach the majority of  master nodes for the specified amount of time, will stop accepting  queries.</p></li> <li><p><strong>cluster-slave-validity-factor <code>&lt;factor&gt;</code></strong>: If set to zero, a slave will always consider itself valid, and will  therefore always try to failover a master, regardless of the amount of  time the link between the master and the slave remained disconnected. If the value is positive, a maximum disconnection time is calculated as  the <em>node timeout</em> value multiplied by the factor provided with  this option, and if the node is a slave, it will not try to start a  failover if the master link was disconnected for more than the specified amount of time. For example, if the node timeout is set to 5 seconds  and the validity factor is set to 10, a slave disconnected from the  master for more than 50 seconds will not try to failover its master.  Note that any value different than zero may result in Redis Cluster  being unavailable after a master failure if there is no slave that is  able to failover it. In that case the cluster will return to being  available only when the original master rejoins the cluster.</p></li> <li><p><strong>cluster-migration-barrier <code>&lt;count&gt;</code></strong>: Minimum number of slaves a master will remain connected with, for  another slave to migrate to a master which is no longer covered by any  slave. See the appropriate section about replica migration in this  tutorial for more information.</p> <div class="language- extra-class"><pre class="language-text"><code>
# Cluster replicas are able to migrate to orphaned masters, that are masters
# that are left without working replicas. This improves the cluster ability
# to resist to failures as otherwise an orphaned master can't be failed over
# in case of failure if it has no working replicas.
#
# Replicas migrate to orphaned masters only if there are still at least a
# given number of other working replicas for their old master. This number
# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica
# will migrate only if there is at least 1 other working replica for its master
# and so forth. It usually reflects the number of replicas you want for every
# master in your cluster.
#
# Default is 1 (replicas migrate only if their masters remain with at least
# one replica). To disable migration just set it to a very large value.
# A value of 0 can be set but is useful only for debugging and dangerous
# in production.
#
cluster-migration-barrier 1
意思是 当mater node A的replica都挂了之后，A变成了 orphaned master node，如果B现在有两个replica，那其中一个replica就可以自动migrate，变成A的replica，然后B就剩一个replica，如果B本来只有一个replica，那么B的replica就不可以migrate成为A的replica，因为barrier是1
</code></pre></div></li> <li><p><strong>cluster-require-full-coverage <code>&lt;yes/no&gt;</code></strong>: If this is set to yes, as it is by default, the cluster stops accepting writes if some percentage of the key space is not covered by any node.  If the option is set to no, the cluster will still serve queries even if only requests about a subset of keys can be processed.</p></li> <li><p><strong>cluster-allow-reads-when-down <code>&lt;yes/no&gt;</code></strong>: If this is set to no, as it is by default, a node in a Redis Cluster  will stop serving all traffic when the cluster is marked as failed,  either when a node can't reach a quorum of masters or when full coverage is not met. This prevents reading potentially inconsistent data from a  node that is unaware of changes in the cluster. This option can be set  to yes to allow reads from a node during the fail state, which is useful for applications that want to prioritize read availability but still  want to prevent inconsistent writes. It can also be used for when using  Redis Cluster with only one or two shards, as it allows the nodes to  continue serving writes when a master fails but automatic failover is  impossible.</p></li> <li><p>cluster-slave-no-failover=》cluster-replica-no-failover</p> <div class="language- extra-class"><pre class="language-text"><code># This option, when set to yes, prevents replicas from trying to failover its
# master during master failures. However the master can still perform a
# manual failover, if forced to do so.
#
# This is useful in different scenarios, especially in the case of multiple
# data center operations, where we want one side to never be promoted if not
# in the case of a total DC failure.
</code></pre></div><p>https://github.com/redis/redis/issues/3021</p></li></ul> <h3 id="_1-6-commands"><a href="#_1-6-commands" class="header-anchor">#</a> 1.6 commands</h3> <div class="language- extra-class"><pre class="language-text"><code>$ redis-cli -c -h &lt;HOSTIP&gt; -p &lt;PORT&gt;

redis&gt; AUTH 'password'
redis&gt; ACL LIST

redis&gt; KEYS 
redis&gt; KEYS &quot;&lt;PATTEN&gt;&quot;

keys vs scan:
https://stackoverflow.com/questions/32603964/scan-vs-keys-performance-in-redis
Note: When using Redis Cluster, the search is optimized for patterns that imply a single slot. If a pattern can only match keys of one slot, Redis only iterates over keys in that slot, rather than the whole database, when searching for keys matching the pattern. For example, with the pattern {a}h*llo, Redis would only try to match it with the keys in slot 15495, which hash tag {a} implies. To use pattern with hash tag, see Hash tags in the Cluster specification for more information.

redis-cli KEYS &quot;&lt;PATTEN&gt;&quot;
redis-cli --scan --pattern &quot;&lt;PATTEN&gt;&quot;

只会返回当前节点上的数据


//字符串(string)
redis&gt; GET &lt;KEY&gt;
redis&gt; SET &lt;KEY&gt; &lt;VALUE&gt;

//哈希(Hash) HSET KEY_NAME FIELD VALUE 
redis 127.0.0.1:6379&gt; HSET myhash field1 &quot;foo&quot;
OK
redis 127.0.0.1:6379&gt; HGET myhash field1
&quot;foo&quot;

redis 127.0.0.1:6379&gt; HSET website google &quot;www.g.cn&quot;       # 设置一个新域
(integer) 1

redis 127.0.0.1:6379&gt;HSET website google &quot;www.google.com&quot; # 覆盖一个旧域
(integer) 0

//列表(List) LRANGE KEY_NAME START END
redis&gt; RPUSH mylist &quot;one&quot;
(integer) 1
redis&gt; RPUSH mylist &quot;two&quot;
(integer) 2
redis&gt; RPUSH mylist &quot;three&quot;
(integer) 3
redis&gt; LRANGE mylist 0 0
1) &quot;one&quot;
redis&gt; LRANGE mylist -3 2
1) &quot;one&quot;
2) &quot;two&quot;
3) &quot;three&quot;
redis&gt; LRANGE mylist -100 100
1) &quot;one&quot;
2) &quot;two&quot;
3) &quot;three&quot;
redis&gt; LRANGE mylist 5 10
(empty list or set)
redis&gt; 

//无序集合(Set) SADD key member [member ...]
redis 127.0.0.1:6379&gt; SADD runoobkey redis
(integer) 1
redis 127.0.0.1:6379&gt; SADD runoobkey mongodb
(integer) 1
redis 127.0.0.1:6379&gt; SADD runoobkey mysql
(integer) 1
redis 127.0.0.1:6379&gt; SADD runoobkey mysql
(integer) 0
redis 127.0.0.1:6379&gt; SMEMBERS runoobkey

1) &quot;mysql&quot;
2) &quot;mongodb&quot;
3) &quot;redis&quot;

//有序集合(sorted set) ZADD KEY_NAME SCORE1 VALUE1.. SCOREN VALUEN
redis&gt; ZADD myzset 1 &quot;one&quot;
(integer) 1
redis&gt; ZADD myzset 1 &quot;uno&quot;
(integer) 1
redis&gt; ZADD myzset 2 &quot;two&quot; 3 &quot;three&quot;
(integer) 2
redis&gt; ZRANGE myzset 0 -1 WITHSCORES

ZRANGEBYSCORE myzset -inf 1

</code></pre></div><h3 id="_1-7-restore-an-rdb-file"><a href="#_1-7-restore-an-rdb-file" class="header-anchor">#</a> 1.7 Restore an RDB file</h3> <p>If you have an RDB file dump.rdb that contains the data you want you can use this file to create a new database</p> <p>Copy the dump.rdb file into the Redis working directory</p> <p>If you do not know what it is folder you can run the command <code>CONFIG get dir</code> where your Redis instance is up and running</p> <p>Start the Redis service with the redis-server</p> <p>The file dump.rdb is automatically imported.</p> <p>Connect to the database using redis-cli or any other client, to check that data have been imported. (for example SCAN)</p> <h2 id="_2-理论基础-theory"><a href="#_2-理论基础-theory" class="header-anchor">#</a> 2. 理论基础 Theory</h2> <p>学习redis源码过程笔记、问题记录，通过代码阅读熟悉分布式NOSQL数据库redis cluster集群功能、主从复制，节点扩容、槽位迁移、failover故障切换、一致性选举完整分析，对理解redis源码很有帮助  https://github.com/daniel416/Reading-and-comprehense-redis/</p> <p>https://redis.io/topics/cluster-spec</p> <p>An introduction to Redis data types and abstractions https://redis.io/topics/data-types-intro</p> <h3 id="_2-1-基本"><a href="#_2-1-基本" class="header-anchor">#</a> 2.1 基本</h3> <h4 id="replication"><a href="#replication" class="header-anchor">#</a> Replication</h4> <p>https://redis.io/topics/replication</p> <h4 id="redis-sentinel-vs-redis-cluster"><a href="#redis-sentinel-vs-redis-cluster" class="header-anchor">#</a> Redis Sentinel vs Redis Cluster</h4> <p>https://stackoverflow.com/questions/31143072/redis-sentinel-vs-clustering</p> <h3 id="_2-2-redis-cluster"><a href="#_2-2-redis-cluster" class="header-anchor">#</a> 2.2 Redis Cluster</h3> <p>Goals:</p> <ul><li><p>High performance and linear scalability up to 1000  nodes. There are no proxies, asynchronous replication is used, and no  merge operations are performed on values.</p> <p>默认是异步的replica，如果需要同步，则可利用 WAIT 命令</p></li> <li><p>Acceptable degree of write safety: the system tries (in a best-effort way) to retain all the writes originating from clients  connected with the majority of the master nodes. Usually there are small windows where acknowledged writes can be lost. Windows to lose  acknowledged writes are larger when clients are in a minority partition.</p></li> <li><p>Availability: Redis Cluster is able to survive  partitions where the majority of the master nodes are reachable and  there is at least one reachable slave for every master node that is no  longer reachable. Moreover using <em>replicas migration</em>, masters no longer replicated by any slave will receive one from a master which is covered by multiple slaves.</p></li></ul> <h4 id="cluster-gossip-protocol"><a href="#cluster-gossip-protocol" class="header-anchor">#</a> Cluster Gossip Protocol</h4> <ul><li><p>Every node maintains the following information about other nodes that it is aware of in the cluster:</p> <p>The node ID, IP and port of the node, a set of flags, what is the master of the node if it is flagged as slave, last time the node was pinged and the last time the pong was received, the current configuration epoch of the node, the link state and finally the set of hash slots served.</p></li> <li><p>Cluster Bus --- TCP PORTS</p> <p>Every Redis Cluster node requires two TCP connections open. The normal Redis TCP port used to serve clients, for example 6379, plus the port obtained by adding 10000 to the data port, so 16379 in the example.</p></li></ul> <h4 id="cluster-data-sharding"><a href="#cluster-data-sharding" class="header-anchor">#</a> Cluster DATA SHARDING</h4> <p>16384 slots, hash slot 哈希槽位（*dict大小）又称为bucket桶（不过很多地方都特指 *dictEntry为桶）,why ? https://cloud.tencent.com/developer/article/1042654
16384这个数字也不是作者随意指定的，Redis集群内部使用位图（bit map）来标志一个slot是否被占用，为了减少集群之间信息交换的大小，信息的大小被固定为2048字节
2048 bytes = 2^11 * 8 bit= 2^14 bit= 16384</p> <p>to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384.</p> <div class="language- extra-class"><pre class="language-text"><code>HASH_SLOT = CRC16(key) mod 16384
</code></pre></div><p>14 out of 16 CRC16 output bits are used (this is why there is a modulo 16384 operation in the formula above).</p> <p>Hash tag and multiple key operations
this{foo}key and another{foo}key are guaranteed to be in the same hash slot, and can be used together in a command with multiple keys as arguments
redis集群不支持模糊匹配partial match，想要模糊匹配只能对一个个server或database操作，不可以整体cluster操作，不过hash tag可以潜在解决这个问题</p> <h4 id="consitensy-guarantee"><a href="#consitensy-guarantee" class="header-anchor">#</a> Consitensy guarantee</h4> <p>Redis Cluster is not able to guarantee strong consistency. In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client.
Tradeoff between Synchronous write and Performance</p> <ul><li><p>scenario 1： Asynchronous writes</p> <p>Your client writes to the master B. =》 The master B replies OK to your client. =》The master B propagates the write to its slaves B1, B2 and B3.</p> <p>As you can see, B does not wait for an acknowledgement from B1, B2, B3 before replying to the client, since this would be a prohibitive latency penalty for Redis, so if your client writes something, B acknowledges the write, but crashes before being able to send the write to its slaves, one of the slaves (that did not receive the write) can be promoted to master, losing the write forever.</p> <p>This is very similar to what happens with most databases that are configured to flush data to disk every second, so it is a scenario you are already able to reason about because of past experiences with traditional database systems not involving distributed systems. Similarly you can improve consistency by forcing the database to flush data to disk before replying to the client, but this usually results in prohibitively low performance. That would be the equivalent of synchronous replication in the case of Redis Cluster.</p></li> <li><p>Scenario 2： synchronous writes</p> <p>Redis Cluster has support for synchronous writes when absolutely needed, implemented via the WAIT command. This makes losing writes a lot less likely. However, note that Redis Cluster does not implement strong consistency even when synchronous replication is used: it is always possible, under more complex failure scenarios, that a slave that was not able to receive the write will be elected as master.</p></li> <li><p>Scenario 3：network partition</p> <p>There is also a client, that we will call Z1.</p> <p>After a partition occurs, it is possible that in one side of the partition we have A, C, A1, B1, C1, and in the other side we have B and Z1.</p> <p>Z1 is still able to write to B, which will accept its writes. If the partition heals in a very short time, the cluster will continue normally. However, if the partition lasts enough time for B1 to be promoted to master on the majority side of the partition, the writes that Z1 has sent to B in the mean time will be lost.</p> <p>Note that there is a maximum window to the amount of writes Z1 will be able to send to B: if enough time has elapsed for the majority side of the partition to elect a slave as master, every master node in the minority side will have stopped accepting writes.</p> <p>This amount of time is a very important configuration directive of Redis Cluster, and is called the node timeout.</p> <p>After node timeout has elapsed, a master node is considered to be failing, and can be replaced by one of its replicas. Similarly, after node timeout has elapsed without a master node to be able to sense the majority of the other master nodes, it enters an error state and stops accepting writes.</p></li></ul> <h4 id="currentepoch-configepoch"><a href="#currentepoch-configepoch" class="header-anchor">#</a> currentEpoch &amp; configEpoch</h4> <ul><li><p>currentEpoch</p> <p>Redis Cluster uses a concept similar to the Raft algorithm &quot;term&quot;. In Redis Cluster the term is called epoch instead, and it is used in order to give incremental versioning to events. When multiple nodes provide  conflicting information, it becomes possible for another node to  understand which state is the most up to date.</p> <p>The <code>currentEpoch</code> is a 64 bit unsigned number.</p> <p>At node creation every Redis Cluster node, both slaves and master nodes, set the <code>currentEpoch</code> to 0.</p></li> <li><p>configEpoch</p> <p>Every master always advertises its <code>configEpoch</code> in ping and pong packets along with a bitmap advertising the set of slots it serves.</p> <p>The <code>configEpoch</code> is set to zero in masters when a new node is created.</p> <p>A new <code>configEpoch</code> is created during slave election.</p></li></ul> <p>This mechanism in Redis Cluster is called last failover wins.
When a slave fails over its master, it obtains a configuration epoch which is guaranteed to be greater than the one of its master (and more generally greater than any other configuration epoch generated previously). For example node B, which is a slave of A, may failover B with configuration epoch of 4. It will start to send heartbeat packets (the first time mass-broadcasting cluster-wide) and because of the following second rule, receivers will update their hash slot tables</p> <p>The same happens during reshardings. When a node importing a hash slot completes the import operation, its configuration epoch is incremented to make sure the change will be propagated throughout the cluster.</p> <blockquote><p>Practical example of configuration epoch usefulness during partitions
This section illustrates how the epoch concept is used to make the slave promotion process more resistant to partitions.</p></blockquote> <blockquote><ul><li>A master is no longer reachable indefinitely. The master has three slaves A, B, C.</li> <li>Slave A wins the election and is promoted to master.</li> <li>A network partition makes A not available for the majority of the cluster.</li> <li>Slave B wins the election and is promoted as master.</li> <li>A partition makes B not available for the majority of the cluster.</li> <li>The previous partition is fixed, and A is available again.
At this point B is down and A is available again with a role of master (actually UPDATE messages would reconfigure it promptly, but here we assume all UPDATE messages were lost). At the same time, slave C will try to get elected in order to fail over B. This is what happens:</li></ul></blockquote> <blockquote><p>1.C will try to get elected and will succeed, since for the majority of masters its master is actually down. It will obtain a new incremental configEpoch.
2.A will not be able to claim to be the master for its hash slots, because the other nodes already have the same hash slots associated with a higher configuration epoch (the one of B) compared to the one published by A.
3.So, all the nodes will upgrade their table to assign the hash slots to C, and the cluster will continue its operations.
https://redis.io/topics/cluster-spec</p></blockquote> <h4 id="cluster-failover-strategy-主从切换"><a href="#cluster-failover-strategy-主从切换" class="header-anchor">#</a> Cluster failover strategy 主从切换</h4> <p>集群是否工作状态可以通过 cluster info查看cluster_state</p> <p>对于一个N个master node的集群来说，如果每个master node有一个slave，总共就是2N个节点：</p> <p>1）任何一个节点挂掉或者被network partitioned away都不影响整体的工作，如果是slave挂，没有影响，如果是master挂，其replica会被选举为新的master，依然没有影响</p> <p>2）如果一个master和其slave同时挂，则cluster无法工作（实际上不会“同时”，肯定是有时间差的，可以利用replica migration提高此情况下的可用性）</p> <p>3）如果一个master挂掉，并且没有slave，集群无法工作</p> <p>4）超半数master挂掉，集群无法选举，从而无法工作</p> <p>N建议为奇数：</p> <p>比如3个master节点和4个master节点的集群相比，如果都挂了一个master节点都能选举新master节点，如果都挂了两个master节点都没法选举新master节点了，所以奇数的master节点可以节省机器资源</p> <h5 id="step-1-failure-detection"><a href="#step-1-failure-detection" class="header-anchor">#</a> Step 1: Failure detection</h5> <p><strong>PFAIL (<em>Possible failure</em>) flag:</strong></p> <p>A node flags another node with the <code>PFAIL</code> flag when the node is not reachable for more than <code>NODE_TIMEOUT</code> time. Both master and slave nodes can flag another node as <code>PFAIL</code>, regardless of its type.</p> <p><strong>FAIL flag:</strong></p> <p>The <code>PFAIL</code> flag alone is just local  information every node has about other nodes, but it is not sufficient  to trigger a slave promotion. For a node to be considered down the <code>PFAIL</code> condition needs to be escalated to a <code>FAIL</code> condition.</p> <p>A <code>PFAIL</code> condition is escalated to a <code>FAIL</code> condition when the following set of conditions are met:</p> <ul><li>Some node, that we'll call A, has another node B flagged as <code>PFAIL</code>.</li> <li>Node A collected, via gossip sections, information about the state of B from the point of view of the majority of masters in the cluster.</li> <li>The majority of masters signaled the <code>PFAIL</code> or <code>FAIL</code> condition within <code>NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT</code> time. (The validity factor is set to 2 in the current implementation, so this is just two times the <code>NODE_TIMEOUT</code> time).</li></ul> <p>If all the above conditions are true, Node A will:</p> <ul><li>Mark the node as <code>FAIL</code>.</li> <li>Send a <code>FAIL</code> message to all the reachable nodes.</li></ul> <p>Note that <em>the FAIL flag is mostly one way</em>. That is, a node can go from <code>PFAIL</code> to <code>FAIL</code>, but a <code>FAIL</code> flag can only be cleared in the following situations:</p> <ul><li>The node is already reachable and is a slave. In this case the <code>FAIL</code> flag can be cleared as slaves are not failed over.</li> <li>The node is already reachable and is a master not serving any slot. In this case the <code>FAIL</code> flag can be cleared as masters without slots do not really participate  in the cluster and are waiting to be configured in order to join the  cluster.</li> <li>The node is already reachable and is a master, but a long time (N times the <code>NODE_TIMEOUT</code>) has elapsed without any detectable slave promotion. It's better for it to rejoin the cluster and continue in this case.</li></ul> <p>However the Redis Cluster failure detection has a liveness  requirement: eventually all the nodes should agree about the state of a  given node. There are two cases that can originate from split brain  conditions. Either some minority of nodes believe the node is in <code>FAIL</code> state, or a minority of nodes believe the node is not in <code>FAIL</code> state. In both the cases eventually the cluster will have a single view of the state of a given node:</p> <p><strong>Case 1</strong>: If a majority of masters have flagged a node as <code>FAIL</code>, because of failure detection and the <em>chain effect</em> it generates, every other node will eventually flag the master as <code>FAIL</code>, since in the specified window of time enough failures will be reported.</p> <p><strong>Case 2</strong>: When only a minority of masters have flagged a node as <code>FAIL</code>, the slave promotion will not happen (as it uses a more formal algorithm that makes sure everybody knows about the promotion eventually) and  every node will clear the <code>FAIL</code> state as per the <code>FAIL</code> state clearing rules above (i.e. no promotion after N times the <code>NODE_TIMEOUT</code> has elapsed).</p> <h5 id="step-2-slave-election-and-promotion"><a href="#step-2-slave-election-and-promotion" class="header-anchor">#</a> Step 2: Slave election and promotion</h5> <p>Slave election and promotion is handled by slave nodes</p> <p>In order for a slave to promote itself to master, it needs to start  an election and win it. All the slaves for a given master can start an  election if the master is in <code>FAIL</code> state, however only one slave will win the election and promote itself to master.</p> <p>A slave starts an election when the following conditions are met:</p> <ul><li>The slave's master is in <code>FAIL</code> state.</li> <li>The master was serving a non-zero number of slots.</li> <li>The slave replication link was disconnected from the  master for no longer than a given amount of time, in order to ensure the promoted slave's data is reasonably fresh. This time is user  configurable.</li></ul> <p>step 1) slave increment its <code>currentEpoch</code> counter, and request votes from master instances.</p> <p>step 2) Request Votes: broadcasting a <code>FAILOVER_AUTH_REQUEST</code> packet to every master node of the cluster. Then it waits for a maximum time of two times the <code>NODE_TIMEOUT</code> for replies to arrive (but always for at least 2 seconds).</p> <p>A slave discards any <code>AUTH_ACK</code> replies with an epoch that is less than the <code>currentEpoch</code> at the time the vote request was sent. This ensures it doesn't count votes intended for a previous election.</p> <p>step 3) Once a master has voted for a given slave, replying positively with a <code>FAILOVER_AUTH_ACK</code>, it can no longer vote for another slave of the same master for a period of <code>NODE_TIMEOUT * 2</code>. In this period it will not be able to reply to other authorization  requests for the same master. This is not needed to guarantee safety,  but useful for preventing multiple slaves from getting elected (even if  with a different <code>configEpoch</code>) at around the same time, which is usually not wanted.</p> <p>how master votes:</p> <p>i. A master only votes a single time for a given epoch, and refuses to vote for older epochs: every master has a lastVoteEpoch  field and will refuse to vote again as long as the <code>currentEpoch</code> in the auth request packet is not greater than the lastVoteEpoch. When a master replies positively to a vote request, the lastVoteEpoch is  updated accordingly, and safely stored on disk.</p> <p>ii. A master votes for a slave only if the slave's master is flagged as <code>FAIL</code>.</p> <p>iii. Auth requests with a <code>currentEpoch</code> that is less than the master <code>currentEpoch</code> are ignored. Because of this the master reply will always have the same <code>currentEpoch</code> as the auth request. If the same slave asks again to be voted, incrementing the <code>currentEpoch</code>, it is guaranteed that an old delayed reply from the master can not be accepted for the new vote.</p> <p>step 4) Once the slave receives ACKs from the majority of masters, it wins the election. Otherwise if the majority is not reached within the period of two times <code>NODE_TIMEOUT</code> (but always at least 2 seconds), the election is aborted and a new one will be tried again after <code>NODE_TIMEOUT * 4</code> (and always at least 4 seconds).</p> <p>Once a slave wins the election, it obtains a new unique and incremental <code>configEpoch</code> which is higher than that of any other existing master. It starts  advertising itself as master in ping and pong packets, providing the set of served slots with a <code>configEpoch</code> that will win over the past ones.</p> <p>In order to speedup the reconfiguration of other nodes, a pong packet is broadcast to all the nodes of the cluster. Currently unreachable nodes  will eventually be reconfigured when they receive a ping or pong packet  from another node or will receive an <code>UPDATE</code> packet from another node if the information it publishes via heartbeat packets are detected to be out of date.</p> <p>The other nodes will detect that there is a new master serving the same slots served by the old master but with a greater <code>configEpoch</code>, and will upgrade their configuration. Slaves of the old master (or the  failed over master if it rejoins the cluster) will not just upgrade the  configuration but will also reconfigure to replicate from the new  master.</p> <h5 id="example"><a href="#example" class="header-anchor">#</a> Example</h5> <ul><li>A master is no longer reachable indefinitely. The master has three slaves A, B, C.</li> <li>Slave A wins the election and is promoted to master.</li> <li>A network partition makes A not available for the majority of the cluster.</li> <li>Slave B wins the election and is promoted as master.</li> <li>A partition makes B not available for the majority of the cluster.</li> <li>The previous partition is fixed, and A is available again.</li></ul> <p>At this point B is down and A is available again with a role of master (actually <code>UPDATE</code> messages would reconfigure it promptly, but here we assume all <code>UPDATE</code> messages were lost). At the same time, slave C will try to get elected in order to fail over B. This is what happens:</p> <ol><li>C will try to get elected and will succeed, since for  the majority of masters its master is actually down. It will obtain a  new incremental <code>configEpoch</code>.</li> <li>A will not be able to claim to be the master for its  hash slots, because the other nodes already have the same hash slots  associated with a higher configuration epoch (the one of B) compared to  the one published by A.</li> <li>So, all the nodes will upgrade their table to assign the hash slots to C, and the cluster will continue its operations.</li></ol> <h5 id="case-1-network-partition-短暂的脑裂"><a href="#case-1-network-partition-短暂的脑裂" class="header-anchor">#</a> Case 1: network partition 短暂的脑裂</h5> <p>除了Step 1提到的选举过程中的脑裂问题，选举前的读写也存在短暂的脑裂问题：</p> <p>Majority master nodes：A B</p> <p>Minority master nodes：C</p> <ol><li><p>A write may reach a master, but while the master may be able to reply to the client, the write may not be propagated to slaves via the  asynchronous replication used between master and slave nodes. If the  master dies without the write reaching the slaves, the write is lost  forever if the master is unreachable for a long enough period that one  of its slaves is promoted.</p></li> <li><p>A client with an out-of-date routing table may write to the old master  before it is converted into a slave (of the new master) by the cluster.</p></li></ol> <p>Notes：</p> <p>for a master to be failed over it must be unreachable by the majority of masters for at least <code>NODE_TIMEOUT</code>, so if the partition is fixed before that time, no writes are lost. When the partition lasts for more than <code>NODE_TIMEOUT</code>, all the writes performed in the minority side up to that point may be  lost. However the minority side of a Redis Cluster will start refusing  writes as soon as <code>NODE_TIMEOUT</code> time has elapsed without  contact with the majority, so there is a maximum window after which the  minority becomes no longer available. Hence, no writes are accepted or  lost after that time.</p> <h5 id="case-2-master-fail-slave-promote-to-master"><a href="#case-2-master-fail-slave-promote-to-master" class="header-anchor">#</a> Case 2: master fail=&gt;slave promote to master</h5> <p>A&lt;-A1</p> <p>B&lt;-B1</p> <p>C&lt;-C1</p> <p>In our example cluster with nodes A, B, C, if node B fails the cluster is not able to continue, since we no longer have a way to serve hash slots in the range 5501-11000.</p> <p>However when the cluster is created (or at a later time) we add a slave node to every master, so that the final cluster is composed of A, B, C that are master nodes, and A1, B1, C1 that are slave nodes. This way, the system is able to continue if node B fails.</p> <p>Node B1 replicates B, and B fails, the cluster will promote node B1 as the new master and will continue to operate correctly.</p> <p>However, note that if nodes B and B1 fail at the same time, Redis Cluster is not able to continue to operate.</p> <h5 id="case-3-mater-slave-both-fail-but-slave-fail-first"><a href="#case-3-mater-slave-both-fail-but-slave-fail-first" class="header-anchor">#</a> Case 3: mater &amp; slave both fail, but slave fail first</h5> <p>或者说出现orphaned master node的情况</p> <p><strong>解决方法:</strong></p> <p>replica migration，参考配置 <strong>cluster-migration-barrier <code>&lt;count&gt;</code></strong>:</p> <p>如果 <strong>cluster-migration-barrier <code>1</code></strong>，对于cluster：</p> <p>A&lt;-A1</p> <p>B&lt;-B1</p> <p>C&lt;-C1</p> <p>需要增加机器VM4，然后VM4可以有一个或两个replica，比如：</p> <p>A&lt;-A2</p> <p>C&lt;-C2</p> <p>或</p> <p>A&lt;-A2</p> <p>A&lt;-A3</p> <p>如果 B1挂掉，B就成为了 orphaned master nodes，（如果B再挂掉，就无法提供服务，simply because there is no other instance to have a copy of the hash slots the master was serving.），所以引入了replica migration，就是当B1挂掉后，因为A有A1和A2等多个replica，所以其中一个可以migration称为B的replica，这样即使B再挂掉，仍然有一个replica可以被promote成为B，可能你会问，这么麻烦，给每个master node都搞多个replica不行吗，当然可以，不过 this is expensive.</p> <h5 id="case-4-slave-of-slave-node"><a href="#case-4-slave-of-slave-node" class="header-anchor">#</a> Case 4：Slave of Slave node</h5> <p>Redis的主从关系是链式的，一个从节点也是可以拥有从节点的，</p> <p>当一个主A和从A1同时挂掉，A2被选举为新主，然后先重启A，主就会变成A2的从节点，再重启A1，A1仍然会是A的从节点，从而出现链式：A1-&gt;A-&gt;A2</p> <p>解决办法：</p> <p>cluster replicate 为A1指定主节点</p> <h5 id="case-5-网络不稳定-频繁主从切换"><a href="#case-5-网络不稳定-频繁主从切换" class="header-anchor">#</a> Case 5：网络不稳定，频繁主从切换</h5> <p>解决办法：合理修正cluster-node-timeout</p> <p>Once the slave receives ACKs from the majority of masters, it wins the election.  Otherwise if the majority is not reached within the period of two times <code>NODE_TIMEOUT</code> (but always at least 2 seconds), the election is aborted and a new one will be tried again after <code>NODE_TIMEOUT * 4</code> (and always at least 4 seconds).</p> <p>As soon as a master is in <code>FAIL</code> state, a slave waits a short period of time before trying to get elected. That delay is computed as follows:</p> <div class="language- extra-class"><pre class="language-text"><code>DELAY = 500 milliseconds + random delay between 0 and 500 milliseconds +
        SLAVE_RANK * 1000 milliseconds.
</code></pre></div><p>The fixed delay ensures that we wait for the <code>FAIL</code> state to propagate across the cluster, otherwise the slave may try to get elected while the masters are still unaware of the <code>FAIL</code> state, refusing to grant their vote.</p> <h5 id="case-6-常见现象-master-nodes-aggregate"><a href="#case-6-常见现象-master-nodes-aggregate" class="header-anchor">#</a> Case 6: 常见现象：master nodes aggregate</h5> <p>假设3台机器M1 M2 M3, 创建cluster，3个master A B C，3个slave(或者6个slave) A1 B1 C1，一般会平均分配：</p> <div class="language- extra-class"><pre class="language-text"><code>M1: A B1
M2: B C1
M3: C A1

假设M2 down，
M1: A B
M3: C A1

M2 up后，
M1: A B
M2: B1 C1
M3: C A1

可以看到M2并不会争夺回B，所以很容易推算当6个slave的情况下，极有可能，最终master节点全部跑到一台机器上
</code></pre></div><p>观点：kafka中类似的概念是topic leader和follower的分配，不同的是，当down掉的节点起来之后会抢夺回之前的topic leader，从而使得节点总是很平均，而redis不会抢夺，所以会越来越集中</p> <p>https://blog.csdn.net/zhouwenjun0820/article/details/105893144</p> <p><strong>解决办法：</strong></p> <p>参考 3.2 自动方式管理=&gt; cluster failover 进行调整</p> <div class="language- extra-class"><pre class="language-text"><code>HOST1:6379&gt; cluster nodes
afabffee7a9076d42c9640a77ae2db6e6eb52fae HOST1:6379@16379 myself,slave 27c88c277aa82340f5e2f9d73078d59399ed6b87 0 1632299474000 13 connected
9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOST3:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299477000 9 connected
27c88c277aa82340f5e2f9d73078d59399ed6b87 HOST2:6380@16380 master - 0 1632299475000 18 connected 0-5460
b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOST2:6379@16379 master - 0 1632299474000 4 connected 5461-10922
36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOST3:6379@16379 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299475000 15 connected
56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOST2:6381@16381 master - 0 1632299477133 15 connected 10923-16383
f24a6554ed2b64b071122bd16c7201aca1b184d0 HOST1:6380@16380 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299476000 15 connected
54d6095aca3e1edd27761e080651bb28144e3a81 HOST3:6380@16380 slave 27c88c277aa82340f5e2f9d73078d59399ed6b87 0 1632299476000 18 connected
bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOST1:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299476130 4 connected

HOST2:6380&gt; connect HOST1 6379
HOST1:6379&gt; cluster nodes
afabffee7a9076d42c9640a77ae2db6e6eb52fae HOST1:6379@16379 myself,master - 0 1632299972000 19 connected 0-5460
9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOST3:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299973000 9 connected
27c88c277aa82340f5e2f9d73078d59399ed6b87 HOST2:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632299974517 19 connected
b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOST2:6379@16379 master - 0 1632299972511 4 connected 5461-10922
36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOST3:6379@16379 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299971508 15 connected
56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOST2:6381@16381 master - 0 1632299974000 15 connected 10923-16383
f24a6554ed2b64b071122bd16c7201aca1b184d0 HOST1:6380@16380 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299974000 15 connected
54d6095aca3e1edd27761e080651bb28144e3a81 HOST3:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632299971000 19 connected
bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOST1:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299973514 4 connected

HOST1:6379&gt; connect HOST3 6379
HOST3:6379&gt; cluster failover
OK
HOST3:6379&gt; cluster nodes
56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOST2:6381@16381 slave 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c 0 1632300416000 20 connected
f24a6554ed2b64b071122bd16c7201aca1b184d0 HOST1:6380@16380 slave 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c 0 1632300416000 20 connected
b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOST2:6379@16379 master - 0 1632300421801 4 connected 5461-10922
afabffee7a9076d42c9640a77ae2db6e6eb52fae HOST1:6379@16379 master - 0 1632300420800 19 connected 0-5460
27c88c277aa82340f5e2f9d73078d59399ed6b87 HOST2:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632300419000 19 connected
36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOST3:6379@16379 myself,master - 0 1632300417000 20 connected 10923-16383
bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOST1:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632300419796 4 connected
54d6095aca3e1edd27761e080651bb28144e3a81 HOST3:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632300418793 19 connected
9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOST3:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632300418000 9 connected
</code></pre></div><h3 id="_2-3-sentinel"><a href="#_2-3-sentinel" class="header-anchor">#</a> 2.3 Sentinel</h3> <h3 id="_2-4-深度探索"><a href="#_2-4-深度探索" class="header-anchor">#</a> 2.4 深度探索</h3> <p><a href="https://mp.weixin.qq.com/s/MGcOl1kGuKdA7om0Ahz5IA" target="_blank" rel="noopener noreferrer">为了拿捏 Redis 数据结构，我画了 40 张图（完整版）<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://codeburst.io/a-closer-look-at-redis-dictionary-implementation-internals-3fd815aae535" target="_blank" rel="noopener noreferrer">A Closer Look at Redis Dictionary Implementation Internals<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><img src="/docs/docs_image/software/buildingblock/redis_dict1.png" alt=""></p> <p>Each Redis database has two dictionaries. The first one is used for keys with expiry date. It’s redisDb.expires, and values stored there are expiration timestamps. The other is for client values; it’s redisDb.dict.</p> <p>Each Redis dict has two hash tables. Both are implemented as a plain array; each slot, or bucket, contains a list of elements — in case of several elements’ hashes point at the same array index (this is known as a hash collision).</p> <p>In Redis, the concept of &quot;hash slots&quot; refers to how Redis Cluster distributes keys across multiple Redis instances. Here’s a breakdown to clarify:</p> <ol><li>Hash Slots in Redis Cluster:</li></ol> <p>Redis Cluster uses a concept of hash slots to determine which Redis instance (node) should store each key-value pair.
There are exactly 16384 hash slots available in Redis Cluster (2^14), numbered from 0 to 16383.
Each key is hashed to determine which hash slot it belongs to. <strong>Redis then uses this hash slot number to determine the node responsible for storing and handling operations for keys within that slot.</strong></p> <ol start="2"><li>dict vs. dictht:</li></ol> <p>In the context of Redis internals, a dict (dictionary) is a data structure used within each Redis instance to store keys and values.
dictht (dictionary hash table) is a specific implementation detail within the dict data structure in Redis.
The dictht is where the actual hash table resides that maps keys to their corresponding values within a dict.</p> <ol start="3"><li>Relationship to Hash Slots:</li></ol> <p>The number of hash slots (16384) in Redis Cluster does not directly correlate to the size of a dict or dictht in terms of memory or capacity.
Instead, hash slots are a logical division used for partitioning data across Redis nodes in a cluster setup.
Each Redis instance (node) manages its own dict, which can grow dynamically as keys and values are added.</p> <ol start="4"><li>Size of dict or dictht:</li></ol> <p>The size of a dict or dictht in Redis depends on several factors:
The number of entries (keys and values) stored within it.
The load factor of the hash table (how full it is relative to its capacity).
Redis dynamically resizes dictht as needed to maintain efficient hash table operations (like rehashing when load factor exceeds a threshold).</p> <p>In summary, the 16384 hash slots in Redis refer to how keys are distributed across nodes in a Redis Cluster, not to the size of individual dict or dictht structures within each Redis instance. Each Redis instance manages its own dict, and the dictht within it grows and shrinks dynamically based on the number of entries and other factors, but its size isn't directly tied to the number of hash slots in Redis Cluster.</p> <div class="language- extra-class"><pre class="language-text"><code>typedef struct dict {
    dictType *type;
    void *privdata;
    dictht ht[2];
    long rehashidx; /* rehashing not in progress if rehashidx == -1 */
    unsigned long iterators; /* number of iterators currently running */
} dict;

typedef struct dictht {
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigned long used;
} dictht;

typedef struct dictEntry {
    void *key;
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    struct dictEntry *next;
} dictEntry;
</code></pre></div><p>The initial size of hash table dictht is 4.{存在哈希冲突时，redis中使用拉链法解决哈希冲突，但是dictentry数组的默认大小为4，发送哈希冲突的概率极高，如果不进行扩容，会导致哈希表的时间复杂度恶化为O(logN)，所以满足一定条件时需要进行dicEntry数组的扩容} As more &amp; more keys enter into the system, the hash table size also grows. When does redis resize hash table? Redis can resize hash tables or simply rehash in following 2 scenarios:</p> <ul><li>total_elements / total_buckets = 1 and dict resize is enabled. Enabling or disabling dict resize is handled by redis internally. Redis tries to avoid rehashing when some background process runs to do some sort of heavy operation like saving the database to disk as rehashing involves movement of memory pages in heavy amount. So simply stating, when background process runs, dict resize is usually disabled otherwise enabled.</li> <li>total_elements / total_buckets &gt; 5 ( force resize ratio, forcefully resizing is done)</li></ul> <ul><li>https://kousiknath.medium.com/a-little-internal-on-redis-key-value-storage-implementation-fdf96bac7453</li> <li></li></ul> <p>注意，上面的hash table  dictEntry **table; 是可以通过rehash扩容的(任意大小)，但是dict本身(上图dict* dict)作为最外层的hash table大小是固定的</p> <p>so the dict size equals to slot size which is fixed 16384</p> <p>Yes, that's correct! In Redis, the size of the dictionary (also known as the hash table) is fixed at 16384 slots. This means that the dictionary can store up to 16384 keys before it starts to overflow and start using additional memory.</p> <p>The size of the dictionary is determined by the dict-size configuration parameter, which is set to 16384 by default. You can adjust this parameter to change the size of the dictionary, but keep in mind that increasing the size of the dictionary will use more memory.</p> <h4 id="内存优化"><a href="#内存优化" class="header-anchor">#</a> 内存优化</h4> <p><a href="https://blog.csdn.net/u011983531/article/details/79598671" target="_blank" rel="noopener noreferrer">容量评估<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
https://cloud.tencent.com/developer/article/1004898
https://www.cnblogs.com/yxhblogs/p/12713739.html</p> <h5 id="内存模型"><a href="#内存模型" class="header-anchor">#</a> 内存模型</h5> <p>Each data type in Redis has its own encoding, and most of them have several encodings for different scenarios. Even sds strings (and yes, string keys are usually sds strings) can have multiple encodings.</p> <p>Sets, sorted sets, lists and hashes use a compact &quot;ziplist&quot; encoding in memory when they are small, but move to a memory wasteful yet faster encoding when they grow.</p> <p>The most complex object is the sorted set, which is a combination of a skiplist and a hash table. And the new streams object also has a very interesting representation.</p> <p>In RDB though, they get serialized into a compact representation and not kept as they are in memory.
https://stackoverflow.com/questions/48057733/is-redis-data-stored-as-sds-or-as-objects</p> <p>Redis Ziplist https://redis.com/glossary/redis-ziplist/</p> <p>Redis automatically switches between ziplist and other data structures, such as linked lists or hash tables, based on certain criteria. The decision to use ziplists depends on factors like the number of elements and their sizes. Redis provides configuration options to control the threshold values for switching between different representations.</p> <p>conn.rpush(‘test’, ‘a’, ‘b’, ‘c’, ‘d’)
4
We start by pushing four items onto a LIST.</p> <p>conn.debug_object(‘test’)
To obtain information about a specific object, we can utilize the “debug object” command.it is important to note that for nonziplist encodings (except for the special encoding of SETs), this number does not accurately reflect the actual memory consumption.</p> <p>redisobject：
但redis大多数情况下并没有直接使用底层数据结构（sds ziplist skiplist等）来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，每个对象都包含了一种具体数据结构。比如，当redis数据库新创建一个键值对时，就需要创建一个值对象，值对象的*ptr属性指向具体的SDS字符串。</p> <h6 id="底层数据结构sting字符串容量评估"><a href="#底层数据结构sting字符串容量评估" class="header-anchor">#</a> 底层数据结构Sting字符串容量评估</h6> <p>一个简单的key-value键值对最终会产生4个消耗内存的结构，中间free掉的不考虑：</p> <p>1个dictEntry结构，24字节，负责保存具体的键值对 向上取整为32；(jemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。举个例子。如果你申请 6 字节空间，jemalloc 实际会分配 8 字节空间；如果你申请 24 字节空间，jemalloc 则会分配 32 字节。所以，在我们刚刚说的场景里，dictEntry 结构就占用了 32 字节。)
1个redisObject结构，16字节，用作val对象；
1个SDS结构，用作key字符串，占9个字节(free4个字节+len4个字节+字符串末尾”\0”1个字节)；
1个SDS结构，用作val字符串，占9个字节(free4个字节+len4个字节+字符串末尾”\0”1个字节)</p> <p>当key个数逐渐增多，redis还会以rehash的方式扩展哈希表节点数组(也就是dictEntry[]数组)，即增大哈希表的bucket个数，每个bucket元素都是个指针(dictEntry*)，占8字节，bucket个数是超过key个数向上求整的2的n次方。</p> <p>真实情况下，每个结构最终真正占用的内存还要考虑jemalloc的内存分配规则，</p> <p>jemalloc是一种通用的内存管理方法，着重于减少内存碎片和支持可伸缩的并发性，做redis容量评估前必须对jemalloc的内存分配规则有一定了解。</p> <p>jemalloc基于申请内存的大小把内存分配分为三个等级：small，large，huge：</p> <p>Small Object的size以8字节，16字节，32字节等分隔开，小于页大小；
Large Object的size以分页为单位，等差间隔排列，小于chunk的大小；
Huge Object的大小是chunk大小的整数倍。
对于64位系统，一般chunk大小为4M，页大小为4K</p> <p>综上所述，string类型的容量评估模型为：</p> <p>总内存消耗 = (dictEntry大小＋redisObject大小＋key_SDS大小＋val_SDS大小) * key个数＋bucket个数 * 8
【换算下来】
总内存消耗 = (32 + 16 + key_SDS大小＋val_SDS大小) * key个数＋bucket个数 * 8</p> <p>（1）举例说明
当key长度为 13，value长度为15，key个数为2000，根据上面总结的容量评估模型，容量预估值为 (32 + 16 + 32 + 32) * 2000 + 2048 * 8 = 240384</p> <p>（2）生产实践
用redis做商品缓存，key为商品id，value为商品信息。key大约占用30个字节，value大约占用1500个字节。
当缓存1百万商品时，容量预估值为(32 + 16 + 64 + 1536) * 1000000+ 1000000(预估) * 8 = 1656000000，约等于1.54G
总结：当value比较大时，占用的内存约等于value的大小*个数</p> <h6 id="底层数据结构哈希表容量评估"><a href="#底层数据结构哈希表容量评估" class="header-anchor">#</a> 底层数据结构哈希表容量评估</h6> <p>一个Hash存储结构最终会产生以下几个消耗内存的结构：</p> <p>1个SDS结构，用作key字符串，占9个字节(free4个字节+len4个字节+字符串末尾”\0”1个字节)；
1个dictEntry结构，24字节，负责保存当前的哈希对象；
1个redisObject结构，16字节，指向当前key下属的dict结构；
1个dict结构，88字节，负责保存哈希对象的键值对；
n个dictEntry结构，24<em>n字节，负责保存具体的field和value，n等于field个数；
n个redisObject结构，16</em>n字节，用作field对象；
n个redisObject结构，16*n字节，用作value对象；
n个SDS结构，（field长度＋9）*n字节，用作field字符串；
n个SDS结构，（value长度＋9）*n字节，用作value字符串；
因为hash类型内部有两个dict结构，所以最终会有产生两种rehash，一种rehash基准是field个数，另一种rehash基准是key个数，结合jemalloc内存分配规则，hash类型的容量评估模型为：</p> <p>总内存消耗 = [key_SDS大小 + redisObject大小 + dictEntry大小 + dict大小 +(redisObject大小 * 2 + field_SDS大小 + val_SDS大小 + dictEntry大小) * field个数 + field_bucket个数 * 指针大小] * key个数 + key_bucket个数 * 指针大小
【换算】
总内存消耗 = [ key_SDS大小 + 16 + 24 + 88 + (16 * 2 + field_SDS大小 + val_SDS大小 + 24) * field个数 + field_bucket个数 * 8] * key个数 + key_bucket个数 * 8
总内存消耗 =[128+ key_SDS大小 +(56 + field_SDS大小 + val_SDS大小 ) * field个数 + field_bucket个数 * 8] * key个数 + key_bucket个数 * 8</p> <p>生产实例
用redis做商品缓存，key为商家id，field为商品id，value为商品信息。
当有1000个key，每个key有1000个field，即总共1百万商品时，总容量跟使用key-value结构差不多，多出来几十兆的空间而已。</p> <h5 id="命令"><a href="#命令" class="header-anchor">#</a> 命令</h5> <p>info memory
memory usage
memory stats
memory doctor
memory purge</p> <h5 id="优化思路"><a href="#优化思路" class="header-anchor">#</a> 优化思路</h5> <p><a href="https://docs.redis.com/latest/ri/memory-optimizations/" target="_blank" rel="noopener noreferrer">Memory Optimization for Redis<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
Redis内存碎片通常是指Redis在内存中使用的空间并不是连续的，这是因为Redis在进行内存分配时遵循特定的内存管理策略，比如jemalloc，来减少内存碎片。</p> <p>如果您发现Redis的内存使用出现碎片问题，可能是因为您的应用程序正在进行频繁的键的添加和删除操作，这导致了内存不能被完全重用。</p> <p>解决方法：</p> <p>如果您正在频繁更换键，并且这些键的大小相似，您可以考虑使用CONFIG SET hash-max-ziplist-entries 512和CONFIG SET hash-max-ziplist-value 64这样的命令来减少哈希表的大小，从而减少内存碎片。</p> <p>如果您正在使用的是Redis的版本大于或等于4.0，您可以使用CONFIG SET activedefrag yes来启用自动内存碎片整理。</p> <p>定期手动运行MEMORY PURGE命令也可以帮助释放内存碎片。</p> <p>如果您正在使用的是Redis集群，请确保集群的配置是正确的，并且没有因为网络问题导致的数据倾斜分配。</p> <p>请注意，这些方法可能会影响Redis的性能，因此在调整配置或执行内存整理时，您应该在低峰时段进行操作，并且在生产环境中应该进行充分的测试。</p> <hr> <p>redis hash slot 内存碎片
Redis中的hash slot是一种数据分布策略，用于实现Redis集群的数据分布和负载均衡。在Redis集群中，所有的键都会根据它们的值被映射到不同的hash slot中。</p> <p>如果你在Redis集群中遇到了内存碎片的问题，这通常是因为某个或某些hash slot的内存使用并没有均衡分配。这可能是因为有些key被分配到了较小的slot，而其他的key被分配到了较大的slot，导致内存使用不均。</p> <p>解决这个问题的一种方法是通过重新分配键到不同的hash slot。Redis Cluster提供了CLUSTER REPLICATE命令，可以用来手动迁移hash slot。你可以先在新的节点上创建一个新的hash slot，然后将旧节点上的部分或全部hash slot迁移到新节点。</p> <p>下面是一个简单的例子，说明如何手动迁移hash slot：</p> <p>假设你想要迁移的hash slot是1，并且源节点是192.168.1.1:7000，目标节点是192.168.1.2:7000。</p> <p>在目标节点上创建一个新的hash slot：</p> <p>redis-cli -h 192.168.1.2 -p 7000 CLUSTER ADDSLOTS {slot}
在源节点上迁移hash slot到目标节点：</p> <p>redis-cli -h 192.168.1.1 -p 7000 CLUSTER GETKEYSINSLOT 1 100
上述命令会获取slot 1中的前100个key。然后，你可以使用MIGRATE命令将这些key迁移到目标节点：</p> <p>redis-cli -h 192.168.1.1 -p 7000 MIGRATE 192.168.1.2:7000 &quot;&quot; KEYS 100
重复这个过程，直到源节点上的slot 1为空。</p> <p>注意：在实际操作中，你可能需要停止对这些key进行写操作，并且可能需要重新配置DNS，以便客户端可以连接到新的节点。</p> <p>此外，Redis 4.0及以上版本提供了CLUSTER RELOCATE命令，可以自动迁移hash slot中的keys，但这个命令不推荐在生产环境中使用，因为它可能会导致数据丢失。</p> <p>最后，定期监控集群的内存使用情况，并对键进行合理分布，可以最大程度上避免内存碎片问题。</p> <h5 id="redis的opsforhash带来的内存空间优化"><a href="#redis的opsforhash带来的内存空间优化" class="header-anchor">#</a> redis的opsForHash带来的内存空间优化</h5> <p>https://my.oschina.net/u/2382040/blog/2236871</p> <h4 id="数据倾斜"><a href="#数据倾斜" class="header-anchor">#</a> 数据倾斜</h4> <p>reshard
https://blog.csdn.net/qq1309664161/article/details/126712760</p> <p>https://cloud.tencent.com/developer/article/1676492</p> <p>big key</p> <p>Scanning for big keys
redis-cli --bigkeys</p> <p>https://programming.vip/docs/ali-yun-redis-big-key-search-tool.html</p> <h4 id="线程安全"><a href="#线程安全" class="header-anchor">#</a> 线程安全</h4> <p>单线程，考虑是否原子操作</p> <p>Get 判断</p> <p>（时间窗口）</p> <p>Set （多线程覆盖）</p> <p>Setnx</p> <p>谈谈Redis的SETNX https://huoding.com/2015/09/14/463</p> <p>https://redis.io/commands/setnx</p> <p>https://github.com/StackExchange/StackExchange.Redis/blob/86b983496d3307903ce9bc2a3c7f207de42a0dea/StackExchange.Redis/StackExchange/Redis/RedisDatabase.cs</p> <h2 id="_3-cluster-集群管理"><a href="#_3-cluster-集群管理" class="header-anchor">#</a> 3. cluster 集群管理</h2> <h3 id="_3-1-commands-gui"><a href="#_3-1-commands-gui" class="header-anchor">#</a> 3.1 Commands&amp;GUI</h3> <p>https://redis.io/topics/rediscli</p> <ul><li><p>非集群模式命令</p> <p>比如 salveof</p> <p>否则会报错 <a href="https://stackoverflow.com/questions/62296087/replicaof-directive-not-allowed-in-cluster-mode" target="_blank" rel="noopener noreferrer">replicaof directive not allowed in cluster mode<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>集群模式命令 cluster mode</p> <p>特点是全部 cluster 开头的命令都可以，然后一部分非cluster开头的命令
注意区分 multi-key command，比如Redis cli - KEYS * not showing all keys （solution：run  on every one of the nodes: redis-cli --cluster call hostname:90001 KEYS &quot;*&quot;）</p> <div class="language- extra-class"><pre class="language-text"><code>redis-cli --cluster help
//连接
redis-cli -c -h &lt;HOSTIP&gt; -p &lt;PORT&gt;
	cluster help
	cluster info
	cluster nodes
	connect HOST PORT #Switch to other nodes
	cluster slots
	replicate NODEID
	cluster failover (on slave node)
		keys * (Check empty)
		flushdb/flushall (on master nodes)
		cluster reset(soft/hard, chang slave to an empty ‘standalone’ master)
		Slaveof no one (change slave to master, cannot execute in cluster mode)
	Slave host port (change slave to replicate another master, cannot execute in cluster mode)
	
//检查	
redis-cli --cluster check &lt;HOSTIP&gt;:&lt;PORT&gt;
//修复
redis-cli --cluster fix &lt;HOSTIP&gt;:&lt;PORT&gt;
redis-cli -h &lt;HOSTIP&gt; -p &lt;PORT&gt; cluster meet &lt;TARGETHOSTIP&gt; &lt;TARGETPORT&gt;
redis-cli cluster forget &lt;NODEID&gt; (cannot perform on itself or it’s slave)
redis-cli --cluster del-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;NODEID&gt;
//add-node默认是empty master，也可以加参数指定为slave ()
redis-cli --cluster add-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;ANOTHER HOSTIP&gt;:&lt;PORT&gt; --cluster-slave --cluster-master-id &lt;NODEID&gt;

redis-cli -p 7002 shutdown
redis-cli -p 7002 debug segfault

redis-cli --cluster reshard &lt;ANYHOSTIP&gt;:&lt;ANYPORT&gt; --cluster-yes
redis-cli --cluster reshard &lt;host&gt;:&lt;port&gt; --cluster-from &lt;node-id&gt; --cluster-to &lt;node-id&gt; --cluster-slots &lt;number of slots&gt; --cluster-yes
redis-cli --cluster rebalance --cluster-threshold 1 &lt;ANYHOSTIP&gt;:&lt;ANYPORT&gt;

client list
redis-cli INFO|grep db
redis-cli INFO|grep db|wc -l
redis-cli INFO keyspace

--More:
all keys from all nodes:
redis-cli --cluster call HOSTIP&gt;:&lt;PORT&gt; KEYS &quot;*&quot;

Database
https://stackoverflow.com/questions/50534492/redis-how-to-get-current-database-name
https://redis.io/commands/client-list

redis implement stored procedure 
https://redis.io/commands/eval

</code></pre></div></li></ul> <p><strong>GUI:</strong>
use colon as separator https://redisdesktop.com/
Dbeaver support nosql but only for enterprise edition
Optionally we can choose fastoredis https://fastoredis.com/anonim_users_downloads</p> <h3 id="_3-2-自动方式管理"><a href="#_3-2-自动方式管理" class="header-anchor">#</a> 3.2 自动方式管理</h3> <h4 id="cluster-failover"><a href="#cluster-failover" class="header-anchor">#</a> cluster failover</h4> <p>A manual failover is a special kind of failover that is usually executed when there are no actual failures, but we wish to swap the current master with one of its replicas (which is the node we send the command to), in a safe way, without any window for data loss.</p> <ol><li>The replica tells the master to stop processing queries from clients.</li> <li>The master replies to the replica with the current <em>replication offset</em>.</li> <li>The replica waits for the replication offset to match  on its side, to make sure it processed all the data from the master  before it continues.</li> <li>The replica starts a failover, obtains a new  configuration epoch from the majority of the masters, and broadcasts the new configuration.</li> <li>The old master receives the configuration update:  unblocks its clients and starts replying with redirection messages so  that they'll continue the chat with the new master.</li></ol> <p>The command behavior can be modified by two options: <strong>FORCE</strong> and <strong>TAKEOVER</strong>*（CLUSTER FAILOVER, unless the TAKEOVER option is specified, does not execute a failover synchronously, it only schedules a manual failover, bypassing the failure detection stage）:</p> <ul><li><p><strong>FORCE</strong></p> <p>If the FORCE option is given, the replica does not perform any handshake with the master, that may be not reachable, but instead just starts a failover ASAP starting from point 4. This is useful when we want to start a manual failover while the master is no longer reachable.</p></li> <li><p><strong>TAKEOVER</strong></p> <p>There are situations where this is not enough, and we want a replica to failover without any agreement with the rest of the cluster. A real world use case for this is to mass promote replicas in a different data center to masters in order to perform a data center switch, while all the masters are down or partitioned away.</p></li></ul> <p>两种场景：</p> <ul><li><p>场景1：再平衡节点，参考前面主从切换发生的所有mater nodes聚集到一个vm上的情况</p></li> <li><p>场景2：升级master 节点</p> <p><strong>consider use failover whenever make changes to master nodes</strong></p> <ol><li>Remove master node (one option is reshard the data to the other master nodes and then remove, An alternative to remove a master node is to perform a manual failover of it over one of its slaves and remove the node after it turned into a slave of the new master.)</li> <li>Before shutdown</li> <li>Upgrade master node (failover to its slave node first before upgrading, upgrade when it becomes a slave node)</li></ol> <p>Use failover rather than manual allocate replica using “REPLICATE <NODEID>”</NODEID></p> <blockquote><p>1.The replica tells the master to stop processing queries from clients.
2.The master replies to the replica with the current replication offset.
3.The replica waits for the replication offset to match on its side, to make sure it processed all the data from the master before it continues.
4.The replica starts a failover, obtains a new configuration epoch from the majority of the masters, and broadcasts the new configuration.
5.The old master receives the configuration update: unblocks its clients and starts replying with redirection messages so that they'll continue the chat with the new master.
https://redis.io/commands/cluster-failover</p></blockquote></li></ul> <h4 id="cluster-replica-migration"><a href="#cluster-replica-migration" class="header-anchor">#</a> cluster replica migration</h4> <p>参考 <strong>cluster-migration-barrier <code>&lt;count&gt;</code></strong>:</p> <h4 id="migration-between-clusters-多个集群之间的交互"><a href="#migration-between-clusters-多个集群之间的交互" class="header-anchor">#</a> migration between clusters 多个集群之间的交互</h4> <p>Assuming you have your preexisting data set split into N masters, where            N=1 if you have no preexisting sharding, the following steps are needed            in order to migrate your data set to Redis Cluster:</p> <ol><li>Stop your clients. No automatic live-migration to Redis  Cluster is currently possible. You may be able to do it orchestrating a  live migration in the context of your application / environment.</li> <li>Generate an append only file for all of your N masters  using the BGREWRITEAOF command, and waiting for the AOF file to be  completely generated.</li> <li>Save your AOF files from aof-1 to aof-N somewhere. At  this point you can stop your old instances if you wish (this is useful  since in non-virtualized deployments you often need to reuse the same  computers).</li> <li>Create a Redis Cluster composed of N masters and zero  slaves. You'll add slaves later. Make sure all your nodes are using the  append only file for persistence.</li> <li>Stop all the cluster nodes, substitute their append only file with your pre-existing append only files, aof-1 for the first  node, aof-2 for the second node, up to aof-N.</li> <li>Restart your Redis Cluster nodes with the new AOF files. They'll complain that there are keys that should not be there according to their configuration.</li> <li>Use <code>redis-cli --cluster fix</code> command in  order to fix the cluster so that keys will be migrated according to the  hash slots each node is authoritative or not.</li> <li>Use <code>redis-cli --cluster check</code> at the end to make sure your cluster is ok.</li> <li>Restart your clients modified to use a Redis Cluster aware client library.</li></ol> <p>There is an alternative way to import data from external instances to a Redis            Cluster, which is to use the <code>redis-cli --cluster import</code> command.</p> <h4 id="多个data-center之间的关系"><a href="#多个data-center之间的关系" class="header-anchor">#</a> 多个data center之间的关系</h4> <p>cluster-replica-no-failover yes 可以用来禁止其中一个data center选举 promote master</p> <h3 id="_3-3-手动方式管理"><a href="#_3-3-手动方式管理" class="header-anchor">#</a> 3.3 手动方式管理</h3> <p>以下完全是我个人实验的总结：</p> <p>如果主从都是在集群模式下（cluster-enable=yes），那么是无法使用非集群模式的命令，比如slaveof/replicaof
<strong>1.del-node/add-node</strong>
del-node如果是slave十分不推荐，因为启动时依然会记得其他节点，虽然可以forget大部分节点，但是无法forget它的master，所以再add-node会有问题，所以此时只能采用meet
Del-node如果是master，必须是空的slots，所以有两种方法：</p> <ul><li>首先要将master做reshard转移到其他master nodes，然后将其对应的slave nodes全部replicate其他的master nodes，以防重启master后，slave nodes仍然记得该master node</li> <li>先将master node变成replica node（通过cluster failover），然后将其删除</li></ul> <div class="language- extra-class"><pre class="language-text"><code>redis-cli --cluster del-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;NODEID&gt;
Redis-server conf/redis6379.conf

redis-cli --cluster add-node &lt;NEW HOSTIP&gt;:&lt;PORT&gt; &lt;ANY EXIST HOSTIP&gt;:&lt;PORT
add-node默认是master，并且没有分配任何slots，如果master node slots是空的，不会参与replica promote election

也可以加参数指定为slave
redis-cli --cluster add-node &lt;NEW HOSTIP&gt;:&lt;PORT&gt; &lt;ANY EXIST HOSTIP&gt;:&lt;PORT&gt; --cluster-slave --cluster-master-id 
</code></pre></div><p><strong>2.slaveof/replicaof</strong></p> <p>注意再cluster集群模式下，是不可以手动分配的，可以更改slave配置，cluster-enabled=no然后再尝试</p> <div class="language- extra-class"><pre class="language-text"><code>Slaveof no one (change slave to master)
Slave host port (change slave to replicate another master)
</code></pre></div><p><strong>3.reset and meet</strong>
After reset, node become a standalone master node, and then execute meet</p> <div class="language- extra-class"><pre class="language-text"><code>$redis-cli -p 6379
127.0.0.1:6379&gt; flushall
127.0.0.1:6379&gt; cluster reset
127.0.0.1:6379&gt; exit
redis-cli -h &lt;HOSTIP&gt; -p &lt;PORT&gt; cluster meet &lt;TARGETHOSTIP&gt; &lt;TARGETPORT&gt;
</code></pre></div><p><strong>4. Final step: reshard or rebalance</strong></p> <div class="language- extra-class"><pre class="language-text"><code>redis-cli --cluster reshard &lt;HOSTIP&gt;:&lt;PORT&gt; --cluster-yes
redis-cli --cluster rebalance --cluster-threshold 1 &lt;HOSTIP&gt;:&lt;PORT&gt;
</code></pre></div><p><strong>5. Allocate slave for master</strong></p> <div class="language- extra-class"><pre class="language-text"><code>成功尝试：
cluster replicate &lt;NODEID&gt;

失败尝试：
redis-cli --cluster del-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;NODEID&gt;
redis-cli cluster forget &lt;NODEID&gt;
redis-cli --cluster add-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;ANY EXIST HOSTIP&gt;:&lt;PORT&gt; --cluster-slave --cluster-master-id &lt;MASTER NODEID&gt;
https://www.jianshu.com/p/ff173ae6e478
失败原因：Failed because cannot forget itself and it’s master!
</code></pre></div><h3 id="_3-4-日常维护"><a href="#_3-4-日常维护" class="header-anchor">#</a> 3.4 日常维护</h3> <p>Read https://redis.io/topics/admin
http://antirez.com/news/96</p> <blockquote><p>Securing Redis
1.Make sure the port Redis uses to listen for connections (by default 6379 and additionally 16379 if you run Redis in cluster mode, plus 26379 for Sentinel) is firewalled, so that it is not possible to contact Redis from the outside world.
2.Use a configuration file where the bind directive is set in order to guarantee that Redis listens on only the network interfaces you are using. For example only the loopback interface (127.0.0.1) if you are accessing Redis just locally from the same computer, and so forth.
3.Use the requirepass option in order to add an additional layer of security so that clients will require to authenticate using the AUTH command.
但是要非常注意，redis服务端如果要配置requirepass，如果是cluster一定要同时配置 masterauth，否则无法replication，当然也无法failover!!!
4.Use spiped or another SSL tunneling software in order to encrypt traffic between Redis servers and Redis clients if your environment requires encryption.</p></blockquote> <blockquote><p>Disabling of specific commands
It is possible to disable commands in Redis or to rename them into an unguessable name, so that normal clients are limited to a specified set of commands.
For instance, a virtualized server provider may offer a managed Redis instance service. In this context, normal users should probably not be able to call the Redis CONFIG command to alter the configuration of the instance, but the systems that provide and remove instances should be able to do so.
In this case, it is possible to either rename or completely shadow commands from the command table. This feature is available as a statement that can be used inside the redis.conf configuration file. For example:
rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
In the above example, the CONFIG command was renamed into an unguessable name. It is also possible to completely disable it (or any other command) by renaming it to the empty string, like in the following example:
rename-command CONFIG &quot;&quot;
https://redis.io/topics/security</p></blockquote> <p><strong>Upgrade</strong>
If you are using Redis Sentinel or Redis Cluster, the simplest way in order to upgrade to newer versions, is to upgrade a slave after the other, then perform a manual fail-over in order to promote one of the upgraded replicas as master, and finally promote the last slave.</p> <p><strong>Monitor</strong>
redis-cli memory doctor
redis-cli latency doctor</p> <p>Monitor:
https://redis.io/commands/MONITOR</p> <h3 id="_3-5-删除集群"><a href="#_3-5-删除集群" class="header-anchor">#</a> 3.5 删除集群</h3> <p>关闭所有集群上节点后，进入各个节点文件夹，删除以下文件：
appendonly.aof
dump.rdb
nodes-*.conf</p> <h2 id="_4-sentinel-管理"><a href="#_4-sentinel-管理" class="header-anchor">#</a> 4. Sentinel 管理</h2> <h2 id="_5-redis操作和系统集成-integration"><a href="#_5-redis操作和系统集成-integration" class="header-anchor">#</a> 5. Redis操作和系统集成 Integration</h2> <h3 id="_5-0-redis基本数据操作"><a href="#_5-0-redis基本数据操作" class="header-anchor">#</a> 5.0 Redis基本数据操作</h3> <h4 id="data-types"><a href="#data-types" class="header-anchor">#</a> Data types</h4> <p>https://redis.io/topics/data-types</p> <p>https://redis.io/topics/data-types-intro</p> <table><thead><tr><th>数据类型</th> <th></th> <th>特性</th> <th>场景</th></tr></thead> <tbody><tr><td>String(字符串)</td> <td>二进制安全 <br>Strings are the most basic kind of Redis value. Redis Strings are binary safe, this means that a Redis string can contain any kind of data, for instance a JPEG image or a serialized Ruby object.<br>最大存储512M</td> <td></td> <td>将复杂对象序列化后存储，下面的hash只能存放key-value键值对的object</td></tr> <tr><td>Hash(字典)</td> <td>键值对集合,即编程语言中的Map类型<br>完美代表对象：the perfect data type to represent objects (e.g. A User with a number of fields like name, surname, age, and so forth)<br>节省空间（内存？）：A hash with a few fields (where few means up to one hundred or so) is stored in a way that takes very little space, so you can store millions of objects in a small Redis instance.<br>Every hash can store up to 2^32 - 1 field-value pairs (more than 4 billion).</td> <td>适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去)</td> <td>HMSET user:1000 username antirez password P1pp0 age 34 <br>HGETALL user:1000 <br>HSET user:1000 password 12345</td></tr> <tr><td>List(列表)</td> <td>链表(双向链表？)<br>Redis Lists are simply lists of strings(如果是list of object，object可以序列化为string存储), sorted by insertion order.<br>The max length of a list is 2^32 - 1 elements (4294967295, more than 4 billion of elements per list).</td> <td>增删快,提供了操作某一段元素的API</td> <td>1,最新消息排行等功能(比如朋友圈的时间线) <br>2,消息队列</td></tr> <tr><td>Set(集合)</td> <td>哈希表实现,元素不重复<br>The max number of members in a set is 2^32 - 1 (4294967295, more than 4 billion of members per set).</td> <td>1、添加、删除,查找的复杂度都是O(1) 2、为集合提供了求交集、并集、差集等操作</td> <td>1、共同好友<br> 2、利用唯一性,统计访问网站的所有独立ip <br>3、好友推荐时,根据tag求交集,大于某个阈值就可以推荐</td></tr> <tr><td>Sorted Set(有序集合)</td> <td>将Set中的元素增加一个权重参数score,元素按score有序排列</td> <td>数据插入集合时,已经进行天然排序</td> <td>1、排行榜<br> 2、带权重的消息队列</td></tr> <tr><td>Bitmaps</td> <td></td> <td></td> <td></td></tr> <tr><td>HyperLogLogs</td> <td></td> <td></td> <td></td></tr></tbody></table> <h4 id="data-operation"><a href="#data-operation" class="header-anchor">#</a> Data operation</h4> <p>我在stackoverflow上面的相关解答：</p> <p>https://stackoverflow.com/questions/46062283/what-is-the-difference-between-the-key-and-hash-key-parameters-used-in-a-redis-p/65406450#65406450</p> <blockquote><p>basically in your scenario:</p> <div class="language- extra-class"><pre class="language-text"><code>your key is: userid:store:multi_select_choices
your hashkey is: userid
and your options objects serialized into jsonRedisValue
</code></pre></div><p>in this case, you don't need to use:</p> <div class="language- extra-class"><pre class="language-text"><code>redisTemplate.opsForHash().put(key, hashKey, jsonRedisValue)
</code></pre></div><p>instead you should use:</p> <div class="language- extra-class"><pre class="language-text"><code>redisTemplate.opsForValue().put(key, jsonRedisValue)
</code></pre></div><p>here is a very good example for you to understand the scenario where opsForHash making sense:</p> <p>first you must understand that hashes in redis is perfect representation for objects, so you don't need to serialize the object, but just store the object in the format of multiple key-value pairs, like for a userid=1000, the object has properties: username/password/age, you can simply store it on redis like this:</p> <div class="language- extra-class"><pre class="language-text"><code>redisTemplate.opsForHash().put(&quot;userid:1000&quot;, &quot;username&quot;, &quot;Liu Yue&quot;)
redisTemplate.opsForHash().put(&quot;userid:1000&quot;, &quot;password&quot;, &quot;123456&quot;)
redisTemplate.opsForHash().put(&quot;userid:1000&quot;, &quot;age&quot;, &quot;32&quot;)
</code></pre></div><p>later on if you want to change the password, just do this:</p> <div class="language- extra-class"><pre class="language-text"><code>redisTemplate.opsForHash().put(&quot;userid:1000&quot;, &quot;password&quot;, &quot;654321&quot;)
</code></pre></div><p>and the corresponding cmd using redis-cli:</p> <div class="language- extra-class"><pre class="language-text"><code>HMSET userid:1000 username 'Liu Yue' password '123456' age 32
HGETALL userid:1000
1) &quot;username&quot;
2) &quot;Liu Yue&quot;
3) &quot;password&quot;
4) &quot;123456&quot;
5) &quot;age&quot;
6) &quot;32&quot;
HSET userid:1000 password '654321'
HGETALL userid:1000
1) &quot;username&quot;
2) &quot;Liu Yue&quot;
3) &quot;password&quot;
4) &quot;654321&quot;
5) &quot;age&quot;
6) &quot;32&quot;
</code></pre></div><p>I haven't explore too much the fundamental of how it implement hashes operation, but I think the difference between key and hashkey is quite obvious based on the documentation, key is just like the other redis key, normal string, hashkey is for the purpose of optimize the storage of the mutliple key-value pairs, so I guess there must be some kind of hash algorithm behind to ensure optimal memory storage and faster query and update.</p> <p>and it's well documented here:</p> <p>https://redis.io/topics/data-types</p> <p>https://redis.io/topics/data-types-intro</p></blockquote> <h4 id="ttl-key"><a href="#ttl-key" class="header-anchor">#</a> TTL key</h4> <p>Returns the remaining time to live of a key that has a timeout.</p> <h3 id="_5-1-stackexchange-redis"><a href="#_5-1-stackexchange-redis" class="header-anchor">#</a> 5.1 StackExchange.Redis</h3> <p>Driver for .net: StackExchange.Redis 1.2https://github.com/StackExchange/StackExchange.Redis
for partial matching
Where are KEYS, SCAN, FLUSHDB etc? https://github.com/StackExchange/StackExchange.Redis/blob/41f427bb5ed8c23d0992a1411d0c92667b133d8e/docs/KeysScan.md</p> <h3 id="_5-2-python"><a href="#_5-2-python" class="header-anchor">#</a> 5.2 Python</h3> <div class="language- extra-class"><pre class="language-text"><code>pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org redis-py-cluster
 from rediscluster import StrictRedisCluster
 rc = StrictRedisCluster(startup_nodes=redis_nodes, decode_responses=True)

&gt;&gt;&gt; rc.smembers(&quot;bitcoin:stg_testnet:1514360&quot;)
&gt;&gt;&gt; set([u'812853c7fe8bfa3f7d625895b3270245861f974f6ff19f8ce21317b5378be41e'])
&gt;&gt;&gt; https://github.com/Grokzen/redis-py-cluster/blob/unstable/tests/test_commands.py
</code></pre></div><h3 id="_5-3-java-spring-boot-integration"><a href="#_5-3-java-spring-boot-integration" class="header-anchor">#</a> 5.3 Java-Spring boot integration</h3> <p>https://docs.spring.io/spring-data/data-redis/docs/current/reference/html/</p> <p>spring-boot-starter-data-redis 依赖于spring-data-redis</p> <p>spring-boot-starter-data-redis 使用：https://zhuanlan.zhihu.com/p/80325707</p> <p>spring-data-redis 解析 https://juejin.im/post/5bac97606fb9a05cd8492e48</p> <p>spring-data-redis依赖jedis或Lettuce，实际上是对jedis这些客户端的封装，提供一套与客户端无关的api RedisTemplate供应用使用，从而你在从一个redis客户端切换为另一个客户端，不需要修改业务代码。</p> <p>其中spring boot 对 RedisTemplate进行了抽象（标准化），从而可以RedisTemplate可以选择不同的具体实现比如 lettuce，jedis，比如选择LettuceConnectionFactory 来作为工厂，当然可以换用 lettuce自己的 RedisClusterClient，不过使用RedisTemplate的好处就是可以随时用其他factory替换掉lettuce，比如默认的RedisConnectionFactory 。</p> <p>**序列化：**核心包是org.springframework.data.redis.serializer，想要自定义自己的序列化，实现RedisSerializer即可，默认有2种实现JdkSerializationRedisSerializer和StringRedisSerializer，RedisTemplate默认使用JdkSerializationRedisSerializer</p> <p><strong>RedisTemplate操作：</strong></p> <p>opsForValue</p> <p>opsForHash</p> <p>opsForList</p> <p>opsForSet</p> <p>opsForZSet</p> <p>Redis Cluster</p> <p>https://docs.spring.io/spring-data/data-redis/docs/current/reference/html/#cluster</p> <h2 id="_6-security-hardening"><a href="#_6-security-hardening" class="header-anchor">#</a> 6.Security hardening</h2> <p>Redis RU330课程 Redis Security 第3周学习笔记 https://blog.csdn.net/stevensxiao/article/details/113542159</p> <h2 id="troubleshooting"><a href="#troubleshooting" class="header-anchor">#</a> Troubleshooting</h2> <h3 id="err-crossslot-keys-in-request-don-t-hash-to-the-same-slot"><a href="#err-crossslot-keys-in-request-don-t-hash-to-the-same-slot" class="header-anchor">#</a> ERR CROSSSLOT Keys in request don't hash to the same slot.</h3> <p>模糊查询批量清理keys出现错误：
redis-cli KEYS &quot;<PATTEN>&quot; | xargs redis-cli DEL
redis-cli --scan --pattern &quot;<PATTEN>&quot; | xargs redis-cli DEL</PATTEN></PATTEN></p> <p>根源：</p> <div class="language- extra-class"><pre class="language-text"><code># Enter redis-cli with cluster mode
$ redis-cli -c
# Try to delete multiple keys
127.0.0.1:6379&gt;  DEL key1 key2 key3 ...
-- (error) CROSSSLOT Keys in request don't hash to the same slot
</code></pre></div><p>解决办法：</p> <div class="language- extra-class"><pre class="language-text"><code>./redis-scan-and-delete.sh:
#!/bin/bash

# Check if the required arguments are provided
if [ &quot;$#&quot; -lt 4 ]; then
    echo &quot;Usage: $0 &lt;redis_host&gt; &lt;redis_port&gt; &lt;database_number&gt; &lt;pattern&gt;&quot;
    exit 1
fi

# Set variables from arguments
REDIS_HOST=&quot;$1&quot;
REDIS_PORT=&quot;$2&quot;
DB_NUMBER=&quot;$3&quot;
PATTERN=&quot;$4&quot;

# Scan and delete keys matching the pattern
CURRENT_CURSOR=0
NEXT_CURSOR=0

while [ &quot;$CURRENT_CURSOR&quot; -ne &quot;0&quot; ]; do
    CURRENT_CURSOR=$NEXT_CURSOR

    # Scan for keys matching the pattern
    KEYS_COUNT=$($REDIS_CLI -h &quot;${REDIS_HOST}&quot; -p &quot;${REDIS_PORT}&quot; -n &quot;${DB_NUMBER}&quot; &quot;SCAN ${CURRENT_CURSOR} COUNT 1000 MATCH '${PATTERN}'&quot;)

    # Delete the scanned keys
    for KEY in $KEYS_COUNT; do
        $REDIS_CLI -h &quot;${REDIS_HOST}&quot; -p &quot;${REDIS_PORT}&quot; -n &quot;${DB_NUMBER}&quot; &quot;DEL $KEY&quot;
    done

    # Get the next cursor
    NEXT_CURSOR=$(echo &quot;$KEYS_COUNT&quot; | awk -F' ' '{print $2}')
done

./redis-scan-and-delete.sh &lt;redis_host&gt; &lt;redis_port&gt; &lt;database_number&gt; &quot;PREFIX_*&quot;
</code></pre></div><p><a href="https://medium.com/@mbh023/redis-multi-key-command-in-cluster-mode-feat-cross-slot-ec27b999f169" target="_blank" rel="noopener noreferrer">[Redis] Multi-key command in cluster mode (feat. CROSS-SLOT)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h3 id="redissystemexception"><a href="#redissystemexception" class="header-anchor">#</a> RedisSystemException</h3> <p>1.org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is io.lettuce.core.RedisException: io.lettuce.core.RedisConnectionException: DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.</p> <p>SOLUTION: redis-server redis.conf</p> <h3 id="unable-to-connect-to-redis"><a href="#unable-to-connect-to-redis" class="header-anchor">#</a> &quot;Unable to connect to Redis;</h3> <p>nested exception is io.lettuce.core.RedisException: Cannot retrieve initial cluster partitions from initial URIs [RedisURI [host='192.168.56.101', port=6379]]&quot;,
telnet result:</p> <p>SOLUTION:
CONFIG SET protected-mode no
CONFIG REWRITE</p> <h3 id="err-protocol-error-invalid-bulk-length"><a href="#err-protocol-error-invalid-bulk-length" class="header-anchor">#</a> ERR Protocol error: invalid bulk length</h3> <p>https://github.com/xetorthio/jedis/issues/1034
https://stackoverflow.com/questions/6752894/predis-protocol-error-invalid-bulk-length</p> <h3 id="timeout-issue"><a href="#timeout-issue" class="header-anchor">#</a> Timeout issue</h3> <p>Line 15222: 2018-05-16 18:23:43,536 [32] DEBUG LaxinoV2Plugin - [Debug      ]Exception :
Timeout performing GET USERREPORT:SSSLZ:LZ:20658216:2bc1af86-d47c-45b8-b552-2d0b1f2078e5, inst: 1, mgr: ExecuteSelect, err: never, queue: 4750554, qu: 0, qs: 4750554, qc: 0, wr: 0, wq: 0, in: 65536, ar: 0,
clientName: TW-SSS-UGS, serverEndpoint: 10.22.103.166:6379, keyHashSlot: 5897, IOCP: (Busy=0,Free=1000,Min=2,Max=1000), WORKER: (Busy=1,Free=32766,Min=2,Max=32767),
Local-CPU: 37.95% (Please take a look at this article for some common client-side issues that can cause timeouts:</p> <p>https://github.com/StackExchange/StackExchange.Redis/tree/master/Docs/Timeouts.md</p> <h3 id="err-node-xxxx-6379-is-not-empty"><a href="#err-node-xxxx-6379-is-not-empty" class="header-anchor">#</a> [ERR] Node XXXX:6379 is not empty.</h3> <p>Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0</p> <div class="language- extra-class"><pre class="language-text"><code>$redis-cli -p 6379
127.0.0.1:6379&gt; flushall
OK
127.0.0.1:6379&gt; cluster reset
OK
127.0.0.1:6379&gt; exit
</code></pre></div><h3 id="issue-raised-by-above-cluster-rest-the-node-become-a-standalone-node-forgot-other-nodes"><a href="#issue-raised-by-above-cluster-rest-the-node-become-a-standalone-node-forgot-other-nodes" class="header-anchor">#</a> issue raised by above ‘cluster rest’, the node become a standalone node, forgot other nodes!!!!</h3> <div class="language- extra-class"><pre class="language-text"><code>redis-cli --cluster help
redis-cli --cluster add-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;ANY EXIST HOSTIP&gt;:&lt;PORT&gt; --cluster-slave
Redis [ERR] Nodes don’t agree about configuration!
https://hzkeung.com/2018/02/25/redis-trib-check
redis-cli --cluster check &lt;HOSTIP&gt;:&lt;PORT&gt;
redis-cli -h &lt;ANY EXIST HOSTIP&gt; -p &lt;PORT&gt; cluster meet &lt;HOSTIP&gt; &lt;PORT&gt;
</code></pre></div><h3 id="promote-slave-node-to-master"><a href="#promote-slave-node-to-master" class="header-anchor">#</a> promote slave node to master</h3> <p>simply delete it and then meet or re-add it</p> <div class="language- extra-class"><pre class="language-text"><code>redis-cli --cluster del-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;NODEID&gt;

&gt;&gt;&gt; Removing node xxxx from cluster xxxxx:6379
&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...
&gt;&gt;&gt; SHUTDOWN the node.
&gt;&gt;&gt; redis-cli -h &lt;ANY EXIST HOSTIP&gt; -p 6379 cluster meet &lt;NEW HOSTIP&gt; &lt;PORT&gt;
&gt;&gt;&gt; redis-cli --cluster add-node &lt;NEW HOSTIP&gt;:&lt;PORT&gt; &lt;ANY EXIST HOSTIP&gt;:&lt;PORT&gt;
&gt;&gt;&gt; redis-cli --cluster rebalance &lt;NEW HOSTIP&gt;:&lt;PORT&gt;
</code></pre></div><h3 id="failed-delete-node"><a href="#failed-delete-node" class="header-anchor">#</a> failed delete node</h3> <p>Solv: meet then delete</p> <div class="language- extra-class"><pre class="language-text"><code>$redis-cli --cluster del-node &lt;HOSTIP&gt;:&lt;PORT&gt; &lt;NODEID&gt;

&gt;&gt;&gt; Removing node XXXXXXX from cluster XXXXX:6379
&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...
&gt;&gt;&gt; Node XXXXX:6381 replied with error:
&gt;&gt;&gt; ERR Unknown node XXXXXXXX

redis-cli -h &lt;ANY EXIST HOSTIP&gt; -p &lt;PORT&gt; cluster meet &lt;HOSTIP&gt; &lt;PORT&gt;
</code></pre></div><h3 id="redistemplate-can-not-access-the-node"><a href="#redistemplate-can-not-access-the-node" class="header-anchor">#</a> RedisTemplate can not access the node</h3> <p>1, the cluster information has been changed (add or stop nodes), RedisTemplate can not access the node</p> <ol start="2"><li>The cluster is not read and written, and the main read and write pressure is high.</li></ol> <p><strong>solve</strong>：</p> <p>method1：</p> <div class="language- extra-class"><pre class="language-text"><code>SpringBoot2.3.0或以上版本
spring.redis.timeout=60s
spring.redis.lettuce.cluster.refresh.period=60s
spring.redis.lettuce.cluster.refresh.adaptive=true
</code></pre></div><p>method2：</p> <div class="language- extra-class"><pre class="language-text"><code>@Configuration
public class RedisConfiguration {
 
 
    @Autowired
    private RedisProperties redisProperties;
 
    @Value(&quot;${redis.maxRedirects:3}&quot;)
    private int maxRedirects;
 
    @Value(&quot;${redis.refreshTime:5}&quot;)
    private int refreshTime;
 
    @Bean
    public LettuceConnectionFactory redisConnectionFactory() {
 
        RedisClusterConfiguration redisClusterConfiguration = new RedisClusterConfiguration(redisProperties.getCluster().getNodes());
 
        redisClusterConfiguration.setMaxRedirects(maxRedirects);
 
        / / Support adaptive cluster topology refresh and static refresh source
        ClusterTopologyRefreshOptions clusterTopologyRefreshOptions =  ClusterTopologyRefreshOptions.builder()
                .enablePeriodicRefresh()
                .enableAllAdaptiveRefreshTriggers()
                .refreshPeriod(Duration.ofSeconds(refreshTime))
                .build();
 
        ClusterClientOptions clusterClientOptions = ClusterClientOptions.builder()
                .topologyRefreshOptions(clusterTopologyRefreshOptions).build();
 
                 / / From the priority, read and write separation, read from the possible inconsistency, the final consistency CP
        LettuceClientConfiguration lettuceClientConfiguration = LettuceClientConfiguration.builder()
                .readFrom(ReadFrom.SLAVE_PREFERRED)
                .clientOptions(clusterClientOptions).build();
 
        return new LettuceConnectionFactory(redisClusterConfiguration, lettuceClientConfiguration);
    }
 
    @Bean
    public RedisTemplate&lt;Object, Object&gt; redisTemplate(LettuceConnectionFactory redisConnectionFactory) {
        RedisTemplate&lt;Object, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;();
        redisTemplate.setConnectionFactory(redisConnectionFactory);
        redisTemplate.setKeySerializer(new StringRedisSerializer());
        redisTemplate.setValueSerializer(new GenericJackson2JsonRedisSerializer());
        redisTemplate.afterPropertiesSet();
        return redisTemplate;
    }
 
}
 
</code></pre></div><hr> <p>refer:
<a href="https://mp.weixin.qq.com/s/m4mquHozppME92Vmb_LqIQ" target="_blank" rel="noopener noreferrer">大半夜排查bug：竟然是同事把Redis用成这鬼样子，坑了我<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://mp.weixin.qq.com/s/MGcOl1kGuKdA7om0Ahz5IA" target="_blank" rel="noopener noreferrer">为了拿捏 Redis 数据结构，我画了 40 张图（完整版）<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <div id="disqus_thread"></div></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/docs/assets/js/app.650a987d.js" defer></script><script src="/docs/assets/js/2.d450803a.js" defer></script><script src="/docs/assets/js/193.04942d8f.js" defer></script><script src="/docs/assets/js/7.be017326.js" defer></script>
  </body>
</html>
