---
sidebar: auto
sidebarDepth: 4
footer: MIT Licensed | Copyright Â© 2018-LIU YUE
---

[å›ç›®å½•](/docs/software)  ã€Šåˆ†å¸ƒå¼ç¼“å­˜redisã€‹

## 0. ç†è®ºåŸºç¡€ Theory

å­¦ä¹ redisæºç è¿‡ç¨‹ç¬”è®°ã€é—®é¢˜è®°å½•ï¼Œé€šè¿‡ä»£ç é˜…è¯»ç†Ÿæ‚‰åˆ†å¸ƒå¼NOSQLæ•°æ®åº“redis clusteré›†ç¾¤åŠŸèƒ½ã€ä¸»ä»å¤åˆ¶ï¼ŒèŠ‚ç‚¹æ‰©å®¹ã€æ§½ä½è¿ç§»ã€failoveræ•…éšœåˆ‡æ¢ã€ä¸€è‡´æ€§é€‰ä¸¾å®Œæ•´åˆ†æï¼Œå¯¹ç†è§£redisæºç å¾ˆæœ‰å¸®åŠ©  https://github.com/daniel416/Reading-and-comprehense-redis/

https://redis.io/topics/cluster-spec

An introduction to Redis data types and abstractions https://redis.io/topics/data-types-intro

### 0.1 åŸºæœ¬

#### Replication

https://redis.io/topics/replication

#### Redis Sentinel vs Redis Cluster



https://stackoverflow.com/questions/31143072/redis-sentinel-vs-clustering

### 0.2 Redis Cluster

Goals:

- High performance and linear scalability up to 1000  nodes. There are no proxies, asynchronous replication is used, and no  merge operations are performed on values.

  é»˜è®¤æ˜¯å¼‚æ­¥çš„replicaï¼Œå¦‚æœéœ€è¦åŒæ­¥ï¼Œåˆ™å¯åˆ©ç”¨ WAIT å‘½ä»¤

- Acceptable degree of write safety: the system tries (in a best-effort way) to retain all the writes originating from clients  connected with the majority of the master nodes. Usually there are small windows where acknowledged writes can be lost. Windows to lose  acknowledged writes are larger when clients are in a minority partition.

- Availability: Redis Cluster is able to survive  partitions where the majority of the master nodes are reachable and  there is at least one reachable slave for every master node that is no  longer reachable. Moreover using *replicas migration*, masters no longer replicated by any slave will receive one from a master which is covered by multiple slaves.

#### Cluster Gossip Protocol

+ Every node maintains the following information about other nodes that it is aware of in the cluster: 

  The node ID, IP and port of the node, a set of flags, what is the master of the node if it is flagged as slave, last time the node was pinged and the last time the pong was received, the current configuration epoch of the node, the link state and finally the set of hash slots served.

+ Cluster Bus --- TCP PORTS

  Every Redis Cluster node requires two TCP connections open. The normal Redis TCP port used to serve clients, for example 6379, plus the port obtained by adding 10000 to the data port, so 16379 in the example.

#### Cluster DATA SHARDING

16384 slots, hash slot å“ˆå¸Œæ§½ä½ï¼ˆ*dictå¤§å°ï¼‰åˆç§°ä¸ºbucketæ¡¶ï¼ˆä¸è¿‡å¾ˆå¤šåœ°æ–¹éƒ½ç‰¹æŒ‡ *dictEntryä¸ºæ¡¶ï¼‰,why ? https://cloud.tencent.com/developer/article/1042654
16384è¿™ä¸ªæ•°å­—ä¹Ÿä¸æ˜¯ä½œè€…éšæ„æŒ‡å®šçš„ï¼ŒRedisé›†ç¾¤å†…éƒ¨ä½¿ç”¨ä½å›¾ï¼ˆbit mapï¼‰æ¥æ ‡å¿—ä¸€ä¸ªslotæ˜¯å¦è¢«å ç”¨ï¼Œä¸ºäº†å‡å°‘é›†ç¾¤ä¹‹é—´ä¿¡æ¯äº¤æ¢çš„å¤§å°ï¼Œä¿¡æ¯çš„å¤§å°è¢«å›ºå®šä¸º2048å­—èŠ‚
2048 bytes = 2^11 * 8 bit= 2^14 bit= 16384

to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384.

```
HASH_SLOT = CRC16(key) mod 16384
```

14 out of 16 CRC16 output bits are used (this is why there is a modulo 16384 operation in the formula above).

Hash tag and multiple key operations 
this{foo}key and another{foo}key are guaranteed to be in the same hash slot, and can be used together in a command with multiple keys as arguments
redisé›†ç¾¤ä¸æ”¯æŒæ¨¡ç³ŠåŒ¹é…partial matchï¼Œæƒ³è¦æ¨¡ç³ŠåŒ¹é…åªèƒ½å¯¹ä¸€ä¸ªä¸ªserveræˆ–databaseæ“ä½œï¼Œä¸å¯ä»¥æ•´ä½“clusteræ“ä½œï¼Œä¸è¿‡hash tagå¯ä»¥æ½œåœ¨è§£å†³è¿™ä¸ªé—®é¢˜

#### Consitensy guarantee

Redis Cluster is not able to guarantee strong consistency. In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client.
Tradeoff between Synchronous write and Performance

+ scenario 1ï¼š Asynchronous writes

  Your client writes to the master B. =ã€‹ The master B replies OK to your client. =ã€‹The master B propagates the write to its slaves B1, B2 and B3.

  As you can see, B does not wait for an acknowledgement from B1, B2, B3 before replying to the client, since this would be a prohibitive latency penalty for Redis, so if your client writes something, B acknowledges the write, but crashes before being able to send the write to its slaves, one of the slaves (that did not receive the write) can be promoted to master, losing the write forever.

  This is very similar to what happens with most databases that are configured to flush data to disk every second, so it is a scenario you are already able to reason about because of past experiences with traditional database systems not involving distributed systems. Similarly you can improve consistency by forcing the database to flush data to disk before replying to the client, but this usually results in prohibitively low performance. That would be the equivalent of synchronous replication in the case of Redis Cluster.

+ Scenario 2ï¼š synchronous writes

  Redis Cluster has support for synchronous writes when absolutely needed, implemented via the WAIT command. This makes losing writes a lot less likely. However, note that Redis Cluster does not implement strong consistency even when synchronous replication is used: it is always possible, under more complex failure scenarios, that a slave that was not able to receive the write will be elected as master.

+ Scenario 3ï¼šnetwork partition

  There is also a client, that we will call Z1.

  After a partition occurs, it is possible that in one side of the partition we have A, C, A1, B1, C1, and in the other side we have B and Z1.

  Z1 is still able to write to B, which will accept its writes. If the partition heals in a very short time, the cluster will continue normally. However, if the partition lasts enough time for B1 to be promoted to master on the majority side of the partition, the writes that Z1 has sent to B in the mean time will be lost.

  Note that there is a maximum window to the amount of writes Z1 will be able to send to B: if enough time has elapsed for the majority side of the partition to elect a slave as master, every master node in the minority side will have stopped accepting writes.

  This amount of time is a very important configuration directive of Redis Cluster, and is called the node timeout.

  After node timeout has elapsed, a master node is considered to be failing, and can be replaced by one of its replicas. Similarly, after node timeout has elapsed without a master node to be able to sense the majority of the other master nodes, it enters an error state and stops accepting writes.

  

#### currentEpoch & configEpoch

+ currentEpoch

  Redis Cluster uses a concept similar to the Raft algorithm "term". In Redis Cluster the term is called epoch instead, and it is used in order to give incremental versioning to events. When multiple nodes provide  conflicting information, it becomes possible for another node to  understand which state is the most up to date.

  The `currentEpoch` is a 64 bit unsigned number.

  At node creation every Redis Cluster node, both slaves and master nodes, set the `currentEpoch` to 0.

+ configEpoch

  Every master always advertises its `configEpoch` in ping and pong packets along with a bitmap advertising the set of slots it serves.

  The `configEpoch` is set to zero in masters when a new node is created.

  A new `configEpoch` is created during slave election. 

  

This mechanism in Redis Cluster is called last failover wins.
When a slave fails over its master, it obtains a configuration epoch which is guaranteed to be greater than the one of its master (and more generally greater than any other configuration epoch generated previously). For example node B, which is a slave of A, may failover B with configuration epoch of 4. It will start to send heartbeat packets (the first time mass-broadcasting cluster-wide) and because of the following second rule, receivers will update their hash slot tables

The same happens during reshardings. When a node importing a hash slot completes the import operation, its configuration epoch is incremented to make sure the change will be propagated throughout the cluster.

>Practical example of configuration epoch usefulness during partitions
>This section illustrates how the epoch concept is used to make the slave promotion process more resistant to partitions.

> + A master is no longer reachable indefinitely. The master has three slaves A, B, C.
> + Slave A wins the election and is promoted to master.
> + A network partition makes A not available for the majority of the cluster.
> + Slave B wins the election and is promoted as master.
> + A partition makes B not available for the majority of the cluster.
> + The previous partition is fixed, and A is available again.
>   At this point B is down and A is available again with a role of master (actually UPDATE messages would reconfigure it promptly, but here we assume all UPDATE messages were lost). At the same time, slave C will try to get elected in order to fail over B. This is what happens:

> 1.C will try to get elected and will succeed, since for the majority of masters its master is actually down. It will obtain a new incremental configEpoch.
> 2.A will not be able to claim to be the master for its hash slots, because the other nodes already have the same hash slots associated with a higher configuration epoch (the one of B) compared to the one published by A.
> 3.So, all the nodes will upgrade their table to assign the hash slots to C, and the cluster will continue its operations.
> https://redis.io/topics/cluster-spec



#### Cluster failover strategy ä¸»ä»åˆ‡æ¢

é›†ç¾¤æ˜¯å¦å·¥ä½œçŠ¶æ€å¯ä»¥é€šè¿‡ cluster infoæŸ¥çœ‹cluster_state

å¯¹äºä¸€ä¸ªNä¸ªmaster nodeçš„é›†ç¾¤æ¥è¯´ï¼Œå¦‚æœæ¯ä¸ªmaster nodeæœ‰ä¸€ä¸ªslaveï¼Œæ€»å…±å°±æ˜¯2Nä¸ªèŠ‚ç‚¹ï¼š

1ï¼‰ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹æŒ‚æ‰æˆ–è€…è¢«network partitioned awayéƒ½ä¸å½±å“æ•´ä½“çš„å·¥ä½œï¼Œå¦‚æœæ˜¯slaveæŒ‚ï¼Œæ²¡æœ‰å½±å“ï¼Œå¦‚æœæ˜¯masteræŒ‚ï¼Œå…¶replicaä¼šè¢«é€‰ä¸¾ä¸ºæ–°çš„masterï¼Œä¾ç„¶æ²¡æœ‰å½±å“

2ï¼‰å¦‚æœä¸€ä¸ªmasterå’Œå…¶slaveåŒæ—¶æŒ‚ï¼Œåˆ™clusteræ— æ³•å·¥ä½œï¼ˆå®é™…ä¸Šä¸ä¼šâ€œåŒæ—¶â€ï¼Œè‚¯å®šæ˜¯æœ‰æ—¶é—´å·®çš„ï¼Œå¯ä»¥åˆ©ç”¨replica migrationæé«˜æ­¤æƒ…å†µä¸‹çš„å¯ç”¨æ€§ï¼‰

3ï¼‰å¦‚æœä¸€ä¸ªmasteræŒ‚æ‰ï¼Œå¹¶ä¸”æ²¡æœ‰slaveï¼Œé›†ç¾¤æ— æ³•å·¥ä½œ

4ï¼‰è¶…åŠæ•°masteræŒ‚æ‰ï¼Œé›†ç¾¤æ— æ³•é€‰ä¸¾ï¼Œä»è€Œæ— æ³•å·¥ä½œ

Nå»ºè®®ä¸ºå¥‡æ•°ï¼š

æ¯”å¦‚3ä¸ªmasterèŠ‚ç‚¹å’Œ4ä¸ªmasterèŠ‚ç‚¹çš„é›†ç¾¤ç›¸æ¯”ï¼Œå¦‚æœéƒ½æŒ‚äº†ä¸€ä¸ªmasterèŠ‚ç‚¹éƒ½èƒ½é€‰ä¸¾æ–°masterèŠ‚ç‚¹ï¼Œå¦‚æœéƒ½æŒ‚äº†ä¸¤ä¸ªmasterèŠ‚ç‚¹éƒ½æ²¡æ³•é€‰ä¸¾æ–°masterèŠ‚ç‚¹äº†ï¼Œæ‰€ä»¥å¥‡æ•°çš„masterèŠ‚ç‚¹å¯ä»¥èŠ‚çœæœºå™¨èµ„æº

##### Step 1: Failure detection

**PFAIL (*Possible failure*) flag:**

A node flags another node with the `PFAIL` flag when the node is not reachable for more than `NODE_TIMEOUT` time. Both master and slave nodes can flag another node as `PFAIL`, regardless of its type.

**FAIL flag:**

The `PFAIL` flag alone is just local  information every node has about other nodes, but it is not sufficient  to trigger a slave promotion. For a node to be considered down the `PFAIL` condition needs to be escalated to a `FAIL` condition.

A `PFAIL` condition is escalated to a `FAIL` condition when the following set of conditions are met:

- Some node, that we'll call A, has another node B flagged as `PFAIL`.
- Node A collected, via gossip sections, information about the state of B from the point of view of the majority of masters in the cluster.
- The majority of masters signaled the `PFAIL` or `FAIL` condition within `NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT` time. (The validity factor is set to 2 in the current implementation, so this is just two times the `NODE_TIMEOUT` time).

If all the above conditions are true, Node A will:

- Mark the node as `FAIL`.
- Send a `FAIL` message to all the reachable nodes.

Note that *the FAIL flag is mostly one way*. That is, a node can go from `PFAIL` to `FAIL`, but a `FAIL` flag can only be cleared in the following situations:

- The node is already reachable and is a slave. In this case the `FAIL` flag can be cleared as slaves are not failed over.
- The node is already reachable and is a master not serving any slot. In this case the `FAIL` flag can be cleared as masters without slots do not really participate  in the cluster and are waiting to be configured in order to join the  cluster.
- The node is already reachable and is a master, but a long time (N times the `NODE_TIMEOUT`) has elapsed without any detectable slave promotion. It's better for it to rejoin the cluster and continue in this case.

However the Redis Cluster failure detection has a liveness  requirement: eventually all the nodes should agree about the state of a  given node. There are two cases that can originate from split brain  conditions. Either some minority of nodes believe the node is in `FAIL` state, or a minority of nodes believe the node is not in `FAIL` state. In both the cases eventually the cluster will have a single view of the state of a given node:

**Case 1**: If a majority of masters have flagged a node as `FAIL`, because of failure detection and the *chain effect* it generates, every other node will eventually flag the master as `FAIL`, since in the specified window of time enough failures will be reported.

**Case 2**: When only a minority of masters have flagged a node as `FAIL`, the slave promotion will not happen (as it uses a more formal algorithm that makes sure everybody knows about the promotion eventually) and  every node will clear the `FAIL` state as per the `FAIL` state clearing rules above (i.e. no promotion after N times the `NODE_TIMEOUT` has elapsed).

##### Step 2: Slave election and promotion

Slave election and promotion is handled by slave nodes

In order for a slave to promote itself to master, it needs to start  an election and win it. All the slaves for a given master can start an  election if the master is in `FAIL` state, however only one slave will win the election and promote itself to master.

A slave starts an election when the following conditions are met:

- The slave's master is in `FAIL` state.
- The master was serving a non-zero number of slots.
- The slave replication link was disconnected from the  master for no longer than a given amount of time, in order to ensure the promoted slave's data is reasonably fresh. This time is user  configurable.



step 1) slave increment its `currentEpoch` counter, and request votes from master instances.

step 2) Request Votes: broadcasting a `FAILOVER_AUTH_REQUEST` packet to every master node of the cluster. Then it waits for a maximum time of two times the `NODE_TIMEOUT` for replies to arrive (but always for at least 2 seconds).

A slave discards any `AUTH_ACK` replies with an epoch that is less than the `currentEpoch` at the time the vote request was sent. This ensures it doesn't count votes intended for a previous election.

step 3) Once a master has voted for a given slave, replying positively with a `FAILOVER_AUTH_ACK`, it can no longer vote for another slave of the same master for a period of `NODE_TIMEOUT * 2`. In this period it will not be able to reply to other authorization  requests for the same master. This is not needed to guarantee safety,  but useful for preventing multiple slaves from getting elected (even if  with a different `configEpoch`) at around the same time, which is usually not wanted.

how master votes:

i. A master only votes a single time for a given epoch, and refuses to vote for older epochs: every master has a lastVoteEpoch  field and will refuse to vote again as long as the `currentEpoch` in the auth request packet is not greater than the lastVoteEpoch. When a master replies positively to a vote request, the lastVoteEpoch is  updated accordingly, and safely stored on disk.

ii. A master votes for a slave only if the slave's master is flagged as `FAIL`.

iii. Auth requests with a `currentEpoch` that is less than the master `currentEpoch` are ignored. Because of this the master reply will always have the same `currentEpoch` as the auth request. If the same slave asks again to be voted, incrementing the `currentEpoch`, it is guaranteed that an old delayed reply from the master can not be accepted for the new vote.

step 4) Once the slave receives ACKs from the majority of masters, it wins the election. Otherwise if the majority is not reached within the period of two times `NODE_TIMEOUT` (but always at least 2 seconds), the election is aborted and a new one will be tried again after `NODE_TIMEOUT * 4` (and always at least 4 seconds).

Once a slave wins the election, it obtains a new unique and incremental `configEpoch` which is higher than that of any other existing master. It starts  advertising itself as master in ping and pong packets, providing the set of served slots with a `configEpoch` that will win over the past ones.

In order to speedup the reconfiguration of other nodes, a pong packet is broadcast to all the nodes of the cluster. Currently unreachable nodes  will eventually be reconfigured when they receive a ping or pong packet  from another node or will receive an `UPDATE` packet from another node if the information it publishes via heartbeat packets are detected to be out of date.

The other nodes will detect that there is a new master serving the same slots served by the old master but with a greater `configEpoch`, and will upgrade their configuration. Slaves of the old master (or the  failed over master if it rejoins the cluster) will not just upgrade the  configuration but will also reconfigure to replicate from the new  master. 

##### Example

- A master is no longer reachable indefinitely. The master has three slaves A, B, C.
- Slave A wins the election and is promoted to master.
- A network partition makes A not available for the majority of the cluster.
- Slave B wins the election and is promoted as master.
- A partition makes B not available for the majority of the cluster.
- The previous partition is fixed, and A is available again.

At this point B is down and A is available again with a role of master (actually `UPDATE` messages would reconfigure it promptly, but here we assume all `UPDATE` messages were lost). At the same time, slave C will try to get elected in order to fail over B. This is what happens:

1. C will try to get elected and will succeed, since for  the majority of masters its master is actually down. It will obtain a  new incremental `configEpoch`.
2. A will not be able to claim to be the master for its  hash slots, because the other nodes already have the same hash slots  associated with a higher configuration epoch (the one of B) compared to  the one published by A.
3. So, all the nodes will upgrade their table to assign the hash slots to C, and the cluster will continue its operations.

##### Case 1: network partition çŸ­æš‚çš„è„‘è£‚

é™¤äº†Step 1æåˆ°çš„é€‰ä¸¾è¿‡ç¨‹ä¸­çš„è„‘è£‚é—®é¢˜ï¼Œé€‰ä¸¾å‰çš„è¯»å†™ä¹Ÿå­˜åœ¨çŸ­æš‚çš„è„‘è£‚é—®é¢˜ï¼š

Majority master nodesï¼šA B

Minority master nodesï¼šC

1) A write may reach a master, but while the master may be able to reply to the client, the write may not be propagated to slaves via the  asynchronous replication used between master and slave nodes. If the  master dies without the write reaching the slaves, the write is lost  forever if the master is unreachable for a long enough period that one  of its slaves is promoted. 

2) A client with an out-of-date routing table may write to the old master  before it is converted into a slave (of the new master) by the cluster.

Notesï¼š

for a master to be failed over it must be unreachable by the majority of masters for at least `NODE_TIMEOUT`, so if the partition is fixed before that time, no writes are lost. When the partition lasts for more than `NODE_TIMEOUT`, all the writes performed in the minority side up to that point may be  lost. However the minority side of a Redis Cluster will start refusing  writes as soon as `NODE_TIMEOUT` time has elapsed without  contact with the majority, so there is a maximum window after which the  minority becomes no longer available. Hence, no writes are accepted or  lost after that time.

##### Case 2: master fail=>slave promote to master

A<-A1

B<-B1

C<-C1

In our example cluster with nodes A, B, C, if node B fails the cluster is not able to continue, since we no longer have a way to serve hash slots in the range 5501-11000.

However when the cluster is created (or at a later time) we add a slave node to every master, so that the final cluster is composed of A, B, C that are master nodes, and A1, B1, C1 that are slave nodes. This way, the system is able to continue if node B fails.

Node B1 replicates B, and B fails, the cluster will promote node B1 as the new master and will continue to operate correctly.

However, note that if nodes B and B1 fail at the same time, Redis Cluster is not able to continue to operate.

##### Case 3: mater & slave both fail, but slave fail first

æˆ–è€…è¯´å‡ºç°orphaned master nodeçš„æƒ…å†µ

**è§£å†³æ–¹æ³•:**

replica migrationï¼Œå‚è€ƒé…ç½® **cluster-migration-barrier `<count>`**:

å¦‚æœ **cluster-migration-barrier `1`**ï¼Œå¯¹äºclusterï¼š

A<-A1

B<-B1

C<-C1

éœ€è¦å¢åŠ æœºå™¨VM4ï¼Œç„¶åVM4å¯ä»¥æœ‰ä¸€ä¸ªæˆ–ä¸¤ä¸ªreplicaï¼Œæ¯”å¦‚ï¼š

A<-A2

C<-C2

æˆ–

A<-A2

A<-A3

å¦‚æœ B1æŒ‚æ‰ï¼ŒBå°±æˆä¸ºäº† orphaned master nodesï¼Œï¼ˆå¦‚æœBå†æŒ‚æ‰ï¼Œå°±æ— æ³•æä¾›æœåŠ¡ï¼Œsimply because there is no other instance to have a copy of the hash slots the master was serving.ï¼‰ï¼Œæ‰€ä»¥å¼•å…¥äº†replica migrationï¼Œå°±æ˜¯å½“B1æŒ‚æ‰åï¼Œå› ä¸ºAæœ‰A1å’ŒA2ç­‰å¤šä¸ªreplicaï¼Œæ‰€ä»¥å…¶ä¸­ä¸€ä¸ªå¯ä»¥migrationç§°ä¸ºBçš„replicaï¼Œè¿™æ ·å³ä½¿Bå†æŒ‚æ‰ï¼Œä»ç„¶æœ‰ä¸€ä¸ªreplicaå¯ä»¥è¢«promoteæˆä¸ºBï¼Œå¯èƒ½ä½ ä¼šé—®ï¼Œè¿™ä¹ˆéº»çƒ¦ï¼Œç»™æ¯ä¸ªmaster nodeéƒ½æå¤šä¸ªreplicaä¸è¡Œå—ï¼Œå½“ç„¶å¯ä»¥ï¼Œä¸è¿‡ this is expensive.

##### Case 4ï¼šSlave of Slave node

Redisçš„ä¸»ä»å…³ç³»æ˜¯é“¾å¼çš„ï¼Œä¸€ä¸ªä»èŠ‚ç‚¹ä¹Ÿæ˜¯å¯ä»¥æ‹¥æœ‰ä»èŠ‚ç‚¹çš„ï¼Œ

å½“ä¸€ä¸ªä¸»Aå’Œä»A1åŒæ—¶æŒ‚æ‰ï¼ŒA2è¢«é€‰ä¸¾ä¸ºæ–°ä¸»ï¼Œç„¶åå…ˆé‡å¯Aï¼Œä¸»å°±ä¼šå˜æˆA2çš„ä»èŠ‚ç‚¹ï¼Œå†é‡å¯A1ï¼ŒA1ä»ç„¶ä¼šæ˜¯Açš„ä»èŠ‚ç‚¹ï¼Œä»è€Œå‡ºç°é“¾å¼ï¼šA1->A->A2

è§£å†³åŠæ³•ï¼š

cluster replicate ä¸ºA1æŒ‡å®šä¸»èŠ‚ç‚¹

##### Case 5ï¼šç½‘ç»œä¸ç¨³å®šï¼Œé¢‘ç¹ä¸»ä»åˆ‡æ¢

è§£å†³åŠæ³•ï¼šåˆç†ä¿®æ­£cluster-node-timeout

Once the slave receives ACKs from the majority of masters, it wins the election.  Otherwise if the majority is not reached within the period of two times `NODE_TIMEOUT` (but always at least 2 seconds), the election is aborted and a new one will be tried again after `NODE_TIMEOUT * 4` (and always at least 4 seconds).

As soon as a master is in `FAIL` state, a slave waits a short period of time before trying to get elected. That delay is computed as follows:

```
DELAY = 500 milliseconds + random delay between 0 and 500 milliseconds +
        SLAVE_RANK * 1000 milliseconds.
```

The fixed delay ensures that we wait for the `FAIL` state to propagate across the cluster, otherwise the slave may try to get elected while the masters are still unaware of the `FAIL` state, refusing to grant their vote.



##### Case 6: å¸¸è§ç°è±¡ï¼šmaster nodes aggregate 

å‡è®¾3å°æœºå™¨M1 M2 M3, åˆ›å»ºclusterï¼Œ3ä¸ªmaster A B Cï¼Œ3ä¸ªslave(æˆ–è€…6ä¸ªslave) A1 B1 C1ï¼Œä¸€èˆ¬ä¼šå¹³å‡åˆ†é…ï¼š

```
M1: A B1
M2: B C1
M3: C A1

å‡è®¾M2 downï¼Œ
M1: A B
M3: C A1

M2 upåï¼Œ
M1: A B
M2: B1 C1
M3: C A1

å¯ä»¥çœ‹åˆ°M2å¹¶ä¸ä¼šäº‰å¤ºå›Bï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“æ¨ç®—å½“6ä¸ªslaveçš„æƒ…å†µä¸‹ï¼Œææœ‰å¯èƒ½ï¼Œæœ€ç»ˆmasterèŠ‚ç‚¹å…¨éƒ¨è·‘åˆ°ä¸€å°æœºå™¨ä¸Š
```

è§‚ç‚¹ï¼škafkaä¸­ç±»ä¼¼çš„æ¦‚å¿µæ˜¯topic leaderå’Œfollowerçš„åˆ†é…ï¼Œä¸åŒçš„æ˜¯ï¼Œå½“downæ‰çš„èŠ‚ç‚¹èµ·æ¥ä¹‹åä¼šæŠ¢å¤ºå›ä¹‹å‰çš„topic leaderï¼Œä»è€Œä½¿å¾—èŠ‚ç‚¹æ€»æ˜¯å¾ˆå¹³å‡ï¼Œè€Œredisä¸ä¼šæŠ¢å¤ºï¼Œæ‰€ä»¥ä¼šè¶Šæ¥è¶Šé›†ä¸­

https://blog.csdn.net/zhouwenjun0820/article/details/105893144

**è§£å†³åŠæ³•ï¼š**

å‚è€ƒ 3.2 è‡ªåŠ¨æ–¹å¼ç®¡ç†=> cluster failover è¿›è¡Œè°ƒæ•´

```
HOST1:6379> cluster nodes
afabffee7a9076d42c9640a77ae2db6e6eb52fae HOST1:6379@16379 myself,slave 27c88c277aa82340f5e2f9d73078d59399ed6b87 0 1632299474000 13 connected
9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOST3:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299477000 9 connected
27c88c277aa82340f5e2f9d73078d59399ed6b87 HOST2:6380@16380 master - 0 1632299475000 18 connected 0-5460
b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOST2:6379@16379 master - 0 1632299474000 4 connected 5461-10922
36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOST3:6379@16379 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299475000 15 connected
56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOST2:6381@16381 master - 0 1632299477133 15 connected 10923-16383
f24a6554ed2b64b071122bd16c7201aca1b184d0 HOST1:6380@16380 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299476000 15 connected
54d6095aca3e1edd27761e080651bb28144e3a81 HOST3:6380@16380 slave 27c88c277aa82340f5e2f9d73078d59399ed6b87 0 1632299476000 18 connected
bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOST1:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299476130 4 connected

HOST2:6380> connect HOST1 6379
HOST1:6379> cluster nodes
afabffee7a9076d42c9640a77ae2db6e6eb52fae HOST1:6379@16379 myself,master - 0 1632299972000 19 connected 0-5460
9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOST3:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299973000 9 connected
27c88c277aa82340f5e2f9d73078d59399ed6b87 HOST2:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632299974517 19 connected
b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOST2:6379@16379 master - 0 1632299972511 4 connected 5461-10922
36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOST3:6379@16379 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299971508 15 connected
56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOST2:6381@16381 master - 0 1632299974000 15 connected 10923-16383
f24a6554ed2b64b071122bd16c7201aca1b184d0 HOST1:6380@16380 slave 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 0 1632299974000 15 connected
54d6095aca3e1edd27761e080651bb28144e3a81 HOST3:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632299971000 19 connected
bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOST1:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632299973514 4 connected

HOST1:6379> connect HOST3 6379
HOST3:6379> cluster failover
OK
HOST3:6379> cluster nodes
56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOST2:6381@16381 slave 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c 0 1632300416000 20 connected
f24a6554ed2b64b071122bd16c7201aca1b184d0 HOST1:6380@16380 slave 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c 0 1632300416000 20 connected
b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOST2:6379@16379 master - 0 1632300421801 4 connected 5461-10922
afabffee7a9076d42c9640a77ae2db6e6eb52fae HOST1:6379@16379 master - 0 1632300420800 19 connected 0-5460
27c88c277aa82340f5e2f9d73078d59399ed6b87 HOST2:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632300419000 19 connected
36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOST3:6379@16379 myself,master - 0 1632300417000 20 connected 10923-16383
bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOST1:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632300419796 4 connected
54d6095aca3e1edd27761e080651bb28144e3a81 HOST3:6380@16380 slave afabffee7a9076d42c9640a77ae2db6e6eb52fae 0 1632300418793 19 connected
9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOST3:6381@16381 slave b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d 0 1632300418000 9 connected
```



### 0.3 Sentinel


### 0.4 æ·±åº¦æ¢ç´¢

#### Redis åº•å±‚æ•°æ®ç»“æ„

[ä¸ºäº†æ‹¿æ Redis æ•°æ®ç»“æ„ï¼Œæˆ‘ç”»äº† 40 å¼ å›¾ï¼ˆå®Œæ•´ç‰ˆï¼‰](https://mp.weixin.qq.com/s/MGcOl1kGuKdA7om0Ahz5IA)
[A Closer Look at Redis Dictionary Implementation Internals](https://codeburst.io/a-closer-look-at-redis-dictionary-implementation-internals-3fd815aae535)

æ’å…¥ä¸€ä¸ªkey valueé”®å€¼å¯¹çš„æ—¶å€™å‘ç”Ÿå•¥ï¼š

ç¬¬ä¸€æ­¥ï¼šå†³å®šé”®å€¼å¯¹å­˜å‚¨åœ¨å“ªä¸ªé›†ç¾¤èŠ‚ç‚¹ï¼ˆé›†ç¾¤å±‚é¢ï¼‰â€‹â€‹

  â€‹è®¡ç®—æ–¹å¼ï¼šâ€‹â€‹ slot = CRC16(key) % 16384

  â€‹ç›®çš„ï¼šâ€‹â€‹ å½“å®¢æˆ·ç«¯è¦å†™å…¥ä¸€ä¸ªé”®å€¼å¯¹æ—¶ï¼ŒRedis Cluster é¦–å…ˆç”¨è¿™ä¸ªå…¬å¼è®¡ç®—å‡ºè¿™ä¸ªé”®å±äºå“ªä¸ª â€‹å“ˆå¸Œæ§½ï¼ˆSlotï¼‰â€‹ã€‚

  â€‹ç»“æœï¼šâ€‹â€‹ æ ¹æ®é›†ç¾¤çš„é…ç½®ï¼ŒçŸ¥é“äº†è¿™ä¸ªæ§½ç”±å“ªä¸ªèŠ‚ç‚¹ï¼ˆæ¯”å¦‚ Node Aï¼‰è´Ÿè´£ç®¡ç†ã€‚

  â€‹åŠ¨ä½œï¼šâ€‹â€‹ å®¢æˆ·ç«¯å°±ä¼šæŠŠè¿™ä¸ªé”®å€¼å¯¹å‘é€åˆ° Node A å»å­˜å‚¨ã€‚

  â€‹è¿™ä¸€æ­¥è§£å†³äº†â€œæ•°æ®åœ¨å“ªå°æœºå™¨ä¸Šâ€çš„é—®é¢˜ã€‚â€‹â€‹

â€‹ç¬¬äºŒæ­¥ï¼šåœ¨èŠ‚ç‚¹å†…éƒ¨ï¼Œå†³å®šé”®å€¼å¯¹æ”¾åœ¨å“ˆå¸Œè¡¨çš„å“ªä¸ªæ¡¶é‡Œï¼ˆå•æœºå†…å­˜å±‚é¢ï¼‰â€‹â€‹

  ç°åœ¨ï¼Œé”®å€¼å¯¹å·²ç»åˆ°è¾¾äº†æ­£ç¡®çš„èŠ‚ç‚¹ï¼ˆæ¯”å¦‚ Node Aï¼‰ã€‚

  Node A ä¼šä½¿ç”¨å®ƒè‡ªå·±çš„ã€æœ¬åœ°çš„å“ˆå¸Œè¡¨ï¼ˆå°±æ˜¯æˆ‘ä»¬ä¹‹å‰è®²çš„ dictã€dicthtã€dictEntryç»“æ„ï¼‰æ¥å­˜å‚¨è¿™ä¸ªé”®å€¼å¯¹ã€‚

  â€‹è®¡ç®—æ–¹å¼ï¼šâ€‹â€‹ index = hashFunction(key) & sizemask

  hashFunctionæ˜¯ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼ˆå¦‚ SipHashï¼‰ã€‚

  sizemaskæ˜¯å“ˆå¸Œè¡¨(tableæ•°ç»„çš„é•¿åº¦)å¤§å° size - 1ã€‚å› ä¸º sizeæ€»æ˜¯ 2 çš„ n æ¬¡å¹‚ï¼Œæ‰€ä»¥ sizemaskçš„äºŒè¿›åˆ¶å½¢å¼å…¨æ˜¯ 1ï¼Œ& sizemaskæ“ä½œç­‰ä»·äº % sizeï¼Œä½†ä½è¿ç®—æ•ˆç‡æ›´é«˜ã€‚

  â€‹ç›®çš„ï¼šâ€‹â€‹ è¿™ä¸ªè®¡ç®—æ˜¯ä¸ºäº†ç¡®å®šè¿™ä¸ªé”®å€¼å¯¹åº”è¯¥æ”¾åœ¨ Node A æœ¬åœ°å“ˆå¸Œè¡¨çš„ tableæ•°ç»„ä¸­çš„å“ªä¸ªä½ç½®ï¼ˆå“ªä¸ªâ€œæ¡¶â€é‡Œï¼‰ã€‚

  â€‹ç»“æœï¼šâ€‹â€‹ å¾—åˆ°ç´¢å¼• indexï¼Œç„¶åå°†é”®å€¼å¯¹ï¼ˆdictEntryï¼‰æŒ‚åˆ°è¿™ä¸ªæ¡¶å¯¹åº”çš„é“¾è¡¨ä¸­ã€‚

  â€‹è¿™ä¸€æ­¥è§£å†³äº†â€œæ•°æ®åœ¨è¿™ä¸ªèŠ‚ç‚¹çš„å†…å­˜çš„å“ªä¸ªä½ç½®â€çš„é—®é¢˜ã€‚â€‹

##### 1. Hash Slots in Redis Cluster:

In Redis, the concept of "hash slots" refers to how Redis Cluster distributes keys across multiple Redis instances. Hereâ€™s a breakdown to clarify:
Redis Cluster uses a concept of hash slots to determine which Redis instance (node) should store each key-value pair.
There are exactly 16384 hash slots available in Redis Cluster (2^14), numbered from 0 to 16383.
Each key is hashed to determine which hash slot it belongs to. **Redis then uses this hash slot number to determine the node responsible for storing and handling operations for keys within that slot.**

Redis Cluster does maintain an in-memory mapping of all hash slots â†’ cluster nodes.
Each node (master or replica) keeps this mapping locally â€” not just the master node.
Thatâ€™s how every node can redirect a client to the correct node (MOVED or ASK replies).

Redis implements cluster logic in the file:
ğŸ“„ src/cluster.c and header src/cluster.h
```
// clusterNode Represents a single node in the cluster (could be this node itself, or another peer).
typedef struct clusterNode {
    mstime_t ctime;          /* Node object creation time. */
    char name[CLUSTER_NAMELEN]; /* Node name (40 char hex + null term). */
    int flags;               /* Node type and state flags. */
    uint64_t configEpoch;    /* Last configEpoch for this node. */
    unsigned char slots[CLUSTER_SLOTS/8]; /* Bitmap of assigned slots. */
    int numslots;            /* How many slots this node owns. */
    ...
} clusterNode;

CLUSTER_SLOTS is defined as 16384
slots[] is a bitmask â€” one bit per hash slot, so each node knows which slots it owns.

//This is the global structure representing the clusterâ€™s state from the perspective of this node.
typedef struct clusterState {
    clusterNode *myself;                /* Pointer to this node */
    uint64_t currentEpoch;
    int state;                          /* CLUSTER_OK, CLUSTER_FAIL, etc. */
    clusterNode *slots[CLUSTER_SLOTS];  /* slot -> node mapping table */
    dict *nodes;                        /* Hash table of all known nodes */
    ...
} clusterState;

clusterNode *slots[CLUSTER_SLOTS]; is a direct array of 16,384 pointers, one per slot, each pointing to the clusterNode that currently owns that slot.

So for example:
clusterState.slots[0]   --> clusterNode A
clusterState.slots[1]   --> clusterNode A
clusterState.slots[5461] --> clusterNode B
clusterState.slots[10923] --> clusterNode C


When Redis receives a key, it computes:

hash_slot = keyHashSlot(key);  // returns value in 0..16383
owner = server.cluster->slots[hash_slot];

If owner == myself â†’ handle locally
Else â†’ reply MOVED <slot> <host:port>
```
##### 2. dict vs. dictht:

Redis ä½¿ç”¨ä¸€ä¸ªéå¸¸é«˜æ•ˆçš„å“ˆå¸Œè¡¨ç»“æ„æ¥å­˜å‚¨é”®å€¼å¯¹ã€‚å®ƒä¸»è¦ç”±ä¸‰ä¸ªæ ¸å¿ƒç»“æ„ä½“ç»„æˆï¼šdictã€dicthtã€dictEntryã€‚ä¸‰è€…çš„å…³ç³»å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ•°æ®åº“ > è¡¨ > è¡Œçš„å…³ç³»ã€‚
keyå’Œ vä¿å­˜äº†å®é™…çš„é”®å’Œå€¼ã€‚

![](/docs/docs_image/software/buildingblock/redis_dict1.png)

Each Redis database has two dictionaries. The first one is used for keys with expiry date. Itâ€™s redisDb.expires, and values stored there are expiration timestamps. The other is for client values; itâ€™s redisDb.dict.

Each Redis dict has two hash tables. Both are implemented as a plain array; each slot, or bucket, contains a list of elements â€” in case of several elementsâ€™ hashes point at the same array index (this is known as a hash collision). 

In the context of Redis internals, a dict (dictionary) is a data structure used within each Redis instance to store keys and values.
dictht (dictionary hash table) is a specific implementation detail within the dict data structure in Redis.
The dictht is where the actual hash table resides that maps keys to their corresponding values within a dict.

Note: 
  The number of hash slots (16384) in Redis Cluster does not directly correlate to the size of a dict or dictht in terms of memory or capacity.
  Instead, hash slots are a logical division used for partitioning data across Redis nodes in a cluster setup.
  Each Redis instance (node) manages its own dict, which can grow dynamically as keys and values are added.
  In summary, the 16384 hash slots in Redis refer to how keys are distributed across nodes in a Redis Cluster, not to the size of individual dict or dictht structures within each Redis instance. Each Redis instance manages its own dict, and the dictht within it grows and shrinks dynamically based on the number of entries and other factors, but its size isn't directly tied to the number of hash slots in Redis Cluster.


```
struct redisDb {
    dict *dict;     // all key-value pairs (main DB)
    dict *expires;  // key expiration times
    ...
};

typedef struct dict {
    dictType *type;
    void *privdata;
    dictht ht[2];
    long rehashidx; /* rehashing not in progress if rehashidx == -1 */
    unsigned long iterators; /* number of iterators currently running */
} dict;

typedef struct dictht {
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigned long used;
} dictht;

typedef struct dictEntry {
    void *key;
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    struct dictEntry *next; 
} dictEntry;

nextæŒ‡é’ˆæ˜¯ä¸ºäº†è§£å†³å“ˆå¸Œå†²çªã€‚å½“ä¸¤ä¸ªä¸åŒçš„é”®é€šè¿‡å“ˆå¸Œå‡½æ•°è®¡ç®—å‡ºçš„ç´¢å¼•ç›¸åŒæ—¶ï¼ˆå“ˆå¸Œå†²çªï¼‰ï¼Œå®ƒä»¬ä¼šé€šè¿‡è¿™ä¸ª nextæŒ‡é’ˆè¢«è¿æ¥æˆä¸€ä¸ªå•å‘é“¾è¡¨ã€‚è¿™ç§æ–¹å¼ç§°ä¸ºâ€œé“¾åœ°å€æ³•â€ã€‚

```

##### 3. Size of dict or dictht:
```
dict *d
  â†“
  +--------------------------------------------------+
  | ht[0] : dictht                                   |
  | ht[1] : dictht (for rehashing)                   |
  +--------------------------------------------------+

dictht ht[0]
  â†“
  +---------------------------------------------+
  | table : dictEntry ** (array of bucket heads)|
  | size, sizemask, used                        |
  +---------------------------------------------+

table (array of dictEntry*)
  â†“
  +------------+------------+------------+
  | bucket[0]  | bucket[1]  | bucket[2]  | ...
  +------------+------------+------------+
        â†“            â†“            â†“
      linked list  linked list  linked list


d (dict)
 â”œâ”€â”€ ht[0].table  ---> array of dictEntry* (length = size)
 â”‚       â”œâ”€â”€ table[0] -> dictEntry(key="foo")
 â”‚       â”œâ”€â”€ table[1] -> NULL
 â”‚       â”œâ”€â”€ table[2] -> dictEntry(key="bar") â†’ dictEntry(key="baz")
 â”‚       â””â”€â”€ ...
 â””â”€â”€ ht[1].table  ---> NULL (if not rehashing)

```

Note:
When people casually say â€œdict size,â€ they could mean:
+ In C terms, â€œsizeâ€ normally means sizeof(dict) â†’ a compile-time constant (e.g. 48 bytes)
+ In Redis/algorithm terms, â€œsizeâ€ usually means the number of keys stored (like the size of a map in high-level languages),they could mean:
  - â€œthe number of keys storedâ€ â†’ (ht[0].used + ht[1].used)
  - or â€œthe hash tableâ€™s current capacityâ€ â†’ (ht[0].size)

The size of a dict or dictht in Redis depends on several factors:
The number of entries (keys and values) stored within it.
The load factor of the hash table (how full it is relative to its capacity).
Redis dynamically resizes dictht as needed to maintain efficient hash table operations (like rehashing when load factor exceeds a threshold).

The initial size of hash table dictht is 4.{å­˜åœ¨å“ˆå¸Œå†²çªæ—¶ï¼Œredisä¸­ä½¿ç”¨æ‹‰é“¾æ³•è§£å†³å“ˆå¸Œå†²çªï¼Œä½†æ˜¯dictentryæ•°ç»„çš„é»˜è®¤å¤§å°ä¸º4ï¼Œå‘é€å“ˆå¸Œå†²çªçš„æ¦‚ç‡æé«˜ï¼Œå¦‚æœä¸è¿›è¡Œæ‰©å®¹ï¼Œä¼šå¯¼è‡´å“ˆå¸Œè¡¨çš„æ—¶é—´å¤æ‚åº¦æ¶åŒ–ä¸ºO(logN)ï¼Œæ‰€ä»¥æ»¡è¶³ä¸€å®šæ¡ä»¶æ—¶éœ€è¦è¿›è¡ŒdicEntryæ•°ç»„çš„æ‰©å®¹} As more & more keys enter into the system, the hash table size also grows. When does redis resize hash table? Redis can resize hash tables or simply [rehash in following 2 scenarios:](https://kousiknath.medium.com/a-little-internal-on-redis-key-value-storage-implementation-fdf96bac7453)

+ total_elements / total_buckets = 1 and dict resize is enabled. 
  Enabling or disabling dict resize is handled by redis internally. Redis tries to avoid rehashing when some background process runs to do some sort of heavy operation like saving the database to disk as rehashing involves movement of memory pages in heavy amount. So simply stating, when background process runs, dict resize is usually disabled otherwise enabled.
+ total_elements / total_buckets > 5 ( force resize ratio, forcefully resizing is done)

rehashçš„è¿‡ç¨‹å°±æ˜¯ht[0]å’Œ ht[1]ä¹‹é—´æ•°æ®æ¬æ¥æ¬å»çš„è¿‡ç¨‹ï¼Œæ¯”å¦‚ht[0]è§¦å‘äº†rehashï¼Œæ‰©ht[1]ï¼ŒæŠŠht[0]æ¬è¿‡æ¥ï¼Œæ¥ç€ht[1]è§¦å‘äº†ï¼Œç±»ä¼¼çš„æ‰©ht[0] 

Rehashçš„ä¸€ä¸ªæ ¸å¿ƒç›®çš„ï¼šå‡å°‘å“ˆå¸Œå†²çªï¼Œæé«˜æ€§èƒ½ã€‚â€‹

  index = hashFunction(key) & sizemask

  hashFunction(key)ï¼šâ€‹è¿™ä¸ªå€¼å¯¹äºåŒä¸€ä¸ª keyæ˜¯æ°¸è¿œä¸å˜çš„ã€‚æ¯”å¦‚å¯¹ key1è®¡ç®—å‡ºçš„å“ˆå¸Œå€¼å§‹ç»ˆæ˜¯ 123456789ã€‚

  å˜åŒ–çš„æ˜¯ sizemaskï¼šå› ä¸º sizemask = size - 1ï¼Œè€Œ sizeåœ¨æ‰©å®¹åå˜å¤§äº†ã€‚

  ä¸¾ä¾‹è¯´æ˜ï¼šâ€‹â€‹

  å‡è®¾å½“å‰å“ˆå¸Œè¡¨ ht[0]çš„ size = 4ï¼Œé‚£ä¹ˆ sizemask = 3ï¼ˆäºŒè¿›åˆ¶ ...0011ï¼‰ã€‚

  è®¡ç®— key1çš„ä½ç½®ï¼šhash1 & 0011-> å‡è®¾ç»“æœæ˜¯ ...0011ï¼ˆåè¿›åˆ¶ 3ï¼‰ã€‚

  è®¡ç®— key2çš„ä½ç½®ï¼šhash2 & 0011-> å‡è®¾ç»“æœä¹Ÿæ˜¯ ...0011ï¼ˆåè¿›åˆ¶ 3ï¼‰ã€‚

  æ‰€ä»¥ key1å’Œ key2å‘ç”Ÿäº†å†²çªï¼Œéƒ½æŒ‚åœ¨ table[3]çš„é“¾è¡¨ä¸Šã€‚

  ç°åœ¨è§¦å‘æ‰©å®¹ï¼Œæ–°çš„ ht[1]çš„ size = 8ï¼Œåˆ™ sizemask = 7ï¼ˆäºŒè¿›åˆ¶ ...0111ï¼‰ã€‚â€‹â€‹

  åœ¨Rehashè¿ç§»æ—¶ï¼Œä¼šä¸ºæ¯ä¸ªkeyé‡æ–°è®¡ç®—å®ƒåœ¨æ–°è¡¨ä¸­çš„ä½ç½®ï¼š

  è®¡ç®— key1çš„æ–°ä½ç½®ï¼šhash1 & 0111-> å‡è®¾ç»“æœæ˜¯ ...0011ï¼ˆåè¿›åˆ¶ 3ï¼‰ã€‚

  è®¡ç®— key2çš„æ–°ä½ç½®ï¼šhash2 & 0111-> å‡è®¾ç»“æœæ˜¯ ...0111ï¼ˆåè¿›åˆ¶ 7ï¼‰ã€‚

  â€‹ä½ çœ‹ï¼Œå› ä¸ºæ©ç ï¼ˆsizemaskï¼‰çš„ä½æ•°å˜å¤šäº†ï¼ˆä»2ä½å˜æˆ3ä½ï¼‰ï¼Œå‚ä¸è®¡ç®—çš„å“ˆå¸Œå€¼ä½æ•°ä¹Ÿå˜å¤šäº†ï¼ŒåŸæœ¬å†²çªçš„ä¸¤ä¸ªkeyï¼Œå…¶å“ˆå¸Œå€¼åœ¨å3ä½ä¸åŒçš„éƒ¨åˆ†è¢«æš´éœ²å‡ºæ¥ï¼Œä»è€Œè®¡ç®—å‡ºäº†ä¸åŒçš„ç´¢å¼•ï¼Œå®ƒä»¬å°±è¢«åˆ†å¼€äº†ï¼â€‹

  è¿™åˆ©ç”¨äº†å“ˆå¸Œå€¼çš„å‡åŒ€åˆ†å¸ƒç‰¹æ€§ã€‚ä¸€ä¸ªå¥½çš„å“ˆå¸Œå‡½æ•°ï¼ˆå¦‚Redisä½¿ç”¨çš„SipHashï¼‰äº§ç”Ÿçš„å“ˆå¸Œå€¼æ˜¯éšæœºçš„ã€å‡åŒ€åˆ†å¸ƒçš„ã€‚

  ä»€ä¹ˆæ—¶å€™ä»ç„¶ä¼šå†²çªï¼Ÿ
  â€‹æœ‰å¯èƒ½rehashåä¸¤ä¸ªkeyä»ç„¶å†²çªã€‚â€‹â€‹ è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨ï¼šè¿™ä¸¤ä¸ªkeyçš„å“ˆå¸Œå€¼åœ¨æ–°æ©ç çš„æ‰€æœ‰ä½ä¸Šä»ç„¶å®Œå…¨ç›¸åŒã€‚

  ç»§ç»­ä¸Šé¢çš„ä¾‹å­ï¼Œå¦‚æœ key1çš„å“ˆå¸Œå€¼æ˜¯ ...1011ï¼Œkey2çš„å“ˆå¸Œå€¼æ˜¯ ...0011ï¼š

  åœ¨ sizemask=3 (0011)æ—¶ï¼š1011 & 0011 = 0011ï¼Œ0011 & 0011 = 0011-> å†²çªã€‚

  åœ¨ sizemask=7 (0111)æ—¶ï¼š1011 & 0111 = 0011ï¼Œ0011 & 0111 = 0011-> â€‹ä»ç„¶å†²çªã€‚

  è¿™æ˜¯å› ä¸ºè¿™ä¸¤ä¸ªå“ˆå¸Œå€¼çš„ä½3ä½éƒ½æ˜¯ 011ã€‚è™½ç„¶æ‰©å®¹äº†ï¼Œä½†åªè¦å®ƒä»¬å“ˆå¸Œå€¼åœ¨æ–°æ©ç è¦†ç›–çš„ä½ä¸Šæ˜¯ç›¸åŒçš„ï¼Œå°±è¿˜æ˜¯ä¼šåˆ†é…åˆ°åŒä¸€ä¸ªæ¡¶é‡Œã€‚è¦è§£å†³è¿™ä¸ªå†²çªï¼Œå¯èƒ½éœ€è¦å†æ¬¡æ‰©å®¹ï¼Œè®©æ©ç èƒ½è¦†ç›–åˆ°å®ƒä»¬å¼€å§‹å‡ºç°å·®å¼‚çš„æ›´é«˜ä½ã€‚

#### å†…å­˜ä¼˜åŒ–

[å®¹é‡è¯„ä¼°](https://blog.csdn.net/u011983531/article/details/79598671)
https://cloud.tencent.com/developer/article/1004898
https://www.cnblogs.com/yxhblogs/p/12713739.html

##### å†…å­˜æ¨¡å‹

Each data type in Redis has its own encoding, and most of them have several encodings for different scenarios. Even sds strings (and yes, string keys are usually sds strings) can have multiple encodings.

Sets, sorted sets, lists and hashes use a compact "ziplist" encoding in memory when they are small, but move to a memory wasteful yet faster encoding when they grow.

The most complex object is the sorted set, which is a combination of a skiplist and a hash table. And the new streams object also has a very interesting representation.

In RDB though, they get serialized into a compact representation and not kept as they are in memory.
https://stackoverflow.com/questions/48057733/is-redis-data-stored-as-sds-or-as-objects

Redis Ziplist https://redis.com/glossary/redis-ziplist/

Redis automatically switches between ziplist and other data structures, such as linked lists or hash tables, based on certain criteria. The decision to use ziplists depends on factors like the number of elements and their sizes. Redis provides configuration options to control the threshold values for switching between different representations.

conn.rpush(â€˜testâ€™, â€˜aâ€™, â€˜bâ€™, â€˜câ€™, â€˜dâ€™)
4
We start by pushing four items onto a LIST.

conn.debug_object(â€˜testâ€™)
To obtain information about a specific object, we can utilize the â€œdebug objectâ€ command.it is important to note that for nonziplist encodings (except for the special encoding of SETs), this number does not accurately reflect the actual memory consumption.

redisobjectï¼š
ä½†rediså¤§å¤šæ•°æƒ…å†µä¸‹å¹¶æ²¡æœ‰ç›´æ¥ä½¿ç”¨åº•å±‚æ•°æ®ç»“æ„ï¼ˆsds ziplist skiplistç­‰ï¼‰æ¥å®ç°é”®å€¼å¯¹æ•°æ®åº“ï¼Œè€Œæ˜¯åŸºäºè¿™äº›æ•°æ®ç»“æ„åˆ›å»ºäº†ä¸€ä¸ªå¯¹è±¡ç³»ç»Ÿï¼Œæ¯ä¸ªå¯¹è±¡éƒ½åŒ…å«äº†ä¸€ç§å…·ä½“æ•°æ®ç»“æ„ã€‚æ¯”å¦‚ï¼Œå½“redisæ•°æ®åº“æ–°åˆ›å»ºä¸€ä¸ªé”®å€¼å¯¹æ—¶ï¼Œå°±éœ€è¦åˆ›å»ºä¸€ä¸ªå€¼å¯¹è±¡ï¼Œå€¼å¯¹è±¡çš„*ptrå±æ€§æŒ‡å‘å…·ä½“çš„SDSå­—ç¬¦ä¸²ã€‚

###### åº•å±‚æ•°æ®ç»“æ„Stingå­—ç¬¦ä¸²å®¹é‡è¯„ä¼°
ä¸€ä¸ªç®€å•çš„key-valueé”®å€¼å¯¹æœ€ç»ˆä¼šäº§ç”Ÿ4ä¸ªæ¶ˆè€—å†…å­˜çš„ç»“æ„ï¼Œä¸­é—´freeæ‰çš„ä¸è€ƒè™‘ï¼š


1ä¸ªdictEntryç»“æ„ï¼Œ24å­—èŠ‚ï¼Œè´Ÿè´£ä¿å­˜å…·ä½“çš„é”®å€¼å¯¹ å‘ä¸Šå–æ•´ä¸º32ï¼›(jemalloc åœ¨åˆ†é…å†…å­˜æ—¶ï¼Œä¼šæ ¹æ®æˆ‘ä»¬ç”³è¯·çš„å­—èŠ‚æ•° Nï¼Œæ‰¾ä¸€ä¸ªæ¯” N å¤§ï¼Œä½†æ˜¯æœ€æ¥è¿‘ N çš„ 2 çš„å¹‚æ¬¡æ•°ä½œä¸ºåˆ†é…çš„ç©ºé—´ï¼Œè¿™æ ·å¯ä»¥å‡å°‘é¢‘ç¹åˆ†é…çš„æ¬¡æ•°ã€‚ä¸¾ä¸ªä¾‹å­ã€‚å¦‚æœä½ ç”³è¯· 6 å­—èŠ‚ç©ºé—´ï¼Œjemalloc å®é™…ä¼šåˆ†é… 8 å­—èŠ‚ç©ºé—´ï¼›å¦‚æœä½ ç”³è¯· 24 å­—èŠ‚ç©ºé—´ï¼Œjemalloc åˆ™ä¼šåˆ†é… 32 å­—èŠ‚ã€‚æ‰€ä»¥ï¼Œåœ¨æˆ‘ä»¬åˆšåˆšè¯´çš„åœºæ™¯é‡Œï¼ŒdictEntry ç»“æ„å°±å ç”¨äº† 32 å­—èŠ‚ã€‚)
1ä¸ªredisObjectç»“æ„ï¼Œ16å­—èŠ‚ï¼Œç”¨ä½œvalå¯¹è±¡ï¼›
1ä¸ªSDSç»“æ„ï¼Œç”¨ä½œkeyå­—ç¬¦ä¸²ï¼Œå 9ä¸ªå­—èŠ‚(free4ä¸ªå­—èŠ‚+len4ä¸ªå­—èŠ‚+å­—ç¬¦ä¸²æœ«å°¾â€\0â€1ä¸ªå­—èŠ‚)ï¼›
1ä¸ªSDSç»“æ„ï¼Œç”¨ä½œvalå­—ç¬¦ä¸²ï¼Œå 9ä¸ªå­—èŠ‚(free4ä¸ªå­—èŠ‚+len4ä¸ªå­—èŠ‚+å­—ç¬¦ä¸²æœ«å°¾â€\0â€1ä¸ªå­—èŠ‚)

ã€€ã€€å½“keyä¸ªæ•°é€æ¸å¢å¤šï¼Œredisè¿˜ä¼šä»¥rehashçš„æ–¹å¼æ‰©å±•å“ˆå¸Œè¡¨èŠ‚ç‚¹æ•°ç»„(ä¹Ÿå°±æ˜¯dictEntry[]æ•°ç»„)ï¼Œå³å¢å¤§å“ˆå¸Œè¡¨çš„bucketä¸ªæ•°ï¼Œæ¯ä¸ªbucketå…ƒç´ éƒ½æ˜¯ä¸ªæŒ‡é’ˆ(dictEntry*)ï¼Œå 8å­—èŠ‚ï¼Œbucketä¸ªæ•°æ˜¯è¶…è¿‡keyä¸ªæ•°å‘ä¸Šæ±‚æ•´çš„2çš„næ¬¡æ–¹ã€‚

ã€€ã€€çœŸå®æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç»“æ„æœ€ç»ˆçœŸæ­£å ç”¨çš„å†…å­˜è¿˜è¦è€ƒè™‘jemallocçš„å†…å­˜åˆ†é…è§„åˆ™ï¼Œ

ã€€jemallocæ˜¯ä¸€ç§é€šç”¨çš„å†…å­˜ç®¡ç†æ–¹æ³•ï¼Œç€é‡äºå‡å°‘å†…å­˜ç¢ç‰‡å’Œæ”¯æŒå¯ä¼¸ç¼©çš„å¹¶å‘æ€§ï¼Œåšrediså®¹é‡è¯„ä¼°å‰å¿…é¡»å¯¹jemallocçš„å†…å­˜åˆ†é…è§„åˆ™æœ‰ä¸€å®šäº†è§£ã€‚

jemallocåŸºäºç”³è¯·å†…å­˜çš„å¤§å°æŠŠå†…å­˜åˆ†é…åˆ†ä¸ºä¸‰ä¸ªç­‰çº§ï¼šsmallï¼Œlargeï¼Œhugeï¼š

Small Objectçš„sizeä»¥8å­—èŠ‚ï¼Œ16å­—èŠ‚ï¼Œ32å­—èŠ‚ç­‰åˆ†éš”å¼€ï¼Œå°äºé¡µå¤§å°ï¼›
Large Objectçš„sizeä»¥åˆ†é¡µä¸ºå•ä½ï¼Œç­‰å·®é—´éš”æ’åˆ—ï¼Œå°äºchunkçš„å¤§å°ï¼›
Huge Objectçš„å¤§å°æ˜¯chunkå¤§å°çš„æ•´æ•°å€ã€‚
å¯¹äº64ä½ç³»ç»Ÿï¼Œä¸€èˆ¬chunkå¤§å°ä¸º4Mï¼Œé¡µå¤§å°ä¸º4K


ç»¼ä¸Šæ‰€è¿°ï¼Œstringç±»å‹çš„å®¹é‡è¯„ä¼°æ¨¡å‹ä¸ºï¼š

æ€»å†…å­˜æ¶ˆè€— = (dictEntryå¤§å°ï¼‹redisObjectå¤§å°ï¼‹key_SDSå¤§å°ï¼‹val_SDSå¤§å°) * keyä¸ªæ•°ï¼‹bucketä¸ªæ•° * 8
ã€æ¢ç®—ä¸‹æ¥ã€‘
æ€»å†…å­˜æ¶ˆè€— = (32 + 16 + key_SDSå¤§å°ï¼‹val_SDSå¤§å°) * keyä¸ªæ•°ï¼‹bucketä¸ªæ•° * 8 

ï¼ˆ1ï¼‰ä¸¾ä¾‹è¯´æ˜
å½“keyé•¿åº¦ä¸º 13ï¼Œvalueé•¿åº¦ä¸º15ï¼Œkeyä¸ªæ•°ä¸º2000ï¼Œæ ¹æ®ä¸Šé¢æ€»ç»“çš„å®¹é‡è¯„ä¼°æ¨¡å‹ï¼Œå®¹é‡é¢„ä¼°å€¼ä¸º (32 + 16 + 32 + 32) * 2000 + 2048 * 8 = 240384 

ï¼ˆ2ï¼‰ç”Ÿäº§å®è·µ
ç”¨redisåšå•†å“ç¼“å­˜ï¼Œkeyä¸ºå•†å“idï¼Œvalueä¸ºå•†å“ä¿¡æ¯ã€‚keyå¤§çº¦å ç”¨30ä¸ªå­—èŠ‚ï¼Œvalueå¤§çº¦å ç”¨1500ä¸ªå­—èŠ‚ã€‚
å½“ç¼“å­˜1ç™¾ä¸‡å•†å“æ—¶ï¼Œå®¹é‡é¢„ä¼°å€¼ä¸º(32 + 16 + 64 + 1536) * 1000000+ 1000000(é¢„ä¼°) * 8 = 1656000000ï¼Œçº¦ç­‰äº1.54G
æ€»ç»“ï¼šå½“valueæ¯”è¾ƒå¤§æ—¶ï¼Œå ç”¨çš„å†…å­˜çº¦ç­‰äºvalueçš„å¤§å°*ä¸ªæ•°

###### åº•å±‚æ•°æ®ç»“æ„å“ˆå¸Œè¡¨å®¹é‡è¯„ä¼°
ä¸€ä¸ªHashå­˜å‚¨ç»“æ„æœ€ç»ˆä¼šäº§ç”Ÿä»¥ä¸‹å‡ ä¸ªæ¶ˆè€—å†…å­˜çš„ç»“æ„ï¼š

1ä¸ªSDSç»“æ„ï¼Œç”¨ä½œkeyå­—ç¬¦ä¸²ï¼Œå 9ä¸ªå­—èŠ‚(free4ä¸ªå­—èŠ‚+len4ä¸ªå­—èŠ‚+å­—ç¬¦ä¸²æœ«å°¾â€\0â€1ä¸ªå­—èŠ‚)ï¼›
1ä¸ªdictEntryç»“æ„ï¼Œ24å­—èŠ‚ï¼Œè´Ÿè´£ä¿å­˜å½“å‰çš„å“ˆå¸Œå¯¹è±¡ï¼›
1ä¸ªredisObjectç»“æ„ï¼Œ16å­—èŠ‚ï¼ŒæŒ‡å‘å½“å‰keyä¸‹å±çš„dictç»“æ„ï¼›
1ä¸ªdictç»“æ„ï¼Œ88å­—èŠ‚ï¼Œè´Ÿè´£ä¿å­˜å“ˆå¸Œå¯¹è±¡çš„é”®å€¼å¯¹ï¼›
nä¸ªdictEntryç»“æ„ï¼Œ24*nå­—èŠ‚ï¼Œè´Ÿè´£ä¿å­˜å…·ä½“çš„fieldå’Œvalueï¼Œnç­‰äºfieldä¸ªæ•°ï¼›
nä¸ªredisObjectç»“æ„ï¼Œ16*nå­—èŠ‚ï¼Œç”¨ä½œfieldå¯¹è±¡ï¼›
nä¸ªredisObjectç»“æ„ï¼Œ16*nå­—èŠ‚ï¼Œç”¨ä½œvalueå¯¹è±¡ï¼›
nä¸ªSDSç»“æ„ï¼Œï¼ˆfieldé•¿åº¦ï¼‹9ï¼‰*nå­—èŠ‚ï¼Œç”¨ä½œfieldå­—ç¬¦ä¸²ï¼›
nä¸ªSDSç»“æ„ï¼Œï¼ˆvalueé•¿åº¦ï¼‹9ï¼‰*nå­—èŠ‚ï¼Œç”¨ä½œvalueå­—ç¬¦ä¸²ï¼›
å› ä¸ºhashç±»å‹å†…éƒ¨æœ‰ä¸¤ä¸ªdictç»“æ„ï¼Œæ‰€ä»¥æœ€ç»ˆä¼šæœ‰äº§ç”Ÿä¸¤ç§rehashï¼Œä¸€ç§rehashåŸºå‡†æ˜¯fieldä¸ªæ•°ï¼Œå¦ä¸€ç§rehashåŸºå‡†æ˜¯keyä¸ªæ•°ï¼Œç»“åˆjemallocå†…å­˜åˆ†é…è§„åˆ™ï¼Œhashç±»å‹çš„å®¹é‡è¯„ä¼°æ¨¡å‹ä¸ºï¼š

æ€»å†…å­˜æ¶ˆè€— = [key_SDSå¤§å° + redisObjectå¤§å° + dictEntryå¤§å° + dictå¤§å° +(redisObjectå¤§å° * 2 + field_SDSå¤§å° + val_SDSå¤§å° + dictEntryå¤§å°) * fieldä¸ªæ•° + field_bucketä¸ªæ•° * æŒ‡é’ˆå¤§å°] * keyä¸ªæ•° + key_bucketä¸ªæ•° * æŒ‡é’ˆå¤§å°
ã€æ¢ç®—ã€‘
æ€»å†…å­˜æ¶ˆè€— = [ key_SDSå¤§å° + 16 + 24 + 88 + (16 * 2 + field_SDSå¤§å° + val_SDSå¤§å° + 24) * fieldä¸ªæ•° + field_bucketä¸ªæ•° * 8] * keyä¸ªæ•° + key_bucketä¸ªæ•° * 8
æ€»å†…å­˜æ¶ˆè€— =[128+ key_SDSå¤§å° +(56 + field_SDSå¤§å° + val_SDSå¤§å° ) * fieldä¸ªæ•° + field_bucketä¸ªæ•° * 8] * keyä¸ªæ•° + key_bucketä¸ªæ•° * 8

ç”Ÿäº§å®ä¾‹
ç”¨redisåšå•†å“ç¼“å­˜ï¼Œkeyä¸ºå•†å®¶idï¼Œfieldä¸ºå•†å“idï¼Œvalueä¸ºå•†å“ä¿¡æ¯ã€‚
å½“æœ‰1000ä¸ªkeyï¼Œæ¯ä¸ªkeyæœ‰1000ä¸ªfieldï¼Œå³æ€»å…±1ç™¾ä¸‡å•†å“æ—¶ï¼Œæ€»å®¹é‡è·Ÿä½¿ç”¨key-valueç»“æ„å·®ä¸å¤šï¼Œå¤šå‡ºæ¥å‡ åå…†çš„ç©ºé—´è€Œå·²ã€‚

##### å‘½ä»¤
info memory
memory usage
memory stats
memory doctor
memory purge

##### ä¼˜åŒ–æ€è·¯

[Memory Optimization for Redis](https://docs.redis.com/latest/ri/memory-optimizations/)
Rediså†…å­˜ç¢ç‰‡é€šå¸¸æ˜¯æŒ‡Redisåœ¨å†…å­˜ä¸­ä½¿ç”¨çš„ç©ºé—´å¹¶ä¸æ˜¯è¿ç»­çš„ï¼Œè¿™æ˜¯å› ä¸ºRedisåœ¨è¿›è¡Œå†…å­˜åˆ†é…æ—¶éµå¾ªç‰¹å®šçš„å†…å­˜ç®¡ç†ç­–ç•¥ï¼Œæ¯”å¦‚jemallocï¼Œæ¥å‡å°‘å†…å­˜ç¢ç‰‡ã€‚

å¦‚æœæ‚¨å‘ç°Redisçš„å†…å­˜ä½¿ç”¨å‡ºç°ç¢ç‰‡é—®é¢˜ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‚¨çš„åº”ç”¨ç¨‹åºæ­£åœ¨è¿›è¡Œé¢‘ç¹çš„é”®çš„æ·»åŠ å’Œåˆ é™¤æ“ä½œï¼Œè¿™å¯¼è‡´äº†å†…å­˜ä¸èƒ½è¢«å®Œå…¨é‡ç”¨ã€‚

è§£å†³æ–¹æ³•ï¼š

å¦‚æœæ‚¨æ­£åœ¨é¢‘ç¹æ›´æ¢é”®ï¼Œå¹¶ä¸”è¿™äº›é”®çš„å¤§å°ç›¸ä¼¼ï¼Œæ‚¨å¯ä»¥è€ƒè™‘ä½¿ç”¨CONFIG SET hash-max-ziplist-entries 512å’ŒCONFIG SET hash-max-ziplist-value 64è¿™æ ·çš„å‘½ä»¤æ¥å‡å°‘å“ˆå¸Œè¡¨çš„å¤§å°ï¼Œä»è€Œå‡å°‘å†…å­˜ç¢ç‰‡ã€‚

å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨çš„æ˜¯Redisçš„ç‰ˆæœ¬å¤§äºæˆ–ç­‰äº4.0ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨CONFIG SET activedefrag yesæ¥å¯ç”¨è‡ªåŠ¨å†…å­˜ç¢ç‰‡æ•´ç†ã€‚

å®šæœŸæ‰‹åŠ¨è¿è¡ŒMEMORY PURGEå‘½ä»¤ä¹Ÿå¯ä»¥å¸®åŠ©é‡Šæ”¾å†…å­˜ç¢ç‰‡ã€‚

å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨çš„æ˜¯Redisé›†ç¾¤ï¼Œè¯·ç¡®ä¿é›†ç¾¤çš„é…ç½®æ˜¯æ­£ç¡®çš„ï¼Œå¹¶ä¸”æ²¡æœ‰å› ä¸ºç½‘ç»œé—®é¢˜å¯¼è‡´çš„æ•°æ®å€¾æ–œåˆ†é…ã€‚

è¯·æ³¨æ„ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä¼šå½±å“Redisçš„æ€§èƒ½ï¼Œå› æ­¤åœ¨è°ƒæ•´é…ç½®æˆ–æ‰§è¡Œå†…å­˜æ•´ç†æ—¶ï¼Œæ‚¨åº”è¯¥åœ¨ä½å³°æ—¶æ®µè¿›è¡Œæ“ä½œï¼Œå¹¶ä¸”åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥è¿›è¡Œå……åˆ†çš„æµ‹è¯•ã€‚

-------------------------

redis hash slot å†…å­˜ç¢ç‰‡
Redisä¸­çš„hash slotæ˜¯ä¸€ç§æ•°æ®åˆ†å¸ƒç­–ç•¥ï¼Œç”¨äºå®ç°Redisé›†ç¾¤çš„æ•°æ®åˆ†å¸ƒå’Œè´Ÿè½½å‡è¡¡ã€‚åœ¨Redisé›†ç¾¤ä¸­ï¼Œæ‰€æœ‰çš„é”®éƒ½ä¼šæ ¹æ®å®ƒä»¬çš„å€¼è¢«æ˜ å°„åˆ°ä¸åŒçš„hash slotä¸­ã€‚

å¦‚æœä½ åœ¨Redisé›†ç¾¤ä¸­é‡åˆ°äº†å†…å­˜ç¢ç‰‡çš„é—®é¢˜ï¼Œè¿™é€šå¸¸æ˜¯å› ä¸ºæŸä¸ªæˆ–æŸäº›hash slotçš„å†…å­˜ä½¿ç”¨å¹¶æ²¡æœ‰å‡è¡¡åˆ†é…ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæœ‰äº›keyè¢«åˆ†é…åˆ°äº†è¾ƒå°çš„slotï¼Œè€Œå…¶ä»–çš„keyè¢«åˆ†é…åˆ°äº†è¾ƒå¤§çš„slotï¼Œå¯¼è‡´å†…å­˜ä½¿ç”¨ä¸å‡ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡é‡æ–°åˆ†é…é”®åˆ°ä¸åŒçš„hash slotã€‚Redis Clusteræä¾›äº†CLUSTER REPLICATEå‘½ä»¤ï¼Œå¯ä»¥ç”¨æ¥æ‰‹åŠ¨è¿ç§»hash slotã€‚ä½ å¯ä»¥å…ˆåœ¨æ–°çš„èŠ‚ç‚¹ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„hash slotï¼Œç„¶åå°†æ—§èŠ‚ç‚¹ä¸Šçš„éƒ¨åˆ†æˆ–å…¨éƒ¨hash slotè¿ç§»åˆ°æ–°èŠ‚ç‚¹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œè¯´æ˜å¦‚ä½•æ‰‹åŠ¨è¿ç§»hash slotï¼š

å‡è®¾ä½ æƒ³è¦è¿ç§»çš„hash slotæ˜¯1ï¼Œå¹¶ä¸”æºèŠ‚ç‚¹æ˜¯192.168.1.1:7000ï¼Œç›®æ ‡èŠ‚ç‚¹æ˜¯192.168.1.2:7000ã€‚

åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„hash slotï¼š

redis-cli -h 192.168.1.2 -p 7000 CLUSTER ADDSLOTS {slot}
åœ¨æºèŠ‚ç‚¹ä¸Šè¿ç§»hash slotåˆ°ç›®æ ‡èŠ‚ç‚¹ï¼š

redis-cli -h 192.168.1.1 -p 7000 CLUSTER GETKEYSINSLOT 1 100
ä¸Šè¿°å‘½ä»¤ä¼šè·å–slot 1ä¸­çš„å‰100ä¸ªkeyã€‚ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨MIGRATEå‘½ä»¤å°†è¿™äº›keyè¿ç§»åˆ°ç›®æ ‡èŠ‚ç‚¹ï¼š

redis-cli -h 192.168.1.1 -p 7000 MIGRATE 192.168.1.2:7000 "" KEYS 100
é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æºèŠ‚ç‚¹ä¸Šçš„slot 1ä¸ºç©ºã€‚

æ³¨æ„ï¼šåœ¨å®é™…æ“ä½œä¸­ï¼Œä½ å¯èƒ½éœ€è¦åœæ­¢å¯¹è¿™äº›keyè¿›è¡Œå†™æ“ä½œï¼Œå¹¶ä¸”å¯èƒ½éœ€è¦é‡æ–°é…ç½®DNSï¼Œä»¥ä¾¿å®¢æˆ·ç«¯å¯ä»¥è¿æ¥åˆ°æ–°çš„èŠ‚ç‚¹ã€‚

æ­¤å¤–ï¼ŒRedis 4.0åŠä»¥ä¸Šç‰ˆæœ¬æä¾›äº†CLUSTER RELOCATEå‘½ä»¤ï¼Œå¯ä»¥è‡ªåŠ¨è¿ç§»hash slotä¸­çš„keysï¼Œä½†è¿™ä¸ªå‘½ä»¤ä¸æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±ã€‚

æœ€åï¼Œå®šæœŸç›‘æ§é›†ç¾¤çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¹¶å¯¹é”®è¿›è¡Œåˆç†åˆ†å¸ƒï¼Œå¯ä»¥æœ€å¤§ç¨‹åº¦ä¸Šé¿å…å†…å­˜ç¢ç‰‡é—®é¢˜ã€‚

##### redisçš„opsForHashå¸¦æ¥çš„å†…å­˜ç©ºé—´ä¼˜åŒ–
https://my.oschina.net/u/2382040/blog/2236871


#### æ•°æ®å€¾æ–œ

reshard
https://blog.csdn.net/qq1309664161/article/details/126712760

https://cloud.tencent.com/developer/article/1676492

big key

Scanning for big keys
redis-cli --bigkeys

https://programming.vip/docs/ali-yun-redis-big-key-search-tool.html

#### çº¿ç¨‹å®‰å…¨

å•çº¿ç¨‹ï¼Œè€ƒè™‘æ˜¯å¦åŸå­æ“ä½œ

Get åˆ¤æ–­

ï¼ˆæ—¶é—´çª—å£ï¼‰

Set ï¼ˆå¤šçº¿ç¨‹è¦†ç›–ï¼‰

Setnx

è°ˆè°ˆRedisçš„SETNX https://huoding.com/2015/09/14/463

https://redis.io/commands/setnx

https://github.com/StackExchange/StackExchange.Redis/blob/86b983496d3307903ce9bc2a3c7f207de42a0dea/StackExchange.Redis/StackExchange/Redis/RedisDatabase.cs


## 1. å®‰è£…ä½¿ç”¨
https://redis.io/topics/quickstart

### 1.1 ç®€å•å®‰è£…

```
yum install gcc

$ wget https://download.redis.io/releases/redis-6.2.1.tar.gz
$ tar xzf redis-6.2.1.tar.gz
$ cd redis-6.2.1
$ make

make test
sudo cp src/redis-server /usr/local/bin/
sudo cp src/redis-cli /usr/local/bin/

å¸è½½çš„æ—¶å€™åˆ«å¿˜è®°
rm /usr/local/bin/redis-server
rm /usr/local/bin/redis-cli
```

### 1.2 ç”Ÿäº§ç¯å¢ƒå®‰è£…ï¼ˆæ¨èï¼‰

> Installing Redis more properly
> Running Redis from the command line is fine just to hack a bit with it or for development. However at some point you'll have some actual application to run on a real server. For this kind of usage you have two different choices:
> 
> + Run Redis using screen.
> + Install Redis in your Linux box in a proper way using an init script, so that after a restart everything will start again properly.
> A proper install using an init script is strongly suggested. The following instructions can be used to perform a proper installation using the init script shipped with Redis 2.4 in a Debian or Ubuntu based distribution.
> 
> We assume you already copied redis-server and redis-cli executables under /usr/local/bin.
> + Create a directory in which to store your Redis config files and your data:
> sudo mkdir /etc/redis
> sudo mkdir /var/redis
> + Copy the init script that you'll find in the Redis distribution under the utils directory into /etc/init.d. We suggest calling it with the name of the port where you are running this instance of Redis. For example:
> sudo cp utils/redis_init_script /etc/init.d/redis_6379
> + Edit the init script.
> sudo vi /etc/init.d/redis_6379
> ...
> https://redis.io/topics/quickstart

### 1.3 å•èŠ‚ç‚¹å¯åŠ¨ Single node
```
redis-server
Or start with config
redis-server redis.conf
```

### 1.4 é›†ç¾¤å¯åŠ¨ Cluster Mode
Redis cluster tutorial https://redis.io/topics/cluster-tutorial

Config:

```
daemonize yes  #åå°è¿è¡Œæ¨¡å¼
logfile â€œredis_6379_logâ€
pidfile /home/web/redis/var/run/redis_6379.pid  #ä¿®æ”¹pidæ–‡ä»¶
dir /home/web/redis/data/ #æŒ‡å®šæœ¬åœ°æ•°æ®åº“å­˜æ”¾ç›®å½•
port 6379  #ç«¯å£
dbfilename dump_6379.rdb #æ•°æ®æ–‡ä»¶
appendfilename "appendonly_6379.aof"
cluster-enabled yes #æ‰“å¼€æ³¨é‡Šï¼Œå¯åŠ¨clusteræ¨¡å¼
cluster-config-file nodes-6379.conf #æ‰“å¼€æ³¨é‡Šï¼Œå¯åŠ¨clusteræ¨¡å¼
# Cluster node timeout is the amount of milliseconds a node must be unreachable
# for it to be considered in failure state.
# Most other internal time limits are multiple of the node timeout.
cluster-node-timeout 15000 #æ‰“å¼€æ³¨é‡Šï¼Œå¯åŠ¨clusteræ¨¡å¼
```
å‚è€ƒæœ€ä½é…ç½®ï¼šhttps://redis.io/topics/cluster-tutorial
_Note that the minimal cluster that works as expected requires to contain at least three master nodes._

#### ä½¿ç”¨utilsè„šæœ¬

ä½ç½®ï¼šredis/utils/create-cluster/README

To create a cluster, follow these steps:

1. Edit create-cluster and change the start / end port, depending on the
number of instances you want to create.
2. Use "./create-cluster start" in order to run the instances.
3. Use "./create-cluster create" in order to execute redis-cli --cluster create, so that
an actual Redis cluster will be created.
4. Now you are ready to play with the cluster. AOF files and logs for each instances are created in the current directory.                                                            

In order to stop a cluster:

1. Use "./create-cluster stop" to stop all the instances. After you stopped the instances you can use "./create-cluster start" to restart them if you change your mind.               
2. Use "./create-cluster clean" to remove all the AOF / log files to restart with a clean environment.                         



#### æ‰‹åŠ¨åˆ›å»º

```
redis-server conf/redis6379.conf
redis-server conf/redis6380.conf
redis-server conf/redis6381.conf
redis-cli --cluster create <HOSTIP1>:6379 <HOSTIP1>:6380 <HOSTIP1>:6381 \
<HOSTIP2>:6379 <HOSTIP2>:6380 <HOSTIP2>:6381 \
<HOSTIP3>:6379 <HOSTIP3>:6380 <HOSTIP3>:6381 \
--cluster-replicas 2

>>> Performing hash slots allocation on 9 nodes...   
Master[0] -> Slots 0 - 5460                            
Master[1] -> Slots 5461 - 10922                                                                    
Master[2] -> Slots 10923 - 16383            
Adding replica HOSTIP2:6380 to HOSTIP1:6379
Adding replica HOSTIP3:6380 to HOSTIP1:6379
Adding replica HOSTIP1:6381 to HOSTIP2:6379
Adding replica HOSTIP3:6381 to HOSTIP2:6379
Adding replica HOSTIP2:6381 to HOSTIP3:6379        
Adding replica HOSTIP1:6380 to HOSTIP3:6379     
M: afabffee7a9076d42c9640a77ae2db6e6eb52fae HOSTIP1:6379           
   slots:[0-5460] (5461 slots) master                           
S: f24a6554ed2b64b071122bd16c7201aca1b184d0 HOSTIP1:6380                               
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c               
S: bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOSTIP1:6381
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d             
M: b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOSTIP2:6379
   slots:[5461-10922] (5462 slots) master                                   
S: 27c88c277aa82340f5e2f9d73078d59399ed6b87 HOSTIP2:6380 
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae                
S: 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOSTIP2:6381 
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c     
M: 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOSTIP3:6379     
   slots:[10923-16383] (5461 slots) master               
S: 54d6095aca3e1edd27761e080651bb28144e3a81 HOSTIP3:6380
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae
S: 9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOSTIP3:6381
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
...
>>> Performing Cluster Check (using node HOSTIP1:6379)
M: afabffee7a9076d42c9640a77ae2db6e6eb52fae HOSTIP1:6379
   slots:[0-5460] (5461 slots) master
   2 additional replica(s)
S: bb483966fa9a7d60c9020a75d19fb2a4d1e8acf0 HOSTIP1:6381
   slots: (0 slots) slave
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d
S: 27c88c277aa82340f5e2f9d73078d59399ed6b87 HOSTIP2:6380
   slots: (0 slots) slave
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae
M: b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d HOSTIP2:6379
   slots:[5461-10922] (5462 slots) master
   2 additional replica(s)
S: 9f92fe21d31b4b18f54321fbedc809ca4afcf187 HOSTIP3:6381
   slots: (0 slots) slave
   replicates b78a3f4b07cc5cf58a871abcb4cc01fcbc05e96d
S: 54d6095aca3e1edd27761e080651bb28144e3a81 HOSTIP3:6380
   slots: (0 slots) slave
   replicates afabffee7a9076d42c9640a77ae2db6e6eb52fae
S: 56ce383e2cb6affedd61317cfb35b05f29dfc7f1 HOSTIP2:6381
   slots: (0 slots) slave
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c
S: f24a6554ed2b64b071122bd16c7201aca1b184d0 HOSTIP1:6380
   slots: (0 slots) slave
   replicates 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c
M: 36d8fdd4eaedd2f601a2e27d9856d9b82dd8017c HOSTIP3:6379
   slots:[10923-16383] (5461 slots) master
   2 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.

```

é‡å»ºé›†ç¾¤ï¼š
è¦æƒ³åˆ é™¤ä¸€ä¸ªé›†ç¾¤ï¼Œé¦–å…ˆå…³é—­RedisæœåŠ¡ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š

redis-cli -h 127.0.0.1 -p 7000 shutdown

å…³é—­æ‰€æœ‰é›†ç¾¤ä¸ŠèŠ‚ç‚¹åï¼Œè¿›å…¥å„ä¸ªèŠ‚ç‚¹æ–‡ä»¶å¤¹ï¼Œåˆ é™¤ä»¥ä¸‹æ–‡ä»¶ï¼š

appendonly.aof
dump.rdb
nodes-7000.conf
æ‰¹é‡åˆ é™¤æŒ‡ä»¤å¦‚ä¸‹ï¼š

`rm -f ./*/nodes-*.conf ./*/appendonly.aof ./*/dump.rdb`

#### customizeè„šæœ¬

```
REDIS_HOME=/opt/redis-5.0.5/src
pushd ${REDIS_HOME} &>/dev/null

while [ "${1:0:1}" == "-" ]; do
  case $1 in
    --start)
      echo "Starting redis nodes..."
      redis-server conf/redis6379.conf
      redis-server conf/redis6380.conf
      redis-server conf/redis6381.conf
      ;;
    --create-cluster)
    	redis-cli --cluster create HOST1:6379 HOST1:6380 HOST1:6381 ....HOST2... --cluster-replicas 2
    --kill)
      echo "Stopping redis nodes..."
      redis-cli -p 6379 shutdown
      redis-cli -p 6380 shutdown
      redis-cli -p 6381 shutdown
          ;;
    --clear-cache)
      masterNodes=($(./redis-cli cluster nodes | grep master | awk '{ print $2 }'))
      testArray=(${masterNodes[@]})
      echo ${testArray[0]}
      for item in "${masterNodes[@]}"; do
                host="$(cut -d':' -f1 <<<$item)"
                tmp="$(cut -d':' -f2 <<<$item)"
                port="$(cut -d'@' -f1 <<<$tmp)"
                redis-cli -c -h ${serverIp} -p ${port} flushall
        done
    *)
      echo "usage: --start|--kill"
      exit 1
      ;;
  esac
  shift
done

```

### 1.5 Config

#### common config

+ replica-read-only

```
# Since Redis 2.6 by default replicas are read-only.
#
# Note: read only replicas are not designed to be exposed to untrusted clients
# on the internet. It's just a protection layer against misuse of the instance.
# Still a read only replica exports by default all the administrative commands
# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve
# security of read only replicas using 'rename-command' to shadow all the
# administrative / dangerous commands.
replica-read-only yes
```

+ Allow writes only with N attached replicas

```
# It is possible for a master to stop accepting writes if there are less than
# N replicas connected, having a lag less or equal than M seconds.
#
# The N replicas need to be in "online" state.
#
# The lag in seconds, that must be <= the specified value, is calculated from
# the last ping received from the replica, that is usually sent every second.
#
# This option does not GUARANTEE that N replicas will accept the write, but
# will limit the window of exposure for lost writes in case not enough replicas
# are available, to the specified number of seconds.
#
# For example to require at least 3 replicas with a lag <= 10 seconds use:
#
# min-replicas-to-write 3
# min-replicas-max-lag 10
#
# Setting one or the other to 0 disables the feature.
#
# By default min-replicas-to-write is set to 0 (feature disabled) and
# min-replicas-max-lag is set to 10.
```

#### cluster config

- **cluster-enabled `<yes/no>`**: If yes, enables Redis Cluster support in a specific Redis instance.  Otherwise the instance starts as a stand alone instance as usual.

- **cluster-config-file `<filename>`**: Note that despite the name of this option, this is not a user editable  configuration file, but the file where a Redis Cluster node  automatically persists the cluster configuration (the state, basically)  every time there is a change, in order to be able to re-read it at  startup. The file lists things like the other nodes in the cluster,  their state, persistent variables, and so forth. Often this file is  rewritten and flushed on disk as a result of some message reception.

- **cluster-node-timeout `<milliseconds>`**: The maximum amount of time a Redis Cluster node can be unavailable,  without it being considered as failing. If a master node is not  reachable for more than the specified amount of time, it will be failed  over by its slaves. This parameter controls other important things in  Redis Cluster. Notably, every node that can't reach the majority of  master nodes for the specified amount of time, will stop accepting  queries.

- **cluster-slave-validity-factor `<factor>`**: If set to zero, a slave will always consider itself valid, and will  therefore always try to failover a master, regardless of the amount of  time the link between the master and the slave remained disconnected. If the value is positive, a maximum disconnection time is calculated as  the *node timeout* value multiplied by the factor provided with  this option, and if the node is a slave, it will not try to start a  failover if the master link was disconnected for more than the specified amount of time. For example, if the node timeout is set to 5 seconds  and the validity factor is set to 10, a slave disconnected from the  master for more than 50 seconds will not try to failover its master.  Note that any value different than zero may result in Redis Cluster  being unavailable after a master failure if there is no slave that is  able to failover it. In that case the cluster will return to being  available only when the original master rejoins the cluster.

- **cluster-migration-barrier `<count>`**: Minimum number of slaves a master will remain connected with, for  another slave to migrate to a master which is no longer covered by any  slave. See the appropriate section about replica migration in this  tutorial for more information.

  ```
  
  # Cluster replicas are able to migrate to orphaned masters, that are masters
  # that are left without working replicas. This improves the cluster ability
  # to resist to failures as otherwise an orphaned master can't be failed over
  # in case of failure if it has no working replicas.
  #
  # Replicas migrate to orphaned masters only if there are still at least a
  # given number of other working replicas for their old master. This number
  # is the "migration barrier". A migration barrier of 1 means that a replica
  # will migrate only if there is at least 1 other working replica for its master
  # and so forth. It usually reflects the number of replicas you want for every
  # master in your cluster.
  #
  # Default is 1 (replicas migrate only if their masters remain with at least
  # one replica). To disable migration just set it to a very large value.
  # A value of 0 can be set but is useful only for debugging and dangerous
  # in production.
  #
  cluster-migration-barrier 1
  æ„æ€æ˜¯ å½“mater node Açš„replicaéƒ½æŒ‚äº†ä¹‹åï¼ŒAå˜æˆäº† orphaned master nodeï¼Œå¦‚æœBç°åœ¨æœ‰ä¸¤ä¸ªreplicaï¼Œé‚£å…¶ä¸­ä¸€ä¸ªreplicaå°±å¯ä»¥è‡ªåŠ¨migrateï¼Œå˜æˆAçš„replicaï¼Œç„¶åBå°±å‰©ä¸€ä¸ªreplicaï¼Œå¦‚æœBæœ¬æ¥åªæœ‰ä¸€ä¸ªreplicaï¼Œé‚£ä¹ˆBçš„replicaå°±ä¸å¯ä»¥migrateæˆä¸ºAçš„replicaï¼Œå› ä¸ºbarrieræ˜¯1
  ```

  

- **cluster-require-full-coverage `<yes/no>`**: If this is set to yes, as it is by default, the cluster stops accepting writes if some percentage of the key space is not covered by any node.  If the option is set to no, the cluster will still serve queries even if only requests about a subset of keys can be processed.

- **cluster-allow-reads-when-down `<yes/no>`**: If this is set to no, as it is by default, a node in a Redis Cluster  will stop serving all traffic when the cluster is marked as failed,  either when a node can't reach a quorum of masters or when full coverage is not met. This prevents reading potentially inconsistent data from a  node that is unaware of changes in the cluster. This option can be set  to yes to allow reads from a node during the fail state, which is useful for applications that want to prioritize read availability but still  want to prevent inconsistent writes. It can also be used for when using  Redis Cluster with only one or two shards, as it allows the nodes to  continue serving writes when a master fails but automatic failover is  impossible.

- cluster-slave-no-failover=ã€‹cluster-replica-no-failover

  ```
  # This option, when set to yes, prevents replicas from trying to failover its
  # master during master failures. However the master can still perform a
  # manual failover, if forced to do so.
  #
  # This is useful in different scenarios, especially in the case of multiple
  # data center operations, where we want one side to never be promoted if not
  # in the case of a total DC failure.
  ```

  https://github.com/redis/redis/issues/3021

### 1.6 commands

```
$ redis-cli -c -h <HOSTIP> -p <PORT>

redis> AUTH 'password'
redis> ACL LIST

redis> KEYS 
redis> KEYS "<PATTEN>"

keys vs scan:
https://stackoverflow.com/questions/32603964/scan-vs-keys-performance-in-redis
Note: When using Redis Cluster, the search is optimized for patterns that imply a single slot. If a pattern can only match keys of one slot, Redis only iterates over keys in that slot, rather than the whole database, when searching for keys matching the pattern. For example, with the pattern {a}h*llo, Redis would only try to match it with the keys in slot 15495, which hash tag {a} implies. To use pattern with hash tag, see Hash tags in the Cluster specification for more information.

redis-cli KEYS "<PATTEN>"
redis-cli --scan --pattern "<PATTEN>"

åªä¼šè¿”å›å½“å‰èŠ‚ç‚¹ä¸Šçš„æ•°æ®


//å­—ç¬¦ä¸²(string)
redis> GET <KEY>
redis> SET <KEY> <VALUE>

//å“ˆå¸Œ(Hash) HSET KEY_NAME FIELD VALUE 
redis 127.0.0.1:6379> HSET myhash field1 "foo"
OK
redis 127.0.0.1:6379> HGET myhash field1
"foo"

redis 127.0.0.1:6379> HSET website google "www.g.cn"       # è®¾ç½®ä¸€ä¸ªæ–°åŸŸ
(integer) 1

redis 127.0.0.1:6379>HSET website google "www.google.com" # è¦†ç›–ä¸€ä¸ªæ—§åŸŸ
(integer) 0

//åˆ—è¡¨(List) LRANGE KEY_NAME START END
redis> RPUSH mylist "one"
(integer) 1
redis> RPUSH mylist "two"
(integer) 2
redis> RPUSH mylist "three"
(integer) 3
redis> LRANGE mylist 0 0
1) "one"
redis> LRANGE mylist -3 2
1) "one"
2) "two"
3) "three"
redis> LRANGE mylist -100 100
1) "one"
2) "two"
3) "three"
redis> LRANGE mylist 5 10
(empty list or set)
redis> 

//æ— åºé›†åˆ(Set) SADD key member [member ...]
redis 127.0.0.1:6379> SADD runoobkey redis
(integer) 1
redis 127.0.0.1:6379> SADD runoobkey mongodb
(integer) 1
redis 127.0.0.1:6379> SADD runoobkey mysql
(integer) 1
redis 127.0.0.1:6379> SADD runoobkey mysql
(integer) 0
redis 127.0.0.1:6379> SMEMBERS runoobkey

1) "mysql"
2) "mongodb"
3) "redis"

//æœ‰åºé›†åˆ(sorted set) ZADD KEY_NAME SCORE1 VALUE1.. SCOREN VALUEN
redis> ZADD myzset 1 "one"
(integer) 1
redis> ZADD myzset 1 "uno"
(integer) 1
redis> ZADD myzset 2 "two" 3 "three"
(integer) 2
redis> ZRANGE myzset 0 -1 WITHSCORES

ZRANGEBYSCORE myzset -inf 1

```

### 1.7 Restore an RDB file
If you have an RDB file dump.rdb that contains the data you want you can use this file to create a new database

Copy the dump.rdb file into the Redis working directory

If you do not know what it is folder you can run the command `CONFIG get dir` where your Redis instance is up and running

Start the Redis service with the redis-server

The file dump.rdb is automatically imported.

Connect to the database using redis-cli or any other client, to check that data have been imported. (for example SCAN)




## 2. cluster é›†ç¾¤ç®¡ç†

### 2.1 Commands&GUI

https://redis.io/topics/rediscli

+ éé›†ç¾¤æ¨¡å¼å‘½ä»¤

  æ¯”å¦‚ salveof

  å¦åˆ™ä¼šæŠ¥é”™ [replicaof directive not allowed in cluster mode](https://stackoverflow.com/questions/62296087/replicaof-directive-not-allowed-in-cluster-mode)

+ é›†ç¾¤æ¨¡å¼å‘½ä»¤ cluster mode

  ç‰¹ç‚¹æ˜¯å…¨éƒ¨ cluster å¼€å¤´çš„å‘½ä»¤éƒ½å¯ä»¥ï¼Œç„¶åä¸€éƒ¨åˆ†éclusterå¼€å¤´çš„å‘½ä»¤
  æ³¨æ„åŒºåˆ† multi-key commandï¼Œæ¯”å¦‚Redis cli - KEYS * not showing all keys ï¼ˆsolutionï¼šrun  on every one of the nodes: redis-cli --cluster call hostname:90001 KEYS "*"ï¼‰

  ```
  redis-cli --cluster help
  //è¿æ¥
  redis-cli -c -h <HOSTIP> -p <PORT>
  	cluster help
  	cluster info
  	cluster nodes
  	connect HOST PORT #Switch to other nodes
  	cluster slots
  	replicate NODEID
  	cluster failover (on slave node)
  		keys * (Check empty)
  		flushdb/flushall (on master nodes)
  		cluster reset(soft/hard, chang slave to an empty â€˜standaloneâ€™ master)
  		Slaveof no one (change slave to master, cannot execute in cluster mode)
  	Slave host port (change slave to replicate another master, cannot execute in cluster mode)
  	
  //æ£€æŸ¥	
  redis-cli --cluster check <HOSTIP>:<PORT>
  //ä¿®å¤
  redis-cli --cluster fix <HOSTIP>:<PORT>
  redis-cli -h <HOSTIP> -p <PORT> cluster meet <TARGETHOSTIP> <TARGETPORT>
  redis-cli cluster forget <NODEID> (cannot perform on itself or itâ€™s slave)
  redis-cli --cluster del-node <HOSTIP>:<PORT> <NODEID>
  //add-nodeé»˜è®¤æ˜¯empty masterï¼Œä¹Ÿå¯ä»¥åŠ å‚æ•°æŒ‡å®šä¸ºslave ()
  redis-cli --cluster add-node <HOSTIP>:<PORT> <ANOTHER HOSTIP>:<PORT> --cluster-slave --cluster-master-id <NODEID>
  
  redis-cli -p 7002 shutdown
  redis-cli -p 7002 debug segfault
  
  redis-cli --cluster reshard <ANYHOSTIP>:<ANYPORT> --cluster-yes
  redis-cli --cluster reshard <host>:<port> --cluster-from <node-id> --cluster-to <node-id> --cluster-slots <number of slots> --cluster-yes
  redis-cli --cluster rebalance --cluster-threshold 1 <ANYHOSTIP>:<ANYPORT>
  
  client list
  redis-cli INFO|grep db
  redis-cli INFO|grep db|wc -l
  redis-cli INFO keyspace
  
  --More:
  all keys from all nodes:
  redis-cli --cluster call HOSTIP>:<PORT> KEYS "*"
  
  Database
  https://stackoverflow.com/questions/50534492/redis-how-to-get-current-database-name
  https://redis.io/commands/client-list
  
  redis implement stored procedure 
  https://redis.io/commands/eval
  
  ```

  

**GUI:**
use colon as separator https://redisdesktop.com/
Dbeaver support nosql but only for enterprise edition
Optionally we can choose fastoredis https://fastoredis.com/anonim_users_downloads

### 2.2 è‡ªåŠ¨æ–¹å¼ç®¡ç†

#### cluster failover

A manual failover is a special kind of failover that is usually executed when there are no actual failures, but we wish to swap the current master with one of its replicas (which is the node we send the command to), in a safe way, without any window for data loss. 

1. The replica tells the master to stop processing queries from clients.
2. The master replies to the replica with the current *replication offset*.
3. The replica waits for the replication offset to match  on its side, to make sure it processed all the data from the master  before it continues.
4. The replica starts a failover, obtains a new  configuration epoch from the majority of the masters, and broadcasts the new configuration.
5. The old master receives the configuration update:  unblocks its clients and starts replying with redirection messages so  that they'll continue the chat with the new master.

The command behavior can be modified by two options: **FORCE** and **TAKEOVER***ï¼ˆCLUSTER FAILOVER, unless the TAKEOVER option is specified, does not execute a failover synchronously, it only schedules a manual failover, bypassing the failure detection stageï¼‰:

+ **FORCE** 

  If the FORCE option is given, the replica does not perform any handshake with the master, that may be not reachable, but instead just starts a failover ASAP starting from point 4. This is useful when we want to start a manual failover while the master is no longer reachable.

+ **TAKEOVER**

  There are situations where this is not enough, and we want a replica to failover without any agreement with the rest of the cluster. A real world use case for this is to mass promote replicas in a different data center to masters in order to perform a data center switch, while all the masters are down or partitioned away.

ä¸¤ç§åœºæ™¯ï¼š

+ åœºæ™¯1ï¼šå†å¹³è¡¡èŠ‚ç‚¹ï¼Œå‚è€ƒå‰é¢ä¸»ä»åˆ‡æ¢å‘ç”Ÿçš„æ‰€æœ‰mater nodesèšé›†åˆ°ä¸€ä¸ªvmä¸Šçš„æƒ…å†µ

+ åœºæ™¯2ï¼šå‡çº§master èŠ‚ç‚¹

  **consider use failover whenever make changes to master nodes**

  1)	Remove master node (one option is reshard the data to the other master nodes and then remove, An alternative to remove a master node is to perform a manual failover of it over one of its slaves and remove the node after it turned into a slave of the new master.)
  2)	Before shutdown
  3)	Upgrade master node (failover to its slave node first before upgrading, upgrade when it becomes a slave node)

  Use failover rather than manual allocate replica using â€œREPLICATE <NODEID>â€

  > 1.The replica tells the master to stop processing queries from clients.
  > 2.The master replies to the replica with the current replication offset.
  > 3.The replica waits for the replication offset to match on its side, to make sure it processed all the data from the master before it continues.
  > 4.The replica starts a failover, obtains a new configuration epoch from the majority of the masters, and broadcasts the new configuration.
  > 5.The old master receives the configuration update: unblocks its clients and starts replying with redirection messages so that they'll continue the chat with the new master.
  > https://redis.io/commands/cluster-failover

#### cluster replica migration

å‚è€ƒ **cluster-migration-barrier `<count>`**:

#### migration between clusters å¤šä¸ªé›†ç¾¤ä¹‹é—´çš„äº¤äº’

Assuming you have your preexisting data set split into N masters, where            N=1 if you have no preexisting sharding, the following steps are needed            in order to migrate your data set to Redis Cluster:

1. Stop your clients. No automatic live-migration to Redis  Cluster is currently possible. You may be able to do it orchestrating a  live migration in the context of your application / environment.
2. Generate an append only file for all of your N masters  using the BGREWRITEAOF command, and waiting for the AOF file to be  completely generated.
3. Save your AOF files from aof-1 to aof-N somewhere. At  this point you can stop your old instances if you wish (this is useful  since in non-virtualized deployments you often need to reuse the same  computers).
4. Create a Redis Cluster composed of N masters and zero  slaves. You'll add slaves later. Make sure all your nodes are using the  append only file for persistence.
5. Stop all the cluster nodes, substitute their append only file with your pre-existing append only files, aof-1 for the first  node, aof-2 for the second node, up to aof-N.
6. Restart your Redis Cluster nodes with the new AOF files. They'll complain that there are keys that should not be there according to their configuration.
7. Use `redis-cli --cluster fix` command in  order to fix the cluster so that keys will be migrated according to the  hash slots each node is authoritative or not.
8. Use `redis-cli --cluster check` at the end to make sure your cluster is ok.
9. Restart your clients modified to use a Redis Cluster aware client library.

There is an alternative way to import data from external instances to a Redis            Cluster, which is to use the `redis-cli --cluster import` command.

#### å¤šä¸ªdata centerä¹‹é—´çš„å…³ç³»

cluster-replica-no-failover yes å¯ä»¥ç”¨æ¥ç¦æ­¢å…¶ä¸­ä¸€ä¸ªdata centeré€‰ä¸¾ promote master

### 2.3 æ‰‹åŠ¨æ–¹å¼ç®¡ç† 

ä»¥ä¸‹å®Œå…¨æ˜¯æˆ‘ä¸ªäººå®éªŒçš„æ€»ç»“ï¼š

å¦‚æœä¸»ä»éƒ½æ˜¯åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼ˆcluster-enable=yesï¼‰ï¼Œé‚£ä¹ˆæ˜¯æ— æ³•ä½¿ç”¨éé›†ç¾¤æ¨¡å¼çš„å‘½ä»¤ï¼Œæ¯”å¦‚slaveof/replicaof
**1.del-node/add-node**
del-nodeå¦‚æœæ˜¯slaveååˆ†ä¸æ¨èï¼Œå› ä¸ºå¯åŠ¨æ—¶ä¾ç„¶ä¼šè®°å¾—å…¶ä»–èŠ‚ç‚¹ï¼Œè™½ç„¶å¯ä»¥forgetå¤§éƒ¨åˆ†èŠ‚ç‚¹ï¼Œä½†æ˜¯æ— æ³•forgetå®ƒçš„masterï¼Œæ‰€ä»¥å†add-nodeä¼šæœ‰é—®é¢˜ï¼Œæ‰€ä»¥æ­¤æ—¶åªèƒ½é‡‡ç”¨meet
Del-nodeå¦‚æœæ˜¯masterï¼Œå¿…é¡»æ˜¯ç©ºçš„slotsï¼Œæ‰€ä»¥æœ‰ä¸¤ç§æ–¹æ³•ï¼š

+ é¦–å…ˆè¦å°†masteråšreshardè½¬ç§»åˆ°å…¶ä»–master nodesï¼Œç„¶åå°†å…¶å¯¹åº”çš„slave nodeså…¨éƒ¨replicateå…¶ä»–çš„master nodesï¼Œä»¥é˜²é‡å¯masteråï¼Œslave nodesä»ç„¶è®°å¾—è¯¥master node
+ å…ˆå°†master nodeå˜æˆreplica nodeï¼ˆé€šè¿‡cluster failoverï¼‰ï¼Œç„¶åå°†å…¶åˆ é™¤

```
redis-cli --cluster del-node <HOSTIP>:<PORT> <NODEID>
Redis-server conf/redis6379.conf

redis-cli --cluster add-node <NEW HOSTIP>:<PORT> <ANY EXIST HOSTIP>:<PORT
add-nodeé»˜è®¤æ˜¯masterï¼Œå¹¶ä¸”æ²¡æœ‰åˆ†é…ä»»ä½•slotsï¼Œå¦‚æœmaster node slotsæ˜¯ç©ºçš„ï¼Œä¸ä¼šå‚ä¸replica promote election

ä¹Ÿå¯ä»¥åŠ å‚æ•°æŒ‡å®šä¸ºslave
redis-cli --cluster add-node <NEW HOSTIP>:<PORT> <ANY EXIST HOSTIP>:<PORT> --cluster-slave --cluster-master-id 
```

**2.slaveof/replicaof**

æ³¨æ„å†clusteré›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ˜¯ä¸å¯ä»¥æ‰‹åŠ¨åˆ†é…çš„ï¼Œå¯ä»¥æ›´æ”¹slaveé…ç½®ï¼Œcluster-enabled=noç„¶åå†å°è¯•

```
Slaveof no one (change slave to master)
Slave host port (change slave to replicate another master)
```

**3.reset and meet**
After reset, node become a standalone master node, and then execute meet 

```
$redis-cli -p 6379
127.0.0.1:6379> flushall
127.0.0.1:6379> cluster reset
127.0.0.1:6379> exit
redis-cli -h <HOSTIP> -p <PORT> cluster meet <TARGETHOSTIP> <TARGETPORT>
```

**4. Final step: reshard or rebalance**

```
redis-cli --cluster reshard <HOSTIP>:<PORT> --cluster-yes
redis-cli --cluster rebalance --cluster-threshold 1 <HOSTIP>:<PORT>
```

**5. Allocate slave for master**

```
æˆåŠŸå°è¯•ï¼š
cluster replicate <NODEID>

å¤±è´¥å°è¯•ï¼š
redis-cli --cluster del-node <HOSTIP>:<PORT> <NODEID>
redis-cli cluster forget <NODEID>
redis-cli --cluster add-node <HOSTIP>:<PORT> <ANY EXIST HOSTIP>:<PORT> --cluster-slave --cluster-master-id <MASTER NODEID>
https://www.jianshu.com/p/ff173ae6e478
å¤±è´¥åŸå› ï¼šFailed because cannot forget itself and itâ€™s master!
```



### 2.4 æ—¥å¸¸ç»´æŠ¤
Read https://redis.io/topics/admin
http://antirez.com/news/96

> Securing Redis
> 1.Make sure the port Redis uses to listen for connections (by default 6379 and additionally 16379 if you run Redis in cluster mode, plus 26379 for Sentinel) is firewalled, so that it is not possible to contact Redis from the outside world.
> 2.Use a configuration file where the bind directive is set in order to guarantee that Redis listens on only the network interfaces you are using. For example only the loopback interface (127.0.0.1) if you are accessing Redis just locally from the same computer, and so forth.
> 3.Use the requirepass option in order to add an additional layer of security so that clients will require to authenticate using the AUTH command.
ä½†æ˜¯è¦éå¸¸æ³¨æ„ï¼ŒredisæœåŠ¡ç«¯å¦‚æœè¦é…ç½®requirepassï¼Œå¦‚æœæ˜¯clusterä¸€å®šè¦åŒæ—¶é…ç½® masterauthï¼Œå¦åˆ™æ— æ³•replicationï¼Œå½“ç„¶ä¹Ÿæ— æ³•failover!!!
> 4.Use spiped or another SSL tunneling software in order to encrypt traffic between Redis servers and Redis clients if your environment requires encryption.

> Disabling of specific commands
> It is possible to disable commands in Redis or to rename them into an unguessable name, so that normal clients are limited to a specified set of commands.
> For instance, a virtualized server provider may offer a managed Redis instance service. In this context, normal users should probably not be able to call the Redis CONFIG command to alter the configuration of the instance, but the systems that provide and remove instances should be able to do so.
> In this case, it is possible to either rename or completely shadow commands from the command table. This feature is available as a statement that can be used inside the redis.conf configuration file. For example:
> rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
> In the above example, the CONFIG command was renamed into an unguessable name. It is also possible to completely disable it (or any other command) by renaming it to the empty string, like in the following example:
> rename-command CONFIG ""
> https://redis.io/topics/security

**Upgrade**
If you are using Redis Sentinel or Redis Cluster, the simplest way in order to upgrade to newer versions, is to upgrade a slave after the other, then perform a manual fail-over in order to promote one of the upgraded replicas as master, and finally promote the last slave.

**Monitor**
redis-cli memory doctor
redis-cli latency doctor

Monitor:
https://redis.io/commands/MONITOR

### 2.5 åˆ é™¤é›†ç¾¤

å…³é—­æ‰€æœ‰é›†ç¾¤ä¸ŠèŠ‚ç‚¹åï¼Œè¿›å…¥å„ä¸ªèŠ‚ç‚¹æ–‡ä»¶å¤¹ï¼Œåˆ é™¤ä»¥ä¸‹æ–‡ä»¶ï¼š
appendonly.aof
dump.rdb
nodes-*.conf

## 3. Sentinel ç®¡ç†



## 4. Redisæ“ä½œå’Œç³»ç»Ÿé›†æˆ Integration

### 4.1 RedisåŸºæœ¬æ•°æ®æ“ä½œ

#### Data types
https://redis.io/topics/data-types

https://redis.io/topics/data-types-intro

| æ•°æ®ç±»å‹             |                                                              | ç‰¹æ€§                                                         | åœºæ™¯                                                         |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| String(å­—ç¬¦ä¸²)       | äºŒè¿›åˆ¶å®‰å…¨ <br>Strings are the most basic kind of Redis value. Redis Strings are binary safe, this means that a Redis string can contain any kind of data, for instance a JPEG image or a serialized Ruby object.<br>æœ€å¤§å­˜å‚¨512M |                                                              | å°†å¤æ‚å¯¹è±¡åºåˆ—åŒ–åå­˜å‚¨ï¼Œä¸‹é¢çš„hashåªèƒ½å­˜æ”¾key-valueé”®å€¼å¯¹çš„object |
| Hash(å­—å…¸)           | é”®å€¼å¯¹é›†åˆ,å³ç¼–ç¨‹è¯­è¨€ä¸­çš„Mapç±»å‹<br>å®Œç¾ä»£è¡¨å¯¹è±¡ï¼šthe perfect data type to represent objects (e.g. A User with a number of fields like name, surname, age, and so forth)<br>èŠ‚çœç©ºé—´ï¼ˆå†…å­˜ï¼Ÿï¼‰ï¼šA hash with a few fields (where few means up to one hundred or so) is stored in a way that takes very little space, so you can store millions of objects in a small Redis instance.<br>Every hash can store up to 2^32 - 1 field-value pairs (more than 4 billion). | é€‚åˆå­˜å‚¨å¯¹è±¡,å¹¶ä¸”å¯ä»¥åƒæ•°æ®åº“ä¸­updateä¸€ä¸ªå±æ€§ä¸€æ ·åªä¿®æ”¹æŸä¸€é¡¹å±æ€§å€¼(Memcachedä¸­éœ€è¦å–å‡ºæ•´ä¸ªå­—ç¬¦ä¸²ååºåˆ—åŒ–æˆå¯¹è±¡ä¿®æ”¹å®Œå†åºåˆ—åŒ–å­˜å›å») | HMSET user:1000 username antirez password P1pp0 age 34 <br>HGETALL user:1000 <br>HSET user:1000 password 12345 |
| List(åˆ—è¡¨)           | é“¾è¡¨(åŒå‘é“¾è¡¨ï¼Ÿ)<br>Redis Lists are simply lists of strings(å¦‚æœæ˜¯list of objectï¼Œobjectå¯ä»¥åºåˆ—åŒ–ä¸ºstringå­˜å‚¨), sorted by insertion order.<br>The max length of a list is 2^32 - 1 elements (4294967295, more than 4 billion of elements per list). | å¢åˆ å¿«,æä¾›äº†æ“ä½œæŸä¸€æ®µå…ƒç´ çš„API                             | 1,æœ€æ–°æ¶ˆæ¯æ’è¡Œç­‰åŠŸèƒ½(æ¯”å¦‚æœ‹å‹åœˆçš„æ—¶é—´çº¿) <br>2,æ¶ˆæ¯é˜Ÿåˆ—      |
| Set(é›†åˆ)            | å“ˆå¸Œè¡¨å®ç°,å…ƒç´ ä¸é‡å¤<br>The max number of members in a set is 2^32 - 1 (4294967295, more than 4 billion of members per set). | 1ã€æ·»åŠ ã€åˆ é™¤,æŸ¥æ‰¾çš„å¤æ‚åº¦éƒ½æ˜¯O(1) 2ã€ä¸ºé›†åˆæä¾›äº†æ±‚äº¤é›†ã€å¹¶é›†ã€å·®é›†ç­‰æ“ä½œ | 1ã€å…±åŒå¥½å‹<br> 2ã€åˆ©ç”¨å”¯ä¸€æ€§,ç»Ÿè®¡è®¿é—®ç½‘ç«™çš„æ‰€æœ‰ç‹¬ç«‹ip <br>3ã€å¥½å‹æ¨èæ—¶,æ ¹æ®tagæ±‚äº¤é›†,å¤§äºæŸä¸ªé˜ˆå€¼å°±å¯ä»¥æ¨è |
| Sorted Set(æœ‰åºé›†åˆ) | å°†Setä¸­çš„å…ƒç´ å¢åŠ ä¸€ä¸ªæƒé‡å‚æ•°score,å…ƒç´ æŒ‰scoreæœ‰åºæ’åˆ—       | æ•°æ®æ’å…¥é›†åˆæ—¶,å·²ç»è¿›è¡Œå¤©ç„¶æ’åº                              | 1ã€æ’è¡Œæ¦œ<br> 2ã€å¸¦æƒé‡çš„æ¶ˆæ¯é˜Ÿåˆ—                            |
| Bitmaps              |                                                              |                                                              |                                                              |
| HyperLogLogs         |                                                              |                                                              |                                                              |


#### Data operation
æˆ‘åœ¨stackoverflowä¸Šé¢çš„ç›¸å…³è§£ç­”ï¼š

https://stackoverflow.com/questions/46062283/what-is-the-difference-between-the-key-and-hash-key-parameters-used-in-a-redis-p/65406450#65406450

> basically in your scenario:
> ```
> your key is: userid:store:multi_select_choices
> your hashkey is: userid
> and your options objects serialized into jsonRedisValue
> ```
> in this case, you don't need to use:
> ```
> redisTemplate.opsForHash().put(key, hashKey, jsonRedisValue)
> ```
> instead you should use:
> ```
> redisTemplate.opsForValue().put(key, jsonRedisValue)
> ```
> here is a very good example for you to understand the scenario where opsForHash making sense:
>
> first you must understand that hashes in redis is perfect representation for objects, so you don't need to serialize the object, but just store the object in the format of multiple key-value pairs, like for a userid=1000, the object has properties: username/password/age, you can simply store it on redis like this:
>
> ```
> redisTemplate.opsForHash().put("userid:1000", "username", "Liu Yue")
> redisTemplate.opsForHash().put("userid:1000", "password", "123456")
> redisTemplate.opsForHash().put("userid:1000", "age", "32")
> ```
> later on if you want to change the password, just do this:
> ```
> redisTemplate.opsForHash().put("userid:1000", "password", "654321")
> ```
> and the corresponding cmd using redis-cli:
> ```
> HMSET userid:1000 username 'Liu Yue' password '123456' age 32
> HGETALL userid:1000
> 1) "username"
> 2) "Liu Yue"
> 3) "password"
> 4) "123456"
> 5) "age"
> 6) "32"
> HSET userid:1000 password '654321'
> HGETALL userid:1000
> 1) "username"
> 2) "Liu Yue"
> 3) "password"
> 4) "654321"
> 5) "age"
> 6) "32"
> ```
>
> I haven't explore too much the fundamental of how it implement hashes operation, but I think the difference between key and hashkey is quite obvious based on the documentation, key is just like the other redis key, normal string, hashkey is for the purpose of optimize the storage of the mutliple key-value pairs, so I guess there must be some kind of hash algorithm behind to ensure optimal memory storage and faster query and update.
>
> and it's well documented here:
>
> https://redis.io/topics/data-types
>
> https://redis.io/topics/data-types-intro

#### TTL key
 Returns the remaining time to live of a key that has a timeout.

### 4.2 StackExchange.Redis
Driver for .net: StackExchange.Redis 1.2https://github.com/StackExchange/StackExchange.Redis
for partial matching
Where are KEYS, SCAN, FLUSHDB etc? https://github.com/StackExchange/StackExchange.Redis/blob/41f427bb5ed8c23d0992a1411d0c92667b133d8e/docs/KeysScan.md

### 4.3 Python

```
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org redis-py-cluster
 from rediscluster import StrictRedisCluster
 rc = StrictRedisCluster(startup_nodes=redis_nodes, decode_responses=True)

>>> rc.smembers("bitcoin:stg_testnet:1514360")
>>> set([u'812853c7fe8bfa3f7d625895b3270245861f974f6ff19f8ce21317b5378be41e'])
>>> https://github.com/Grokzen/redis-py-cluster/blob/unstable/tests/test_commands.py
```

### 4.4 Java-Spring boot integration

https://docs.spring.io/spring-data/data-redis/docs/current/reference/html/

spring-boot-starter-data-redis ä¾èµ–äºspring-data-redis

spring-boot-starter-data-redis ä½¿ç”¨ï¼šhttps://zhuanlan.zhihu.com/p/80325707

spring-data-redis è§£æ https://juejin.im/post/5bac97606fb9a05cd8492e48

spring-data-redisä¾èµ–jedisæˆ–Lettuceï¼Œå®é™…ä¸Šæ˜¯å¯¹jedisè¿™äº›å®¢æˆ·ç«¯çš„å°è£…ï¼Œæä¾›ä¸€å¥—ä¸å®¢æˆ·ç«¯æ— å…³çš„api RedisTemplateä¾›åº”ç”¨ä½¿ç”¨ï¼Œä»è€Œä½ åœ¨ä»ä¸€ä¸ªrediså®¢æˆ·ç«¯åˆ‡æ¢ä¸ºå¦ä¸€ä¸ªå®¢æˆ·ç«¯ï¼Œä¸éœ€è¦ä¿®æ”¹ä¸šåŠ¡ä»£ç ã€‚

å…¶ä¸­spring boot å¯¹ RedisTemplateè¿›è¡Œäº†æŠ½è±¡ï¼ˆæ ‡å‡†åŒ–ï¼‰ï¼Œä»è€Œå¯ä»¥RedisTemplateå¯ä»¥é€‰æ‹©ä¸åŒçš„å…·ä½“å®ç°æ¯”å¦‚ lettuceï¼Œjedisï¼Œæ¯”å¦‚é€‰æ‹©LettuceConnectionFactory æ¥ä½œä¸ºå·¥å‚ï¼Œå½“ç„¶å¯ä»¥æ¢ç”¨ lettuceè‡ªå·±çš„ RedisClusterClientï¼Œä¸è¿‡ä½¿ç”¨RedisTemplateçš„å¥½å¤„å°±æ˜¯å¯ä»¥éšæ—¶ç”¨å…¶ä»–factoryæ›¿æ¢æ‰lettuceï¼Œæ¯”å¦‚é»˜è®¤çš„RedisConnectionFactory ã€‚



**åºåˆ—åŒ–ï¼š**æ ¸å¿ƒåŒ…æ˜¯org.springframework.data.redis.serializerï¼Œæƒ³è¦è‡ªå®šä¹‰è‡ªå·±çš„åºåˆ—åŒ–ï¼Œå®ç°RedisSerializerå³å¯ï¼Œé»˜è®¤æœ‰2ç§å®ç°JdkSerializationRedisSerializerå’ŒStringRedisSerializerï¼ŒRedisTemplateé»˜è®¤ä½¿ç”¨JdkSerializationRedisSerializer

**RedisTemplateæ“ä½œï¼š**

opsForValue

opsForHash

opsForList

opsForSet

opsForZSet

Redis Cluster

https://docs.spring.io/spring-data/data-redis/docs/current/reference/html/#cluster



## 5. Security hardening

Redis RU330è¯¾ç¨‹ Redis Security ç¬¬3å‘¨å­¦ä¹ ç¬”è®° https://blog.csdn.net/stevensxiao/article/details/113542159


## 6. Troubleshooting

### ERR CROSSSLOT Keys in request don't hash to the same slot.
æ¨¡ç³ŠæŸ¥è¯¢æ‰¹é‡æ¸…ç†keyså‡ºç°é”™è¯¯ï¼š
redis-cli KEYS "<PATTEN>" | xargs redis-cli DEL
redis-cli --scan --pattern "<PATTEN>" | xargs redis-cli DEL

æ ¹æºï¼š
```
# Enter redis-cli with cluster mode
$ redis-cli -c
# Try to delete multiple keys
127.0.0.1:6379>  DEL key1 key2 key3 ...
-- (error) CROSSSLOT Keys in request don't hash to the same slot
```
è§£å†³åŠæ³•ï¼š
```
./redis-scan-and-delete.sh:
#!/bin/bash

# Check if the required arguments are provided
if [ "$#" -lt 4 ]; then
    echo "Usage: $0 <redis_host> <redis_port> <database_number> <pattern>"
    exit 1
fi

# Set variables from arguments
REDIS_HOST="$1"
REDIS_PORT="$2"
DB_NUMBER="$3"
PATTERN="$4"

# Scan and delete keys matching the pattern
CURRENT_CURSOR=0
NEXT_CURSOR=0

while [ "$CURRENT_CURSOR" -ne "0" ]; do
    CURRENT_CURSOR=$NEXT_CURSOR

    # Scan for keys matching the pattern
    KEYS_COUNT=$($REDIS_CLI -h "${REDIS_HOST}" -p "${REDIS_PORT}" -n "${DB_NUMBER}" "SCAN ${CURRENT_CURSOR} COUNT 1000 MATCH '${PATTERN}'")

    # Delete the scanned keys
    for KEY in $KEYS_COUNT; do
        $REDIS_CLI -h "${REDIS_HOST}" -p "${REDIS_PORT}" -n "${DB_NUMBER}" "DEL $KEY"
    done

    # Get the next cursor
    NEXT_CURSOR=$(echo "$KEYS_COUNT" | awk -F' ' '{print $2}')
done

./redis-scan-and-delete.sh <redis_host> <redis_port> <database_number> "PREFIX_*"
```

[[Redis] Multi-key command in cluster mode (feat. CROSS-SLOT)](https://medium.com/@mbh023/redis-multi-key-command-in-cluster-mode-feat-cross-slot-ec27b999f169)

### RedisSystemException

1.org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is io.lettuce.core.RedisException: io.lettuce.core.RedisConnectionException: DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.

SOLUTION: redis-server redis.conf

### "Unable to connect to Redis; 

nested exception is io.lettuce.core.RedisException: Cannot retrieve initial cluster partitions from initial URIs [RedisURI [host='192.168.56.101', port=6379]]",
telnet result:

SOLUTION:
CONFIG SET protected-mode no
CONFIG REWRITE

### ERR Protocol error: invalid bulk length

https://github.com/xetorthio/jedis/issues/1034
https://stackoverflow.com/questions/6752894/predis-protocol-error-invalid-bulk-length

### Timeout issue
Line 15222: 2018-05-16 18:23:43,536 [32] DEBUG LaxinoV2Plugin - [Debug      ]Exception :
Timeout performing GET USERREPORT:SSSLZ:LZ:20658216:2bc1af86-d47c-45b8-b552-2d0b1f2078e5, inst: 1, mgr: ExecuteSelect, err: never, queue: 4750554, qu: 0, qs: 4750554, qc: 0, wr: 0, wq: 0, in: 65536, ar: 0, 
clientName: TW-SSS-UGS, serverEndpoint: 10.22.103.166:6379, keyHashSlot: 5897, IOCP: (Busy=0,Free=1000,Min=2,Max=1000), WORKER: (Busy=1,Free=32766,Min=2,Max=32767), 
Local-CPU: 37.95% (Please take a look at this article for some common client-side issues that can cause timeouts: 

https://github.com/StackExchange/StackExchange.Redis/tree/master/Docs/Timeouts.md

### [ERR] Node XXXX:6379 is not empty. 

Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0

```
$redis-cli -p 6379
127.0.0.1:6379> flushall
OK
127.0.0.1:6379> cluster reset
OK
127.0.0.1:6379> exit
```

### issue raised by above â€˜cluster restâ€™, the node become a standalone node, forgot other nodes!!!!

```
redis-cli --cluster help
redis-cli --cluster add-node <HOSTIP>:<PORT> <ANY EXIST HOSTIP>:<PORT> --cluster-slave
Redis [ERR] Nodes donâ€™t agree about configuration!
https://hzkeung.com/2018/02/25/redis-trib-check
redis-cli --cluster check <HOSTIP>:<PORT>
redis-cli -h <ANY EXIST HOSTIP> -p <PORT> cluster meet <HOSTIP> <PORT>
```

### promote slave node to master

simply delete it and then meet or re-add it

```
redis-cli --cluster del-node <HOSTIP>:<PORT> <NODEID>

>>> Removing node xxxx from cluster xxxxx:6379
>>> Sending CLUSTER FORGET messages to the cluster...
>>> SHUTDOWN the node.
>>> redis-cli -h <ANY EXIST HOSTIP> -p 6379 cluster meet <NEW HOSTIP> <PORT>
>>> redis-cli --cluster add-node <NEW HOSTIP>:<PORT> <ANY EXIST HOSTIP>:<PORT>
>>> redis-cli --cluster rebalance <NEW HOSTIP>:<PORT>
```

### failed delete node

Solv: meet then delete

```
$redis-cli --cluster del-node <HOSTIP>:<PORT> <NODEID>

>>> Removing node XXXXXXX from cluster XXXXX:6379
>>> Sending CLUSTER FORGET messages to the cluster...
>>> Node XXXXX:6381 replied with error:
>>> ERR Unknown node XXXXXXXX

redis-cli -h <ANY EXIST HOSTIP> -p <PORT> cluster meet <HOSTIP> <PORT>
```

### RedisTemplate can not access the node

1, the cluster information has been changed (add or stop nodes), RedisTemplate can not access the node

2. The cluster is not read and written, and the main read and write pressure is high.

**solve**ï¼š

method1ï¼š

```
SpringBoot2.3.0æˆ–ä»¥ä¸Šç‰ˆæœ¬
spring.redis.timeout=60s
spring.redis.lettuce.cluster.refresh.period=60s
spring.redis.lettuce.cluster.refresh.adaptive=true
```



method2ï¼š

```
@Configuration
public class RedisConfiguration {
 
 
    @Autowired
    private RedisProperties redisProperties;
 
    @Value("${redis.maxRedirects:3}")
    private int maxRedirects;
 
    @Value("${redis.refreshTime:5}")
    private int refreshTime;
 
    @Bean
    public LettuceConnectionFactory redisConnectionFactory() {
 
        RedisClusterConfiguration redisClusterConfiguration = new RedisClusterConfiguration(redisProperties.getCluster().getNodes());
 
        redisClusterConfiguration.setMaxRedirects(maxRedirects);
 
        / / Support adaptive cluster topology refresh and static refresh source
        ClusterTopologyRefreshOptions clusterTopologyRefreshOptions =  ClusterTopologyRefreshOptions.builder()
                .enablePeriodicRefresh()
                .enableAllAdaptiveRefreshTriggers()
                .refreshPeriod(Duration.ofSeconds(refreshTime))
                .build();
 
        ClusterClientOptions clusterClientOptions = ClusterClientOptions.builder()
                .topologyRefreshOptions(clusterTopologyRefreshOptions).build();
 
                 / / From the priority, read and write separation, read from the possible inconsistency, the final consistency CP
        LettuceClientConfiguration lettuceClientConfiguration = LettuceClientConfiguration.builder()
                .readFrom(ReadFrom.SLAVE_PREFERRED)
                .clientOptions(clusterClientOptions).build();
 
        return new LettuceConnectionFactory(redisClusterConfiguration, lettuceClientConfiguration);
    }
 
    @Bean
    public RedisTemplate<Object, Object> redisTemplate(LettuceConnectionFactory redisConnectionFactory) {
        RedisTemplate<Object, Object> redisTemplate = new RedisTemplate<>();
        redisTemplate.setConnectionFactory(redisConnectionFactory);
        redisTemplate.setKeySerializer(new StringRedisSerializer());
        redisTemplate.setValueSerializer(new GenericJackson2JsonRedisSerializer());
        redisTemplate.afterPropertiesSet();
        return redisTemplate;
    }
 
}
 
```

---
refer:
[å¤§åŠå¤œæ’æŸ¥bugï¼šç«Ÿç„¶æ˜¯åŒäº‹æŠŠRedisç”¨æˆè¿™é¬¼æ ·å­ï¼Œå‘äº†æˆ‘](https://mp.weixin.qq.com/s/m4mquHozppME92Vmb_LqIQ)
[ä¸ºäº†æ‹¿æ Redis æ•°æ®ç»“æ„ï¼Œæˆ‘ç”»äº† 40 å¼ å›¾ï¼ˆå®Œæ•´ç‰ˆï¼‰](https://mp.weixin.qq.com/s/MGcOl1kGuKdA7om0Ahz5IA)
<disqus/>


