---
sidebar: auto
sidebarDepth: 4
footer: MIT Licensed | Copyright Â© 2018-LIU YUE
---

[å›ç›®å½•](/docs/software)  ã€Škafkaã€‹



[Kafka Client sourcecode in Java](https://github.com/apache/kafka/tree/trunk/clients)

[kafka Server sourcecode in Scala](https://github.com/apache/kafka/tree/trunk/core/src/main/scala/kafka)

## 1. Basic concepts

+ zookeeperï¼škafka é€šè¿‡ zookeeper æ¥å­˜å‚¨é›†ç¾¤çš„ meta ä¿¡æ¯ã€‚

+ brokerï¼škafka é›†ç¾¤ä¸­åŒ…å«çš„æœåŠ¡å™¨ã€‚Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster. 

  Kafka service = kafka broker, kafka cluster=multiple instances of kafka broker

+ controllerï¼škafka é›†ç¾¤ä¸­çš„å…¶ä¸­ä¸€ä¸ªæœåŠ¡å™¨ï¼Œç”¨æ¥è¿›è¡Œ leader election ä»¥åŠ å„ç§ failoverã€‚

+ producerï¼šæ¶ˆæ¯ç”Ÿäº§è€…ï¼Œå‘å¸ƒæ¶ˆæ¯åˆ° kafka é›†ç¾¤çš„ç»ˆç«¯æˆ–æœåŠ¡ã€‚

+ consumerï¼šä» kafka é›†ç¾¤ä¸­æ¶ˆè´¹æ¶ˆæ¯çš„ç»ˆç«¯æˆ–æœåŠ¡ã€‚

+ Consumer groupï¼šhigh-level consumer API ä¸­ï¼Œæ¯ä¸ª consumer éƒ½å±äºä¸€ä¸ª consumer groupï¼Œæ¯æ¡æ¶ˆæ¯åªèƒ½è¢« consumer group ä¸­çš„ä¸€ä¸ª Consumer æ¶ˆè´¹ï¼Œä½†å¯ä»¥è¢«å¤šä¸ª consumer group æ¶ˆè´¹ã€‚

  + group coordinator: is nothing but one of the brokers which receives heartbeats (or polling for messages) from all consumers of a consumer group. Every consumer group has a group coordinator. If a consumer stops sending heartbeats, the coordinator will trigger a rebalance.
  + consumer coordinator: each kafkaconsumer instance has a private member of consumer coordinator
  + consumer group leader:  is one of the consumer in a consumer group. 

  + subscribe mode VS assign mode

    å…ˆæ¥çœ‹ä¸€æ®µè¯

    > By having a notion of parallelismâ€”the partitionâ€”within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that **each partition is consumed by exactly one consumer in the group. **By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. **Note however that there cannot be more consumer instances in a consumer group than partitions. **
    >
    > **If we add more consumers to a single group with a single topic than we  have partitions, some of the consumers will be idle and get no messages  at all.**

    æ³¨æ„åˆ°è¿™å¥è¯ï¼šhowever that there cannot be more consumer instances in a consumer group than partitions. 

    å®é™…ä¸Šè¿™ä¸ªæ˜¯æŒ‡çš„æ˜¯assign modeä¸‹ï¼ŒåŒä¸€ä¸ªconsumer groupä¸èƒ½å¤šä¸ªconsumeræ¥assignåˆ°åŒä¸€ä¸ªpartition:

    ```
    Properties props = new Properties();
    props.put(ConsumerConfig.GROUP_ID_CONFIG, "MyConsumerGroup");
    props.put("enable.auto.commit", "false");
    consumer = new KafkaConsumer<>(props);
    TopicPartition partition0 = new TopicPartition("mytopic", 0);
    consumer.assign(Arrays.asList(partition0));
    ConsumerRecords<Integer, String> records = consumer.poll(1000);
    ```

    å¦‚æœä¸€ä¸ªconsumer groupå¯åŠ¨å¤šä¸ªconsumeréƒ½é€‰æ‹©è¿™ç§assignæ¨¡å¼ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰é—®é¢˜ï¼Œå› ä¸º

    It is important to recall that Kafka keeps one offset per [consumer-group, topic, partition]. That is the reason.

    å°±æ˜¯kafkaè‡ªåŠ¨ç»´æŠ¤çš„__consumer_offsetæ˜¯æŒ‰ç…§[consumer-group, topic, partition]æ¥çš„[ä½†æ˜¯è¿™é‡Œå·²ç»ç½®ä¸ºfalseäº†ï¼Œæ‰€ä»¥åº”è¯¥æ²¡æœ‰é—®é¢˜ï¼Œå¦‚æœæ˜¯trueå°±æœ‰é—®é¢˜äº†ï¼Œå¤šä¸ªconsumerè¯»å†™åŒä¸€ä¸ª[consumer-group, topic, partition]å°±ä¼šæœ‰å†²çª]

    è€Œå¦‚æœæ˜¯ä½¿ç”¨subscribe modeï¼Œå°±ä¼šè‡ªåŠ¨è¿›è¡Œrebalanceï¼Œå¦‚æœåŒä¸€ä¸ªconsumer groupä¸­çš„instanceå¤šäºpartitionï¼Œé‚£ä¹ˆæ²¡æœ‰é—®é¢˜ï¼Œå¤§ä¸äº†consumerå°±idleæˆ–standbyè€Œå·²

  

+ Topics & logs (Logical concept)ï¼šæ¯æ¡å‘å¸ƒåˆ° kafka é›†ç¾¤çš„æ¶ˆæ¯å±äºçš„ç±»åˆ«ï¼Œå³ kafka æ˜¯é¢å‘ topic çš„ã€‚

   + LEO:: log end offset	offset+1
   + ISR:: in-sync replicas

   + internal topic:

     + __consumer_offsets

       ```
       key:   <consumer_group>,<topic>,<partition>
       value: <offset>,<partition_leader_epoch>,<metadata>,<timestamp>
       ```

+ Partition (Physical concept): æ˜¯ç‰©ç†ä¸Šçš„æ¦‚å¿µï¼Œæ¯ä¸ª topic åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª partitionã€‚kafka åˆ†é…çš„å•ä½æ˜¯ partitionã€‚

  Ordering (global order == one partition only)
  	
  Each partition is a totally ordered log, but there is no global ordering between partitions (other than perhaps some wall-clock time you might include in your messages). The assignment of the messages to a particular partition is controllable by the writer, with most users choosing to partition by some kind of key (e.g. user id). Partitioning allows log appends to occur without co-ordination between shards and allows the throughput of the system to scale linearly with the Kafka cluster size.

+ replicaï¼špartition çš„å‰¯æœ¬ï¼Œä¿éšœ partition çš„é«˜å¯ç”¨ã€‚

  **high watermark**: indicated the offset of messages that are fully  replicated, while the end-of-log offset might be larger if there are  newly appended records to the leader partition which are not replicated  yet.

+ leaderï¼šreplica ä¸­çš„ä¸€ä¸ªè§’è‰²ï¼Œ producer å’Œ consumer åªè·Ÿ leader äº¤äº’ã€‚

  Leader = topic leader = leader of replicas

+ followerï¼šreplica ä¸­çš„ä¸€ä¸ªè§’è‰²ï¼Œä» leader ä¸­å¤åˆ¶æ•°æ®ã€‚
  	

Overview:

![](/docs/docs_image/software/buildingblock/kafka/kafka_architect.png)

![](/docs/docs_image/software/buildingblock/kafka/kafka11.png)

![](/docs/docs_image/software/buildingblock/kafka/kafka01.png)

![](/docs/docs_image/software/buildingblock/kafka/kafka22.png)

![](/docs/docs_image/software/buildingblock/kafka/kafka_zookeeper_nodes.png)

![](/docs/docs_image/software/buildingblock/kafka/kafka_zookeeper_nodes2.png)

å›¾ä¸­ä¸åŒtopicçš„ä¸åŒçš„partitionå¯èƒ½ä½äºåŒä¸€ä¸ªkafka brokeræˆ–è€…ä¸åŒçš„kafka brokerï¼Œå…·ä½“åœ¨å“ªé‡Œï¼Œå®Œå…¨å¯ä»¥å»æ¯ä¸ªkafkaèŠ‚ç‚¹ä¸‹é¢å¯»æ‰¾ï¼Œè·¯å¾„ï¼š

`/kafka/kafka-logs/<TOPIC-PARTITION>`



## 2. å®‰è£…ä½¿ç”¨

### 2.1 å®‰è£… install

#### å®‰è£…kafka

```shell
------------------------------------------------------------------------
--- Creating a User for Kafka
------------------------------------------------------------------------
sudo useradd kafka -m
sudo passwd kafka
for ubuntu: sudo adduser kafka sudo
for centos: sudo usermod -aG wheel kafka
su -l kafka

------------------------------------------------------------------------
-- Downloading and Extracting the Kafka Binaries
------------------------------------------------------------------------
mkdir ~/Downloads
curl "https://www.apache.org/dist/kafka/2.1.1/kafka_2.11-2.1.1.tgz" -o ~/Downloads/kafka.tgz
mkdir ~/kafka && cd ~/kafka
tar -xvzf ~/Downloads/kafka.tgz --strip 1 ï¼ˆWe specify the --strip 1 flag to ensure that the archiveâ€™s contents are extracted in ~/kafka/ itself and not in another directory (such as ~/kafka/kafka_2.11-2.1.1/) inside of itï¼‰

------------------------------------------------------------------------
--- Configuring the Kafka Server
------------------------------------------------------------------------
vim ~/kafka/config/server.properties:

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

port=9092
host.name=X.X.X.48
advertised.host.name=X.X.X.48
advertised.port=9092

delete.topic.enable = true
log.retention.hours=168
log.dirs=/opt/kafka_2.12-2.2.0/kafka-logs
#å¤–ç½®zookeeper
zookeeper.connect=1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181

------------------------------------------------------------------------
--- Option 1: Creating Systemd Unit Files and Starting the Kafka Server
------------------------------------------------------------------------
sudo vim /etc/systemd/system/zookeeper.service
[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties
ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target

The [Unit] section specifies that Zookeeper requires networking and the filesystem to be ready before it can start.
The [Service] section specifies that systemd should use the zookeeper-server-start.sh and zookeeper-server-stop.sh shell files for starting and stopping the service. It also specifies that Zookeeper should be restarted automatically if it exits abnormally.

sudo vim /etc/systemd/system/kafka.service
[Unit]
Requires=zookeeper.service
After=zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties > /home/kafka/kafka/kafka.log 2>&1'
ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
The [Unit] section specifies that this unit file depends on zookeeper.service. This will ensure that zookeeper gets started automatically when the kafka service starts.
The [Service] section specifies that systemd should use the kafka-server-start.sh and kafka-server-stop.sh shell files for starting and stopping the service. It also specifies that Kafka should be restarted automatically if it exits abnormally. 

sudo systemctl start kafka
sudo journalctl -u kafka
sudo systemctl enable kafka

------------------------------------------------------------------------
--- Option 2: ç¼–å†™å¯åŠ¨è„šæœ¬
------------------------------------------------------------------------
readonly PROGNAME=$(basename $0)
readonly PROGDIR=$(readlink -m $(dirname $0))

# source env
L_INVOCATION_DIR="$(pwd)"
L_CMD_DIR="/opt/scripts"

if [ "${L_INVOCATION_DIR}" != "${L_CMD_DIR}" ]; then
  pushd ${L_CMD_DIR} &> /dev/null
fi

#source ../set_env.sh

#--------------- Function Definition ---------------#
showUsage() {
  echo "Usage:"
  echo "$0 kafka start|kill"
  echo ""
  echo "--start or -b:  Start kafka"
  echo "--kill or -k:   Stop kafka"
}

#---------------  Main ---------------#

# Parse arguments
while [ "${1:0:1}" == "-" ]; do
  case $1 in
    --start)
      L_FLAG="B"
      ;;
    --kill)
      L_FLAG="K"
          ;;
     --status)
      L_FLAG="S"
      ;;
    *)
      echo "Unknown option: $1"
          echo ""
      showUsage
          echo ""
      exit 1
      ;;
  esac
  shift
done

L_RETURN_FLAG=0 # 0 for success while 99 for failure

KAFKA_HOME=/opt/kafka_2.12-2.2.0/bin
ZK_CLUSTER=$HOST1:2181,$HOST2:2181,$HOST3:2181

pushd ${KAFKA_HOME} &>/dev/null

if [ "$L_FLAG" == "B" ]; then
        echo "Starting kafka service..."
        ./kafka-server-start.sh -daemon ../config/server.properties
elif
        echo "Stopping kafka service..."
        ./kafka-server-stop.sh -daemon ../config/server.properties
elif [ "$L_FLAG" == "S" ]; then
        echo "Checking kafka status..."
        arr=(${ZK_CLUSTER//","/ })
        echo "${arr[@]}"
        for ZK_NODE in "${arr[@]}";
        do
                echo "ZK_NODE: $ZK_NODE"
                ./zookeeper-shell.sh $ZK_NODE ls /brokers/ids
                exit_code=$?
                echo $exit_code
                if [ "$exit_code" = "0" ]; then
                        exit 1
                fi
        done
fi

exit $L_RETURN_FLAG

æ³¨æ„ï¼š
å¦‚æœæ˜¯ä½¿ç”¨äº‘ä¸Šçš„DMSï¼Œzookeeperæ˜¯ä¸å¼€æ”¾çš„ï¼Œæ‰€ä»¥æŸ¥è¯¢èŠ‚ç‚¹å¯ä»¥æ¢ç”¨ï¼š
kafka/bin/kafka-broker-api-versions.sh --bootstrap-server "xxxx" | awk '/id/{print $1}'	
------------------------------------------------------------------------
--- Restricting the Kafka User  as a security precaution. 
------------------------------------------------------------------------
This step in the prerequisite disables sudo access for the kafka user
for ubuntu:
sudo deluser kafka sudo
for centos:
sudo gpasswd -d kafka wheel

sudo passwd kafka -l (å¯¹åº”unlockï¼šsudo passwd kafka -u)
sudo su - kafka


```

#### ä½¿ç”¨external zookeeper

```
kafkaé…ç½®ï¼š

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000

zookeeperé…ç½®ï¼š
This example is for a 3 node ensembleï¼š

# The number of milliseconds of each tick                      
tickTime=2000                                                  
# The number of ticks that the initial                         
# synchronization phase can take                               
initLimit=10                                                   
# The number of ticks that can pass between                    
# sending a request and getting an acknowledgement             
syncLimit=5                                                    
# the directory where the snapshot is stored.                  
# do not use /tmp for storage, /tmp here is just               
# example sakes.                                               
dataDir=/zookeeper-3.4.8/zkdata           
dataLogDir=/zookeeper-3.4.8/logs          
# the port at which the clients will connect                   
clientPort=2181                                                
server.1=1.1.1.1:2888:3888                               
server.2=1.1.1.2:2888:3888                               
server.3=1.1.1.3:2888:3888                               
SERVER_JVMFLAGS=-Xmx1024m'                                     
```



### 2.2 ä½¿ç”¨ Usage

GUIï¼š

+ KafkaEsque  https://kafka.esque.at

+ https://github.com/airbnb/kafkat


å•æœºå‘½ä»¤ --broker-list
é›†ç¾¤å‘½ä»¤ --bootstrap-server


#### 2.2.1 å•æœº Local è°ƒè¯•

Quick start
https://kafka.apache.org/quickstart 

```
Start zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

Start kafka server
bin/kafka-server-start.sh config/server.properties
bin/kafka-server-start.sh config/server-1.properties &
bin/kafka-server-start.sh config/server-2.properties &

config/server-1.properties:
    broker.id=1
    listeners=PLAINTEXT://:9093
    log.dirs=/tmp/kafka-logs-1

```

##### Create topic

```
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic

bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1  --topic mytopic

#You can use kafka-topics.sh to see how the Kafka topic is laid out among the Kafka brokers. The ---describe will show partitions, ISRs, and broker partition leadership.
[test@localhost kafka_2.12-2.2.0]$ bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
Topic:my-replicated-topic       PartitionCount:1        ReplicationFactor:3     Configs:segment.bytes=1073741824
        Topic: my-replicated-topic      Partition: 0    Leader: 1       Replicas: 1,2,0 Isr: 1,2,0
```

##### Reassign Partition

```
kafka-reassign-partitions.sh
-- 1.generate current assignment
kafka-reassign-partitions --zookeeper hostname:port --topics-to-move-json-file topics to move.json --broker-list broker 1, broker 2 --generate
-- 2.modify and apply
kafka-reassign-partitions --zookeeper hostname:port  --reassignment-json-file reassignment configuration.json --bootstrap-server hostname:port --execute
-- 3.verify
kafka-reassign-partitions --zookeeper hostname:port --reassignment-json-file reassignment configuration.json  --bootstrap-server hostname:port --verify
```



##### Producer 

```
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic

# for test 
bin/kafka-verifiable-producer.sh --topic consumer-tutorial --max-messages 200000 --broker-list localhost:9092
```

##### Consumer 

```
bin/kafka-topics.sh --list --zookeeper localhost:2181
bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic "ngs.svl.20220519.fib.result"

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic

bin/kafka-console-consumer.sh --bootstrap-server <ä½ çš„kafkaé…ç½®> --topic T-RISK --partition 0 --offset 3350 --max-messages 1

#inspect partition assignments and consumption progress 
bin/kafka-consumer-groups.sh --new-consumer --describe --group consumer-tutorial-group --bootstrap-server localhost:9092 
=>
shows all the partitions assigned within the consumer group, which consumer instance owns it, and the last committed offset (reported here as the â€œcurrent offsetâ€). The lag of a partition is the difference between the log end offset and the last committed offset. 

#count total messages for a topic:
kafka-console-consumer.sh \
--from-beginning \
--bootstrap-server <BROKER:PORT> \
--property print.key=true \
--property print.value=false \
--property print.partition \
--topic <TOPIC_NAME> | tail -n 10|grep "Msg Count="

$ kafka-run-class.sh kafka.tools.GetOffsetShell \
--broker-list <HOST1:PORT,HOST2:PORT> \
--topic <TOPIC_NAME>

kafka-run-class.sh kafka.admin.ConsumerGroupCommand \
--group <GROUP_NAME> \
--bootstrap-server localhost:9092 \
--describe

kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
--topic <TOPIC_NAME> \
--zookeeper localhost:2181 \
--group <GROUP_NAME>


#!/usr/bin/bash
# Copyright (c) 2016 AsiaInvestment Pte. Ltd. Singapore
# All rights reserved.
BOOTS_STRAP_SERVER=127.0.0.1:9092
ZK_SERVER=127.0.0.1:2181
pushd /kafka_2.12-2.2.0/bin &>/dev/null
echo "#################################"
echo "### TOPICS"
echo "#################################"
topics=(`./kafka-topics.sh --list --zookeeper $ZK_SERVER | grep -v grep  | awk '{print $1}'`)
for topic in ${topics[@]}
do
./kafka-topics.sh --describe --bootstrap-server $BOOTS_STRAP_SERVER --topic $topic
done
echo "#################################"
echo "### CONSUMER GROUP"
echo "#################################"
consumer_groups=(`./kafka-consumer-groups.sh --list --bootstrap-server $BOOTS_STRAP_SERVER | grep -v grep | awk '{print $1}'`)
for group in ${consumer_groups[@]}
do
echo "                  >>>group:$group<<<"
./kafka-consumer-groups.sh --describe --group $group --bootstrap-server $BOOTS_STRAP_SERVER
done

popd &>/dev/null

```

#### 2.2.2  è™šæ‹Ÿæœºè¿œç¨‹è°ƒè¯• Remote 

**VM Host only mode**

```
systemctl stop firewalld

Test from windows bat
.\kafka-console-producer.bat --broker-list x.x.x.x:9092 --topic test
C:\Workspace\Temp\kafka_2.12-2.2.1\bin\windows> .\kafka-console-consumer.bat --bootstrap-server x.x.x.x:9092 --from-beginning --topic test

Test from windows with python
pip install python-kafka
#python NO brokeravaialble
producer = KafkaProducer(bootstrap_servers=['x.x.x.x:9092'], api_version=(0,10))
#assert type(value_bytes) in (bytes, bytearray, memoryview, type(None)) AssertionError
 producer.send('test', 'hi'.encode('utf-8'))

```
![](/docs/docs_image/software/buildingblock/kafka/kafka02.png)

**Failed on VM NAT mode**

Someone succeed: https://boristyukin.com/connecting-to-kafka-on-virtualbox-from-windows/
But keep got: org.apache.kafka.common.errors.TimeoutException: Topic not present in metadata after 60000 ms

Port forward: 9092
Changed vm hostname
And map on host machine

When producer/consumer accesses the Kafka broker, the Kafka broker returns its host name for data producer or consumer at default settings. So producers/consumers need to resolve broker's host name to IPAddress.
For broker returning an arbitrary host name, use the advertised.listeners settings.
listeners vs. advertised.listeners https://stackoverflow.com/questions/42998859/kafka-server-configuration-listeners-vs-advertised-listeners

Turned on log level to DEBUG

May be need to do something like:
https://forums.virtualbox.org/viewtopic.php?f=1&t=29990
https://github.com/trivago/gollum/issues/93

Telnet 127.0.0.1/guesthost 9092 WORKS, but it doesnâ€™t mean can reach to the guest machine, simply because of port forwarding opened 9092 port on host

![](/docs/docs_image/software/buildingblock/kafka/kafka03.png)

**Final vm workaround (NAT+HOSTONLY)**
vim /etc/sysconfig/network-scripts/ifcfg-enp0s3

![](/docs/docs_image/software/buildingblock/kafka/kafka04.png)

#### 2.2.3 é›†æˆå¼€å‘ springboot

##### spring-kafka

https://www.baeldung.com/spring-kafka

org.springframework.kafkaåŒ…å«ï¼š

+ spring-context
+ spring-messaging
+ spring-tx
+ spring-retry
+ kafka-clients

##### apache-kafka

org.apache.kafka.kafka-client 

https://docs.confluent.io/clients-kafka-java/current/overview.html

https://www.baeldung.com/kafka-exactly-once

https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html

https://dzone.com/articles/kafka-producer-and-consumer-example

é»˜è®¤é…ç½®ï¼š

https://kafka.apache.org/documentation.html#configuration
https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/ConsumerConfig.html




### 2.3 ç®¡ç†ç»´æŠ¤ Maintain

https://kafka.apache.org/documentation/#operations

#### 2.3.1 èŠ‚ç‚¹çŠ¶æ€

```
------------------------------------------
--- zookeeper status
------------------------------------------
/zookeeper/bin/zkServer.sh status

------------------------------------------
--- æŸ¥çœ‹kafka brokerèŠ‚ç‚¹
------------------------------------------
>/zookeeper/bin/zkCli.sh -server localhost:2181 #Make sure your Broker is already running
#ls /brokers/ids # Gives the list of active brokers
#ls /brokers/topics #Gives the list of topics
#get /brokers/ids/0 #Gives more detailed information of the broker id '0'
```

#### 2.3.2 House keeping

##### kafka offset not increase by 1

https://stackoverflow.com/questions/54636524/kafka-streams-does-not-increment-offset-by-1-when-producing-to-topic

https://issues.apache.org/jira/browse/KAFKA-6607

[http://trumandu.github.io/2019/04/13/%E5%A6%82%E4%BD%95%E7%9B%91%E6%8E%A7kafka%E6%B6%88%E8%B4%B9Lag%E6%83%85%E5%86%B5/](http://trumandu.github.io/2019/04/13/å¦‚ä½•ç›‘æ§kafkaæ¶ˆè´¹Lagæƒ…å†µ/)

https://stackoverflow.com/questions/54544074/how-to-make-restart-able-producer

 https://github.com/confluentinc/confluent-kafka-go/issues/195

##### clean up 

__consumer_offsets https://stackoverflow.com/questions/41429053/how-to-change-consumer-offsets-cleanup-plicy-to-delete-from-compact

##### partition

1 partition 2 replica, where is the replica?? On the same parition but different segment??

Alter:
	bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic my_topic_name \
      --partitions 40
kafka-reassign-partitionså‘½ä»¤æ˜¯é’ˆå¯¹Partitionè¿›è¡Œé‡æ–°åˆ†é…ï¼Œè€Œä¸èƒ½å°†æ•´ä¸ªTopicçš„æ•°æ®é‡æ–°å‡è¡¡åˆ°æ‰€æœ‰çš„Partitionä¸­ã€‚
https://segmentfault.com/a/1190000011721643
https://cloud.tencent.com/developer/article/1349448

Be aware that one use case for partitions is to semantically partition data, and adding partitions doesn't change the partitioning of existing data so this may disturb consumers if they rely on that partition. That is if data is partitioned by hash(key) % number_of_partitions then this partitioning will potentially be shuffled by adding partitions but Kafka will not attempt to automatically redistribute data in any way.

#### 2.3.3 å·¥å…·/æ—¥å¿—æ’æŸ¥

https://kafka.apache.org/documentation/#monitoring

##### ç³»ç»Ÿæ’æŸ¥

```
$ jps
17303 Jps
29819 Kafka
$ ll /proc/29819/fd
```



##### æ—¥å¿—ä½ç½®

+ /kafka/logs

  controller.log.2021-03-01-00 kafkaé›†ç¾¤é€‰ä¸¾å‡ºçš„Controllerè´Ÿè´£é€šä¿¡
  server.log.2021-03-01-00 å…·ä½“è´Ÿè´£æŸä¸ªpartition replicaçš„leaderçš„æ—¥å¿—ä»¥åŠè´Ÿè´£æŸä¸ªconsumer groupçš„group coordinatoræ—¥å¿—
  state-change.log.2021-03-01 è®°å½•topic partitionçš„online offlineç­‰çŠ¶æ€ä¿¡æ¯

  kafkaServer.outè·Ÿserver.logä¸€æ ·ï¼Œåªæ˜¯ä¼šå®šæœŸarchiveåˆ°server.log

+ /kafka/kafka-logs

   __consumer_offsets-0/

  __transaction_state-0/

  [TOPIC]-[PARTITION]/

##### kafka serverç«¯æ—¥å¿—è§£æ

```
---------------------------------------------------------------------------
--- create/delete topic, from controller.log
1ï¼‰è¿™ä¸ªæ˜¯å‘½ä»¤åˆ›å»ºçš„
./kafka-topics.sh --create --bootstrap-server $KFK_CLUSTER --replication-factor 2 --partitions 3 --topic T-XXX
replicaæ˜¯2ï¼Œæ‰€ä»¥å¯¹åº”ä¸¤ä¸ªbrokerï¼Œæ‰€ä»¥åé¢è¿˜è¦é€‰ä¸€ä¸ªprefer replicaä½œä¸ºleader
[2021-04-14 11:04:11,239] INFO [Controller id=0] New topics: [Set(T-XXX)], deleted topics: [Set()], new partition replica assignment [Map(T-XXX-2 -> Vector(3, 0), T-XXX-1 -> Vector(0, 1), T-XXX-0 -> Vector(1, 3))] (kafka.controller.KafkaController)

--- elect TOPIC-PARTITION replica leader
INFO [Controller id=0] Partition T-XXX-2 completed preferred replica leader election. New leader is 3 (kafka.controller.KafkaController)

2ï¼‰ä¸‹é¢è¿™ä¸ªå†…éƒ¨topicæ˜¯ä½•æ—¶è¢«åˆ›å»ºçš„ï¼ˆæ›´å‡†ç¡®çš„åº”è¯¥æ˜¯è¯´å…¶å„ä¸ªåˆ†åŒºæ˜¯ä½•æ—¶åˆ›å»ºçš„ï¼‰
https://cloud.tencent.com/developer/news/19958
__consumer_offsetsåˆ›å»ºçš„æ—¶æœºæœ‰å¾ˆå¤šç§ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
brokerå“åº”FindCoordinatorRequestè¯·æ±‚æ—¶
brokerå“åº”MetadataRequestæ˜¾å¼è¯·æ±‚__consumer_offsetså…ƒæ•°æ®æ—¶
å…¶ä¸­ä»¥ç¬¬ä¸€ç§æœ€ä¸ºå¸¸è§ï¼Œè€Œç¬¬ä¸€ç§æ—¶æœºçš„è¡¨ç°å½¢å¼å¯èƒ½æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ç”¨æˆ·å¯åŠ¨äº†ä¸€ä¸ªæ¶ˆè´¹è€…ç»„(ä¸‹ç§°consumer group)è¿›è¡Œæ¶ˆè´¹æˆ–è°ƒç”¨kafka-consumer-groups --describeç­‰

æ³¨æ„ï¼Œå„è‡ªåˆ†åŒºéƒ½æ˜¯å¯¹åº”åˆ°ä¸€ä¸ªbrokerï¼Œæ‰€ä»¥consumer groupä¹Ÿå°±æ˜¯ç›´æ¥å¯¹åº”åˆ°äº†ç›¸åº”çš„brokerï¼ˆgroup coordinatorï¼‰
[2021-04-14 11:14:20,601] INFO [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-22 -> Vector(3), __consumer_offsets-30 -> Vector(1), __consumer_offsets-8 -> Vector(0), __consumer_offsets-21 -> Vector(1), __co
å¯¹åº”åˆ°é¡¹ç›®ä»£ç åº”è¯¥å°±æ˜¯æ¶ˆè´¹ç»„ consumer groupå¯åŠ¨çš„æ—¶å€™

3ï¼‰è¿™ä¸ªæ˜¯ä»£ç åˆ›å»ºçš„ï¼š
auto.create.topics.enable=trueï¼Œä»£ç è¯»å–T-XXX-SNPå°±ä¼šåˆ›å»ºï¼š
SNPå°±æ˜¯æˆ‘ä»¬åé¢ä¼šæåˆ°çš„æ‰€è°“è‡ªå·±ç»´æŠ¤çš„å¢é‡å¿«ç…§
é»˜è®¤ç”¨äº†1ä¸ªreplicaï¼Œæ‰€ä»¥ç›´æ¥å°±å¯¹åº”æŸä¸ªbroker
[2021-04-14 11:14:27,347] INFO [Controller id=0] New topics: [Set(T-XXX-SNP)], deleted topics: [Set()], new partition replica assignment [Map(T-sss-SNP-2 -> Vector(3), T-XXX-SNP-1 -> Vector(1), T-XXX-SNP-0 -> Vector(0))] (kafka.controller.KafkaController)
[2021-04-14 11:14:27,347] INFO [Controller id=0] New partition creation callback for T-XXX-SNP-2,T-XXX-SNP-1,T-XXX-SNP-0 (kafka.controller.KafkaController)

æœ€åå˜æˆä¸€å¼ broker mapï¼š
[2021-04-14 11:18:36,979] DEBUG [Controller id=0] Preferred replicas by broker Map(1 -> Map(T-JOB-SNP-0 -> Vector(1),  __consumer_offsets-27 -> Vector(1), T-TEST-1 -> Vector(1, 0), __transaction_state-2 -> Vector(1, 3), __transaction_state-20 -> Vector(1, 3), __consumer_offsets-33 -> Vector(1), T-DBMS-SNP-0 -> Vector(1), T-CAPTURE-0 -> Vector(1, 0), __consumer_offsets-36 -> Vector(1), __transaction_state-29 -> Vector(1, 0), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __transaction_state-38 -> Vector(1, 3), T-TEST-SNP-2 -> Vector(1, 3), T-MEMBER-1 -> Vector(1, 0), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), T-EOD-1 -> Vector(1, 0), T-QUOTATION-2 -> Vector(1, 0), __transaction_state-14 -> Vector(1, 3), __transaction_state-44 -> Vector(1, 3), T-RISK-0 -> Vector(1, 3), T-RISK-SNP-2 -> Vector(1), __transaction_state-32 -> Vector(1, 3), __consumer_offsets-48 -> Vector(1), T-CAPTURE-SNP-1 -> Vector(1), T-TEST-0 -> Vector(1, 0), T-EOD-SNP-1 -> Vector(1), __transaction_state-17 -> Vector(1, 0), __transaction_state-23 -> Vector(1, 0), __transaction_state-47 -> Vector(1, 0), __consumer_offsets-6 -> Vector(1), T-QUOTATION-SNP-1 -> Vector(1), __transaction_state-26 -> Vector(1, 3), T-JOB-2 -> Vector(1, 0), __transaction_state-5 -> Vector(1, 0), __transaction_state-8 -> Vector(1, 3)

---------------------------------------------------------------------------
--- elect Controller from controller.log
[2021-04-14 10:53:31,013] DEBUG [Controller id=1] Broker 0 has been elected as the controller, so stopping the election process. (kafka.controller.KafkaController)


---------------------------------------------------------------------------
--- rebalance by group coordinator broker 3,from server.log or kafkaServer.out
[2021-04-15 08:24:20,809] INFO [GroupCoordinator 3]: Preparing to rebalance group XXXX-SZL in state PreparingRebalance with old generation 0 (__consumer_offsets-28) (reason: Adding new member consumer-1-11aac9d7-8a72-44fe-bf5c-0519941bbb6a) (kafka.coordinator.group.GroupCoordinator)
[2021-04-15 08:24:20,811] INFO [GroupCoordinator 3]: Stabilized group XXX-SZL generation 1 (__consumer_offsets-28) (kafka.coordinator.group.GroupCoordinator)
[2021-04-15 08:24:20,823] INFO [GroupCoordinator 3]: Assignment received from leader for group XXXX-SZL for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2021-04-15 08:24:22,543] INFO [TransactionCoordinator id=3] Initialized transactionalId XXX-TID-0 with producerId 1004 and producer epoch 1 on partition __transaction_state-15 (kafka.coordinator.transaction.TransactionCoordinator)
[2021-04-15 08:24:23,957] INFO [GroupCoordinator 3]: Preparing to rebalance group XXXX-SZL in state PreparingRebalance with old generation 0 (__consumer_offsets-49) (reason: Adding new member consumer-1-416c9379-0f89-48f4-b125-eadf648d57c7) (kafka.coordinator.group.GroupCoordinator)
[2021-04-15 08:24:23,959] INFO [GroupCoordinator 3]: Stabilized group XXXXE-SZL generation 1 (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)
[2021-04-15 08:24:23,972] INFO [GroupCoordinator 3]: Assignment received from leader for group XXX-SZL for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2021-04-15 08:24:25,524] INFO [TransactionCoordinator id=3] Initialized transactionalId XXXX-TID-0 with producerId 1005 and producer epoch 1 on partition __transaction_state-18 (kafka.coordinator.transaction.TransactionCoordinator)

---------------------------------------------------------------------------
--- closed kafka client, from server.log or kafkaServer.out

[2021-04-16 17:51:05,705] INFO [GroupCoordinator 3]: Member consumer-1-416c9379-0f89-48f4-b125-eadf648d57c7 in group TEST-REALTIME-SZL has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2021-04-16 17:51:05,707] INFO [GroupCoordinator 3]: Preparing to rebalance group TEST-REALTIME-SZL in state PreparingRebalance with old generation 1 (__consumer_offsets-49) (reason: removing member consumer-1-416c9379-0f89-48f4-b125-eadf648d57c7 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
å®¢æˆ·ç«¯consumer groupæ‰€æœ‰çš„consumeréƒ½åœæ‰äº†ï¼Œæ‰€ä»¥æ˜¯emptyï¼Œç„¶åæ•´ä¸ªgroupå®£å¸ƒdead
[2021-04-16 17:51:05,707] INFO [GroupCoordinator 3]: Group XXX-SZL with generation 2 is now empty (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)
[2021-04-16 17:53:56,956] INFO [GroupMetadataManager brokerId=3] Group XXX-SZL transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)

---------------------------------------------------------------------------
---shutdown kafka borker 3, from server.log or kafkaServer.out
[2021-04-15 08:59:31,965] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2021-04-15 08:59:31,973] INFO [KafkaServer id=3] shutting down (kafka.server.KafkaServer)
[2021-04-15 08:59:31,977] INFO [KafkaServer id=3] Starting controlled shutdown (kafka.server.KafkaServer)
[2021-04-15 08:59:32,066] INFO [ReplicaFetcherManager on broker 3] Removed fetcher for partitions Set(__transaction_state-45, __transaction_state-27, __transaction_state-9, T-XXX-2, T-XXX-1, __transaction_state-39, __transaction_state-36, ...... __transaction_state-0) (kafka.server.ReplicaFetcherManager)

-------------------------------------------------------- 
--- consumer subscribe topic å¼•èµ·çš„rebalance
[2022-03-12 18:33:00,630] INFO [GroupCoordinator 3]: 
	Preparing to rebalance group TEST-TRADEFRONT-SZL in state PreparingRebalance with old generation 0 (__consumer_offsets-43) 
	(reason: Adding new member consumer-2-808a151c-06dd-4df8-83fc-8b1b15a53a4d) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:33:00,631] INFO [GroupCoordinator 3]: 
	Stabilized group TEST-TRADEFRONT-SZL generation 1 (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:33:00,634] INFO [GroupCoordinator 3]: 
	Assignment received from leader for group TEST-TRADEFRONT-SZL for generation 1 (kafka.coordinator.group.GroupCoordinator)

-----------------------------------------------------
--- consumer removed
[2022-03-12 18:41:58,862] INFO [GroupCoordinator 3]: 
	Member consumer-2-808a151c-06dd-4df8-83fc-8b1b15a53a4d in group TEST-TRADEFRONT-SZL has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:41:58,862] INFO [GroupCoordinator 3]: 
	Preparing to rebalance group TEST-TRADEFRONT-SZL in state PreparingRebalance with old generation 1 (__consumer_offsets-43) 
	(reason: removing member consumer-2-808a151c-06dd-4df8-83fc-8b1b15a53a4d on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:41:58,863] INFO [GroupCoordinator 3]: 
	Group TEST-TRADEFRONT-SZL with generation 2 is now empty (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:43:22,158] INFO [GroupCoordinator 3]: 
	Preparing to rebalance group TEST-TRADEFRONT-SZL in state PreparingRebalance with old generation 2 (__consumer_offsets-43) 
	(reason: Adding new member consumer-2-aedafa05-22cb-4130-8e91-927c99f3fd06) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:43:22,159] INFO [GroupCoordinator 3]: 
	Stabilized group TEST-TRADEFRONT-SZL generation 3 (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:43:22,162] INFO [GroupCoordinator 3]: 
	Assignment received from leader for group TEST-TRADEFRONT-SZL for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:46:57,292] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2022-03-12 18:54:50,418] INFO [GroupCoordinator 3]: 
	Member consumer-2-aedafa05-22cb-4130-8e91-927c99f3fd06 in group TEST-TRADEFRONT-SZL has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:54:50,418] INFO [GroupCoordinator 3]: 
	Preparing to rebalance group TEST-TRADEFRONT-SZL in state PreparingRebalance with old generation 3 (__consumer_offsets-43) (reason: removing member consumer-2-aedafa05-22cb-4130-8e91-927c99f3fd06 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:54:50,418] INFO [GroupCoordinator 3]: 
	Group TEST-TRADEFRONT-SZL with generation 4 is now empty (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2022-03-12 18:56:57,292] INFO [GroupMetadataManager brokerId=3] 
	Group TEST-TRADEFRONT-SZL transitioned to Dead in generation 4 (kafka.coordinator.group.GroupMetadataManager)

Reasons:
A consumer left the group (clean shut down)
A consumer seems to be dead in the view of Kafka
https://stackoverflow.com/questions/54183045/unexpected-failing-rebalancing-of-consumers

----Shrinking ISR from
ä»£è¡¨æœ‰brokerèŠ‚ç‚¹å¤±è”æˆ–æŒ‚æ‰
----Expanding ISR from
ä»£ç brokerèŠ‚ç‚¹æ¢å¤æˆ–æ–°å¢èŠ‚ç‚¹

----Will not attempt to authenticate using SASL (unknown error)
kafka serverç«¯æ— æ³•è¿æ¥zookeeperæˆ–è€…è¿æ¥å¼‚å¸¸

```


##### kafka clientç«¯æ—¥å¿—è§£æ

```
---------------------------------------------------------------------------
--- metadata
this.kafkaConsumer.partitionsFor(context.getConfig().getTaskTopic())
=>
2021-04-01 14:37:00.622  INFO 32380GG [main] o.a.k.c.Metadata : Cluster ID: uEekh0baSnKon5ENwtY9dg

consumer.endOffsets(Collections.singleton(topicPartition)).get(topicPartition) 
=>
2021-04-01 14:37:51.146  INFO 32380GG [RKER-RECOVERY-2] o.a.k.c.Metadata : Cluster ID: uEekh0baSnKon5ENwtY9dg
æˆ–
2021-03-27 15:40:29 395-[org.apache.kafka.clients.Metadata.update(Metadata.java:365)]-[INFO]  Cluster ID: pjnHKkklRtuSQjVDsUbgVw

---------------------------------------------------------------------------
--- subscribe to topic or to topic|partition

this.kafkaConsumer.subscribe(Collections.singleton(context.getConfig().getTaskTopic()), new SimpleWorkBalancer(context.getRestorer(), this::removeWorker, this::addWorker));
=>
2021-04-01 14:37:00.639  INFO 32380GG [main] o.a.k.c.c.KafkaConsumer : [Consumer clientId=consumer-1, groupId=XXXX-SZL] Subscribed to topic(s): T-XXXX

consumer.assign(Collections.singleton(topicPartition));
=>
2021-04-01 14:38:09.783  INFO 32380GG [RKER-RECOVERY-2] o.a.k.c.c.KafkaConsumer : [Consumer clientId=consumer-2, groupId=RESTORE-1] Subscribed to partition(s): T-XXXX-SNP-1

---------------------------------------------------------------------------
--- Discover group
consumer.poll(Duration.ofMillis(10_000L));
=>
å¦‚æœæ˜¯assign modeï¼Œå¦‚æœå‰é¢æ²¡æœ‰è°ƒç”¨endOffsetsä¹‹ç±»è·å–metadataï¼Œæ­¤æ—¶ä¼šæ‰“å°ï¼ˆä¼°è®¡è·Ÿconsumer.seek(topicPartition, checkpointOffset);æœ‰å…³ï¼Œå½“ç„¶å¦‚æœä¹‹å‰è°ƒç”¨è¿‡å°±ä¼šåœ¨è°ƒç”¨æ—¶æ‰“å°ï¼Œæ­¤æ—¶ä¸ä¼šæ‰“å°ï¼‰ï¼š
2021-04-01 15:56:50.755  INFO 22064GG [RKER-RECOVERY-1] o.a.k.c.Metadata : Cluster ID: uEekh0baSnKon5ENwtY9dg 
ç„¶åæ‰“å°
2021-04-01 14:37:40.379  INFO 32380GG [XXXX-MANAGER] ordinator$FindCoordinatorResponseHandler : [Consumer clientId=consumer-1, groupId=XXX-SZL] Discovered group coordinator 1.1.1.1:9092 (id: 2147483647 rack: null)
2021-04-01 14:38:41.369  INFO 32380GG [RKER-RECOVERY-2] ordinator$FindCoordinatorResponseHandler : [Consumer clientId=consumer-2, groupId=RESTORE-1] Discovered group coordinator 1.1.1.1:9092 (id: 2147483647 rack: null)

å¦‚æœè§¦å‘äº†rebalanceï¼Œåˆ™æ¥ç€æ‰“å°
2021-03-31 08:59:01.727  INFO 20080GG [XXX-MANAGER] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-1, groupId=TEST-PRICEENGINE-SZL] (Re-)joining group
2021-03-31 08:59:01.904  INFO 20080GG [XXX-MANAGER] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-1, groupId=TEST-PRICEENGINE-SZL] (Re-)joining group
2021-03-31 08:59:04.122  INFO 20080GG [XXX-MANAGER] o.a.k.c.c.i.AbstractCoordinator$1 : [Consumer clientId=consumer-1, groupId=TEST-PRICEENGINE-SZL] Successfully joined group with generation 10

---------------------------------------------------------------------------
--- todo
Leader imbalance ratio for broker 3 is 0.0
https://stackoverflow.com/questions/57475580/whats-the-difference-between-kafka-preferred-replica-election-sh-and-auto-leade

```

##### kafka å¸¸è§å¼‚å¸¸Exceptions

Kafkaå¸¸è§é”™è¯¯æ•´ç† https://cloud.tencent.com/developer/article/1508919

```
--- NotEnoughReplicasException
The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement
https://stackoverflow.com/questions/62770272/notenoughreplicasexception-the-size-of-the-current-isr-set2-is-insufficient-t

ä¾‹å¦‚ï¼šå¯¹äºproduceræ¥è¯´å°±æ˜¯
The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition __transaction_state-
å¯¹åº”clientç«¯çš„é”™è¯¯æ—¥å¿—ä¸ºï¼š
java.lang.reflect.UndeclaredThrowableException: null
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while initializing transactional state in 60000ms.

--- LEADER_NOT_AVAILABLE: 
topic å¯èƒ½ä¸å­˜åœ¨ï¼Œkafka apié»˜è®¤ä¼šè‡ªåŠ¨åˆ›å»º

--- offset commit failed on partition this is not the correct coordinator

--- Offset commit failed on partition xxx at offset 957: The coordinator is not aware of this member
https://www.cnblogs.com/chuijingjing/p/12797035.html

--- topic not presetn in metadata after 6000ms
partition å¯èƒ½ä¸å­˜åœ¨æˆ–è€…æ˜¯å…¶ä»–é—®é¢˜ï¼Œæ¯”å¦‚
https://blog.csdn.net/bay_bai/article/details/104799498
https://github.com/wurstmeister/kafka-docker/issues/553

--- Connection to node -1 could not be established. Broker may not be available.
listenerè®¾ç½®ä¸å¯¹
https://blog.csdn.net/Mr_hou2016/article/details/79484032

--- Connection to node -2 could not be established. Broker may not be available.

--- org.apache.kafka.common.errors.TimeoutException: Failed to get offsets by times in 30000ms
endOffsets()->fetchOffsetsByTimes

--- UNKNOWN_MEMBER_ID
Attempt to heartbeat failed for since member id consumer-1-c4ff67d3-b776-4994-9179-4a19f9ff87a6 is not valid
å¯èƒ½1ï¼šå¦‚æœå½“å‰ group çš„çŠ¶æ€ä¸º Deadï¼Œåˆ™è¯´æ˜å¯¹åº”çš„ group ä¸å†å¯ç”¨ï¼Œæˆ–è€…å·²ç»ç”±å…¶å®ƒ GroupCoordinator å®ä¾‹ç®¡ç†ï¼Œç›´æ¥å“åº” UNKNOWN_MEMBER_ID é”™è¯¯ï¼Œæ¶ˆè´¹è€…å¯ä»¥å†æ¬¡è¯·æ±‚è·å–æ–°æ¥ç®¡çš„ GroupCoordinator å®ä¾‹æ‰€åœ¨çš„ä½ç½®ä¿¡æ¯ã€‚
å¯èƒ½2ï¼šæ¶ˆè´¹è€…ä¼šåœ¨è½®è¯¢è·å–æ¶ˆæ¯æˆ–æäº¤åç§»é‡æ—¶å‘é€å¿ƒè·³ï¼Œå¦‚æœæ¶ˆè´¹è€…åœæ­¢å‘é€å¿ƒè·³çš„æ—¶é—´è¶³å¤Ÿé•¿ï¼Œä¼šè¯å°±ä¼šè¿‡æœŸï¼Œç»„åè°ƒå™¨è®¤ä¸ºå®ƒå·²ç»æ­»äº¡ï¼Œå°±ä¼šè§¦å‘ä¸€æ¬¡å†å‡è¡¡ï¼Œè‡³äºåŸå› ï¼Œæœ‰å¯èƒ½æ˜¯ï¼š
ä¸€èˆ¬æ¥è¯´producerçš„ç”Ÿäº§æ¶ˆæ¯çš„é€»è¾‘é€Ÿåº¦éƒ½ä¼šæ¯”consumerçš„æ¶ˆè´¹æ¶ˆæ¯çš„é€»è¾‘é€Ÿåº¦å¿«ï¼Œå½“produceråœ¨çŸ­æ—¶é—´å†…äº§ç”Ÿå¤§é‡çš„æ•°æ®ä¸¢è¿›kafkaçš„brokeré‡Œé¢æ—¶ï¼Œå¯èƒ½å‡ºç°ç±»ä¼¼é”™è¯¯ï¼šOffset commit failed on partition : The coordinator is not aware of this member.
1) kafkaçš„consumerä¼šä»brokeré‡Œé¢å–å‡ºä¸€æ‰¹æ•°æ®ï¼Œç»™æ¶ˆè´¹çº¿ç¨‹è¿›è¡Œæ¶ˆè´¹ï¼›
2) ç”±äºå–å‡ºçš„ä¸€æ‰¹æ¶ˆæ¯æ•°é‡å¤ªå¤§ï¼Œconsumeråœ¨session.timeout.msæ—¶é—´ä¹‹å†…æ²¡æœ‰æ¶ˆè´¹å®Œæˆï¼›
3) consumer coordinator ä¼šç”±äºæ²¡æœ‰æ¥å—åˆ°å¿ƒè·³è€ŒæŒ‚æ‰ï¼›
4) ç”±äºè‡ªåŠ¨æäº¤offsetå¤±è´¥ï¼Œreblanceä¹‹ååˆé‡æ–°æ¶ˆè´¹ä¹‹å‰çš„ä¸€æ‰¹æ•°æ®ï¼ˆoffsetæäº¤å¤±è´¥ï¼‰ï¼Œæ¶æ€§å¾ªç¯ï¼Œè¶Šç§¯è¶Šå¤šï¼›
- https://www.cnblogs.com/chuijingjing/p/12797035.html

--- Group coordinator is unavailable or invalid
Group coordinator x.x.x.x:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery

--- CommitFailedException
If a simple consumer(assign mode) tries to commit offsets with a group id which matches an active consumer group, the coordinator will reject the commit (which will result in a CommitFailedException). However, there wonâ€™t be any errors if another simple consumer instance shares the same group id.

--- INVALID_FETCH_SESSION_EPOCH.
Node 1 was unable to process the fetch request with (sessionId=1972558084, epoch=904746): INVALID_FETCH_SESSION_EPOCH.

--- UnkownProducerIdException
åŸºæœ¬åŸå› å°±æ˜¯produceråˆ›å»ºåè¶…è¿‡ retention expire è¿‡æœŸæ—¶é—´æˆ–è€…å¤§å°ï¼Œæ‰€ä»¥è¢«æ¸…ç†ï¼ŒkafkaæœåŠ¡ç«¯å‘å®¢æˆ·ç«¯æŠ¥é”™åä¼šç«‹å³é‡æ–°æ³¨å†Œè¯¥producerï¼Œæ‰€ä»¥æœ€å¥½çš„å¤„ç†åŠæ³•æ˜¯callbackä¸­é‡è¯•
https://stackoverflow.com/questions/61084031/how-to-handle-unkownproduceridexception/69999568#69999568
2021-11-16 09:08:29.206 [31mERROR[m [35m5527GG[m [ad | producer-1] [36mo.a.k.c.p.i.Sender[m : [Producer clientId=producer-1] The broker returned org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception. for topic-partition T-TEST-1 at offset -1. This indicates data loss on the broker, and should be investigated.
2021-11-16 09:08:29.207 [32m INFO[m [35m5527GG[m [ad | producer-1] [36mo.a.k.c.p.i.TransactionManager[m : [Producer clientId=producer-1] ProducerId set to -1 with epoch -1
2021-11-16 09:08:29.219 [32m INFO[m [35m5527GG[m [ad | producer-1] [36mo.a.k.c.p.i.TransactionManager[m : [Producer clientId=producer-1] ProducerId set to 35804 with epoch 0


```



##### kafka-log-dirs.sh

```
./bin/kafka-log-dirs.sh --describe --bootstrap-server hostname:port --broker-list broker 1, broker 2 --topic-list topic 1, topic 2
```

##### kafka-dump-log.sh

KAFKA Internal consumer topic logï¼š

```
./bin/kafka-dump-log.sh --files ./kafka-logs/T-TOPIC-1/00000000000000000192.log --print-data-log
```

##### kafka-console-consumer.sh 

KAFKA Internal offset topic: `__consumer_offsets`:

```
#Create consumer config
echo "exclude.internal.topics=false" > /tmp/consumer.config
#Consume all offsets
./kafka-console-consumer.sh --consumer.config /tmp/consumer.config \
--formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter" \
--bootstrap-server localhost:9092 --topic __consumer_offsets --from-beginning
```



KAFKA Internal transaction log:

æš‚æ—¶æ²¡æ‰¾åˆ°æ–¹æ³•çœ‹ï¼Œå‚è€ƒ

> You can look to source code of `TransactionLogMessageParser` class inside `kafka/tools/DumpLogSegments.scala` file as an example.  It uses `readTxnRecordValue` function from `TransactionLog` class.  The first argument for this function could be retrieved via `readTxnRecordKey` function of the same class.
>
> https://stackoverflow.com/questions/47670477/reading-data-from-transaction-state-topic-in-kafka-0-11-0-1

KAFKA Internal transaction topic: `__transaction_state`

```
echo "exclude.internal.topics=false" > consumer.config
./bin/kafka-console-consumer.sh --consumer.config consumer.config --formatter "kafka.coordinator.transaction.TransactionLog\$TransactionLogMessageFormatter" --bootstrap-server x.x.x.x:9092,X.X.X.46:9092,X.X.X.47:9092 --topic __transaction_state --from-beginning
```



#### 2.3.4 Backup (point-in-time snapshot) & Restore

ä¸ºä»€ä¹ˆéœ€è¦å¤‡ä»½ï¼Ÿ

https://medium.com/@anatolyz/introducing-kafka-backup-9dc0677ea7ee

> Replication handles many error cases but by far not all. What about the case that  there is a bug in Kafka that deletes old data? What about a  misconfiguration of the topic (are you sure, that your value of  retention.ms is a millisecond value?)? What about an admin that  accidentally deleted the whole Prod Cluster because they thought they  were on dev? What about security breaches? If an attacker gets access to your Kafka Management interface, they can do whatever they like.
>
> Of course, this does not matter too much if you are using Kafka to  distribute click-streams data for your analytics department and it is  tolerable to loose some data. But if you use Kafka as your â€œcentral  nervous systemâ€ for your company and you store your core business data  in Kafka you better think about a cold storage backup for your Kafka  Cluster.
>
> 

##### åœæœºå¤‡ä»½

https://www.digitalocean.com/community/tutorials/how-to-back-up-import-and-migrate-your-apache-kafka-data-on-ubuntu-18-04

```
å•æœºç‰ˆä¾‹å­ï¼Œé›†ç¾¤ç±»ä¼¼ï¼Œåªæ˜¯éœ€è¦åœæ‰æ‰€æœ‰çš„zookeeperå’Œkafkaï¼Œç„¶åå¤‡ä»½å…¶ä¸­ä¸€å°æœºå™¨çš„zookeeperå’Œkafkaï¼Œç„¶ååœ¨æ‰€æœ‰æœºå™¨ä¸Šæ¢å¤

sudo -iu kafka

~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic BackupTopic

echo "Test Message 1" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic BackupTopic > /dev/null

~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic BackupTopic --from-beginning

-----------------------------------------------------------------------------------
--- Backing Up the ZooKeeper State Data
-----------------------------------------------------------------------------------
kafkaå†…ç½®zookeeperï¼š
ZooKeeper stores its data in the directory specified by the dataDir field in the /kafka/config/zookeeper.properties:
dataDir=/tmp/zookeeper
ä½¿ç”¨å¤–ç½®zookeeperï¼š
/zookeeper/conf/zoo.cfg
dataDir=/opt/zookeeper-3.4.8/zkdata
dataLogDir=/opt/zookeeper-3.4.8/logs

compressed archive files are a better option over regular archive files to save disk space:
tar -czf /opt/kafka_backup/zookeeper-backup.tar.gz /opt/zookeeper-3.4.8/zkdata/*
å¿½ç•¥é”™è¯¯ tar: Removing leading `/' from member names

-----------------------------------------------------------------------------------
--- Backing Up the Kafka Topics and Messages
-----------------------------------------------------------------------------------
Kafka stores topics, messages, and internal files in the directory that the log.dirs field specifies 
/kafka/config/server.properties:
log.dirs=/opt/kafka_2.12-2.2.0/kafka-logs

stop the Kafka service so that the data in the log.dirs directory is in a consistent state when creating the archive with tar

sudo systemctl stop kafka (å‰é¢å®‰è£…æ—¶ç§»é™¤äº†kafkaçš„sudoæƒé™ï¼Œéœ€è¦ä½¿ç”¨å…¶ä»–æœ‰sudoæƒé™çš„érootç”¨æˆ·æ‰§è¡Œ)
sudo -iu kafka

tar -czf /opt/kafka_backup/kafka-backup.tar.gz /opt/kafka_2.12-2.2.0/kafka-logs/*

sudo systemctl start kafka ï¼ˆåŒæ ·åˆ‡æ¢å…¶ä»–ç”¨æˆ·ï¼‰
sudo -iu kafka

-----------------------------------------------------------------------------------
--- Restoring the ZooKeeper Data & Kafka Data
-----------------------------------------------------------------------------------
You need to stop the Kafka and ZooKeeper services as a precaution against the data directories receiving invalid data during the restoration process.

sudo systemctl stop kafka
sudo systemctl stop zookeeper
sudo -iu kafka

rm -r /opt/zookeeper-3.4.8/zkdata/*
tar -C /opt/zookeeper-3.4.8/zkdata -xzf /opt/kafka_backup/zookeeper-backup.tar.gz --strip-components 2
ï¼ˆspecify the --strip 2 flag to make tar extract the archiveâ€™s contents in /tmp/zookeeper/ itself and not in another directory (such as /tmp/zookeeper/tmp/zookeeper/) inside of it.ï¼‰

rm -r /opt/kafka_2.12-2.2.0/kafka-logs/*
tar -C /opt/kafka_2.12-2.2.0/kafka-logs -xzf /opt/kafka_backup/kafka-backup.tar.gz --strip-components 2
sudo systemctl start kafka
sudo systemctl start zookeeper
sudo -iu kafka

-----------------------------------------------------------------------------------
--- Verifying the Restoration
-----------------------------------------------------------------------------------
~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic BackupTopic --from-beginning

```

https://stackoverflow.com/questions/47791039/backup-restore-kafka-and-zookeeper/48337651

##### åœ¨çº¿å¤‡ä»½

> Still no. Youâ€™re dealing with a distributed system. Itâ€™s not magic. Any  attempt to trigger a snapshot â€˜simultaneouslyâ€™ across multiple  hosts/disks is going to be subject to some small level of timing  difference whether those are VMs managed by you in the Cloud or  containers in K8s with persistent disks managed by the Cloud provider.  Itâ€™d probably work in small scale tests, but break under significant  production load.

https://www.reddit.com/r/apachekafka/comments/jb400p/kafka_backup_and_recovery/g8vkju7/

https://www.reddit.com/r/apachekafka/comments/g73nk9/how_to_take_full_backupsnapshot_of_kafka/

è§£å†³æ–¹æ¡ˆï¼š

Support point-in-time backups :

æå‡ºéœ€æ±‚ï¼šhttps://github.com/itadventurer/kafka-backup/issues/52

è§£å†³æ–¹æ¡ˆï¼š

1ï¼‰å½“å‰ç‰ˆæœ¬åœ¨ä¸€å®šåœºæ™¯ä¸‹å¯ä»¥ä½¿ç”¨ï¼š

- Let Kafka Backup running in the background
- Kafka Backup writes data continuously in the background to the file system
- `kill -9` Kafka Backup as soon as it is "finished", i.e.  it finished writing your data. This should be promptly after you  finished producing data
- move the data of Kafka Backup to your new destination.

```
éœ€è¦ç”¨åˆ°kafkaè‡ªå¸¦çš„connect-standalone.sh æ‰€ä»¥è¦é…ç½®ç¯å¢ƒå˜é‡
export PATH=$PATH:~/kafka/bin


backupï¼š
sudo env "PATH=$PATH" backup-standalone.sh --bootstrap-server localhost:9092 --target-dir /path/to/backup/dir --topics 'topic1,topic2'

~/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic topic1

~/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic 'topic.*'

restoreï¼š
restore-standalone.sh --bootstrap-server localhost:9092 --target-dir /path/to/backup/dir --topics 'topic1,topic2'

~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic BackupTopic --from-beginning
```

2ï¼‰æ–°å¢çš„æ”¯æŒ https://github.com/itadventurer/kafka-backup/pull/99 ä½†æ˜¯æ²¡æœ‰å‘å¸ƒ



https://www.confluent.io/blog/3-ways-prepare-disaster-recovery-multi-datacenter-apache-kafka-deployments/

## 3.Kafka stream
ã€ŠKafka Streamã€‹è°ƒç ”ï¼šä¸€ç§è½»é‡çº§æµè®¡ç®—æ¨¡å¼ https://yq.aliyun.com/articles/58382
Kafka Streams - Not Looking at Facebook https://timothyrenner.github.io/engineering/2016/08/11/kafka-streams-not-looking-at-facebook.html
https://cloud.tencent.com/developer/ask/203192
https://www.codota.com/code/java/methods/org.apache.kafka.streams.kstream.KStream/groupBy
http://www.jasongj.com/kafka/kafka_stream/

### 3.1 Architecture 
https://kafka.apache.org/22/documentation/streams/architecture

**Stream Partitions and Tasks**

![](/docs/docs_image/software/buildingblock/kafka/kafka05.png)

Task(consumer, run topology over one or more partition), Thread can run one or multiple Task, Instance ID is consumer group ID,multiple instance with same group ID belong to one consumer group, 
4 partition, 1 instance will start 4 task, (thread quantity is defined by code), now start a new instance, join the consumer group, relocate 4 partition, each instance got 2 partition, so each instance run 2 task
Kafka Streamçš„å¹¶è¡Œæ¨¡å‹ä¸­ï¼Œæœ€å°ç²’åº¦ä¸ºTaskï¼Œè€Œæ¯ä¸ªTaskåŒ…å«ä¸€ä¸ªç‰¹å®šå­Topologyçš„æ‰€æœ‰Processorã€‚å› æ­¤æ¯ä¸ªTaskæ‰€æ‰§è¡Œçš„ä»£ç å®Œå…¨ä¸€æ ·ï¼Œå”¯ä¸€çš„ä¸åŒåœ¨äºæ‰€å¤„ç†çš„æ•°æ®é›†äº’è¡¥ã€‚è¿™ä¸€ç‚¹è·ŸStormçš„Topologyå®Œå…¨ä¸ä¸€æ ·ã€‚Stormçš„Topologyçš„æ¯ä¸€ä¸ªTaskåªåŒ…å«ä¸€ä¸ªSpoutæˆ–Boltçš„å®ä¾‹ã€‚å› æ­¤Stormçš„ä¸€ä¸ªTopologyå†…çš„ä¸åŒTaskä¹‹é—´éœ€è¦é€šè¿‡ç½‘ç»œé€šä¿¡ä¼ é€’æ•°æ®ï¼Œè€ŒKafka Streamçš„TaskåŒ…å«äº†å®Œæ•´çš„å­Topologyï¼Œæ‰€ä»¥Taskä¹‹é—´ä¸éœ€è¦ä¼ é€’æ•°æ®ï¼Œä¹Ÿå°±ä¸éœ€è¦ç½‘ç»œé€šä¿¡ã€‚è¿™ä¸€ç‚¹é™ä½äº†ç³»ç»Ÿå¤æ‚åº¦ï¼Œä¹Ÿæé«˜äº†å¤„ç†æ•ˆç‡ã€‚

å¦‚æœæŸä¸ªStreamçš„è¾“å…¥Topicæœ‰å¤šä¸ª(æ¯”å¦‚2ä¸ªTopicï¼Œ1ä¸ªPartitionæ•°ä¸º4ï¼Œå¦ä¸€ä¸ªPartitionæ•°ä¸º3)ï¼Œåˆ™æ€»çš„Taskæ•°ç­‰äºPartitionæ•°æœ€å¤šçš„é‚£ä¸ªTopicçš„Partitionæ•°ï¼ˆmax(4,3)=4ï¼‰ã€‚è¿™æ˜¯å› ä¸ºKafka Streamä½¿ç”¨äº†Consumerçš„Rebalanceæœºåˆ¶ï¼Œæ¯ä¸ªPartitionå¯¹åº”ä¸€ä¸ªTaskã€‚
Kafka Streamå¯è¢«åµŒå…¥ä»»æ„Javaåº”ç”¨ï¼ˆç†è®ºä¸ŠåŸºäºJVMçš„åº”ç”¨éƒ½å¯ä»¥ï¼‰ä¸­ï¼Œä¸‹å›¾å±•ç¤ºäº†åœ¨åŒä¸€å°æœºå™¨çš„ä¸åŒè¿›ç¨‹ä¸­åŒæ—¶å¯åŠ¨åŒä¸€Kafka Streamåº”ç”¨æ—¶çš„å¹¶è¡Œæ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™é‡Œè¦ä¿è¯ä¸¤ä¸ªè¿›ç¨‹çš„StreamsConfig.APPLICATION_ID_CONFIGå®Œå…¨ä¸€æ ·ã€‚å› ä¸ºKafka Streamå°†APPLICATION_ID_CONFIä½œä¸ºéšå¼å¯åŠ¨çš„Consumerçš„Group IDã€‚åªæœ‰ä¿è¯APPLICATION_ID_CONFIç›¸åŒï¼Œæ‰èƒ½ä¿è¯è¿™ä¸¤ä¸ªè¿›ç¨‹çš„Consumerå±äºåŒä¸€ä¸ªGroupï¼Œä»è€Œå¯ä»¥é€šè¿‡Consumer Rebalanceæœºåˆ¶æ‹¿åˆ°äº’è¡¥çš„æ•°æ®é›†ã€‚
https://yq.aliyun.com/articles/222900?spm=5176.10695662.1996646101.searchclickresult.13d4446d1xNbRq


å›¾äºŒ:ä¸Šå›¾ä¸­çš„Consumerå’ŒProducerå¹¶ä¸éœ€è¦å¼€å‘è€…åœ¨åº”ç”¨ä¸­æ˜¾ç¤ºå®ä¾‹åŒ–ï¼Œè€Œæ˜¯ç”±Kafka Streamæ ¹æ®å‚æ•°éšå¼å®ä¾‹åŒ–å’Œç®¡ç†ï¼Œä»è€Œé™ä½äº†ä½¿ç”¨é—¨æ§›ã€‚å¼€å‘è€…åªéœ€è¦ä¸“æ³¨äºå¼€å‘æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼Œä¹Ÿå³ä¸Šå›¾ä¸­Taskå†…çš„éƒ¨åˆ†ã€‚

å›¾ä¸‰: ä¸¤å›¾éƒ½æ˜¯åŒä¸€ä¸ªæœºå™¨ï¼Œéƒ½åªæœ‰ä¸€ä¸ªinstance,éƒ½æ˜¯4ä¸ªtaskï¼Œåˆ†åˆ«è¿è¡Œåœ¨ä¸€ä¸ªthreadå’Œ2ä¸ªthread

å›¾å››:å·¦å›¾ä¸€å°æœºå™¨ï¼Œä¸¤ä¸ªinstanceï¼Œ4ä¸ªtaskåˆ†åˆ«å±äºä¸¤ä¸ªinstanceï¼›è€Œå³å›¾æ˜¯éƒ¨ç½²ä¸¤å°æœºå™¨ä¸Š

**Threading Model**

Kafka Streams work allocation https://medium.com/@andy.bryant/kafka-streams-work-allocation-4f31c24753cc

https://www.slideshare.net/ConfluentInc/robust-operations-of-kafka-streams

![](/docs/docs_image/software/buildingblock/kafka/kafka06.png)

**Local State Stores**

**Fault Tolerance**

![](/docs/docs_image/software/buildingblock/kafka/kafka07.png)

### 3.2 Concepts 
https://kafka.apache.org/22/documentation/streams/core-concepts
Task â‡” ä¸€ä¸ªconsumerå¯ä»¥åŒ…å«å¤šä¸ªtaskï¼Œconsumeræœ¬èº«æ˜¯éšå¼ç®¡ç†
Task vs thread
https://stackoverflow.com/questions/48106568/kafka-streams-thread-number

Kstream ktable

https://www.slideshare.net/vitojeng/streaming-process-with-kafka-connect-and-kafka-streams-80721215
**Stream Processing Topology**
	Kafka Streams DSL 
	Processor API 
**Time**
	Event time
	Processing time
	Ingestion time
	Stream time, wall-clock time
**Aggregation**

**Windowing**
Late arriving records
**Duality of Stream and table**

**States**

**Processing guarantees**

 Lambda Architecture http://lambda-architecture.net/

**Out-of-order handling**
For stateless operations, out-of-order data will not impact processing logic since only one record is considered at a time, without looking into the history of past processed records; for stateful operations such as aggregations and joins, however, out-of-order data could cause the processing logic to be incorrect.

Physical order = offset order
Logical order = timestamp order
https://dl.acm.org/citation.cfm?id=3242155

Since timestamps, in contrast tooffsets, are not necessarily unique, we use the record offsetas â€œtie breakerâ€ [15] to derive a logical order that isstrictandtotalover all records.
In theKafka Streams DSL, there are two first-class abstractions:aKStreamand aKTable. AKStreamis an abstraction ofa record stream, while a KTable is an abstraction of both a table changelog stream and its corresponding materializedtables in the Dual Streaming Model. In addition, users of theDSL can query a KTableâ€™s materialized state in real-time.
Whenever a record is received from the source Kafka topics,it will be processed immediately by traversing through allthe connected operators specified in the Kafka Streams DSL until it has been materialized to some result KTable, or writ-ten back to a sink Kafka topic. During the processing, therecordâ€™s timestamp will be maintained/updated according toeach operatorâ€™s semantics as defined in Section 4
Handling out-of-order records injoins requires several strategies. For stream-table joins, out-of-order records do not require special handling. However,out-of-order table updates could yield incorrect join results, ifnot treated properly. Assume that the table update in Figure 6fromâŸ¨A,a,2âŸ©toâŸ¨A,aâ€²,5âŸ©is delayed. Stream recordâŸ¨A,Î±â€²,6âŸ©would join with the first table version and incorrectly emitâŸ¨A,Î±â€²â–·â—a,6âŸ©. To handle this case, it is required to buffer record stream input record in the stream-table join operatorand re-trigger the join computation for late table updates.Thus, if a late table update occurs, corresponding updaterecords are sent downstream to â€œoverwriteâ€ previously emit-ted join records. Note, that the result of stream-table joinsis not a record stream but a regular data stream because itmight contain update records.

Record stream , normal data stream
Time window , session window

https://kafka.apache.org/22/documentation/streams/developer-guide/

![](/docs/docs_image/software/buildingblock/kafka/kafka08.png)

### 3.3 Basic usage

![](/docs/docs_image/software/buildingblock/kafka/kafka09.png)

![](/docs/docs_image/software/buildingblock/kafka/kafka10.png)

```
mvn clean package
mvn exec:java -Dexec.mainClass=myapps.WordCount
bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-plaintext-input
bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-wordcount-output \
    --config cleanup.policy=compact
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe
bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic my_topic_name

bin/kafka-run-class.sh myapps.WordCount
/home/test/workspace/kafka/kafka_2.12-2.2.0/bin/kafka-run-class.sh myapps.WordCount
mvn exec:java -Dexec.mainClass=myapps.WordCount

bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo

bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-plaintext-input
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \
    --topic streams-wordcount-output \
    --from-beginning \
    --formatter kafka.tools.DefaultMessageFormatter \
    --property print.key=true \
    --property print.value=true \
    --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
    --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

```

https://kafka.apache.org/22/documentation/streams/tutorial
https://github.com/apache/kafka/tree/2.2/streams/examples

https://www.draw.io/#G13TFIxfbM3VN9R5Pg7nFwNguUUNUXKChO

?# Windowed

``` 
bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-plaintext-input
bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-windowed-wordcount-output \
    --config cleanup.policy=compact
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \
    --topic streams-windowed-wordcount-output \
    --from-beginning \
    --formatter kafka.tools.DefaultMessageFormatter \
    --property print.key=true \
    --property print.value=true \
    --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
    --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
```

### 3.4 kafka stream VS storm (wordcount)

Kafka wordcountæ˜¯stateful operationï¼Œå› ä¸ºæ¯ä¸ªtask/consumerå®Œå…¨ç‹¬ç«‹è·‘å®Œæ•´çš„topologyï¼Œæ¯ä¸ªconsumerå¤„ç†æŸä¸€ä¸ªpartitionï¼Œæ‰€ä»¥è¦å€ŸåŠ©data storeæ¥å­˜å‚¨ktableâ€œä¸­é—´â€çŠ¶æ€ï¼Œdata storeä¹Ÿæ˜¯å¤šä¸ªconsumer/taskâ€œåä½œâ€çš„ç»“æœ
è€Œstorm wordcountæ˜¯stateless operationï¼Œå› ä¸ºä¸€ä¸ªtopologyæ˜¯åˆ†æˆsproutï¼Œboltï¼Œæ¯ä¸ªsprout/boltä¼šå¼€å¯ä¸€ä¸ªæˆ–å¤šä¸ªtaskï¼Œè¿™äº›taskåœ¨work processä¸­æ‰§è¡Œï¼Œè¿™äº›work processå¯èƒ½ä½äºä¸åŒçš„æœºå™¨ï¼Œæ‰€ä»¥ç¬¬ä¸€æ­¥æ˜¯split boltï¼Œç„¶åå°†è¿™äº›å•è¯è¿›è¡Œpartitionå‘é€è‡³ç›¸åº”çš„tasksï¼Œæ¯”å¦‚theè¿™ä¸ªå•è¯ä¼šä¸€ç›´å‘é€åˆ°æŸä¸ªç‰¹å®šçš„taskè¿›è¡Œcountï¼Œæ‰€ä»¥å¯¹äºæœ€åä¸€æ­¥countæ˜¯å¾ˆç®€å•çš„ï¼Œä¸éœ€è¦reduceæ“ä½œï¼Œæ¯ä¸ªcount taskéƒ½åªç»Ÿè®¡ç›¸åº”çš„å•è¯ï¼Œäº’ç›¸ä¹‹é—´æ²¡æœ‰é‡å ï¼Œä¸åƒkafkaé‚£æ ·å› ä¸ºpartitionæ¯”è¾ƒæ—©ï¼Œæ‰€ä»¥ä¸åŒçš„parttionä¹‹é—´æ˜¯æœ‰é‡å çš„å•è¯çš„ï¼Œæ‰€ä»¥å¿…é¡»å€ŸåŠ©ä¸€ä¸ªç¬¬ä¸‰è€…å­˜å‚¨æ¥ç»Ÿè®¡

### 3.5 Advance

Ktables vs global ktables
Kafka has several features for reducing the need to move data on startup 

- Standby Replicas 
- Disk Checkpoints
- Compacted topics
  Command and Query Responsibility Segregation (CQRS) pattern [with event sourcing]

![](/docs/docs_image/software/buildingblock/kafka/kafka12.png)

Using an event-streaming approach, we can materialize the data locally via the Kafka Streams API. We define a query for the data in our grid: â€œselect * from orders, payments, customers whereâ€¦â€ and Kafka Streams executes it, stores it locally, keeps it up to date. This ensures highly available should the worst happen and your service fails unexpectedly (this approach is discussed in more detail here).
To combat the challenges of being stateful, Kafka ships with a range of features to make the storage, movement, and retention of state practical: notably standby replicas and disk checkpointsto mitigate the need for complete rebuilds, and compacted topics to reduce the size of datasets that need to be moved.



State store, global or local?
Ktable, globalktable

Kafka Streamæœ‰ä¸€äº›å…³é”®ä¸œè¥¿æ²¡æœ‰è§£å†³ï¼Œä¾‹å¦‚åœ¨joinåœºæ™¯ä¸­ï¼Œéœ€è¦ä¿è¯æ¥æº2ä¸ªTopicæ•°æ®Shardä¸ªæ•°å¿…é¡»æ˜¯ä¸€å®šçš„ï¼Œå› ä¸ºæœ¬èº«åšä¸åˆ°MapJoinç­‰æŠ€æœ¯




## 4.Indepth 
nothing to guarantee/at-most-once => at-least-once => exactly-once

https://kafka.apache.org/documentation/#design

### 4.0 Config

https://kafka.apache.org/26/documentation/

https://docs.confluent.io/platform/current/installation/configuration

#### Server Config

##### å¤åˆ¶å› å­ replica factor è¯¦è§£

min.insync.replicasï¼ˆdefault value=1ï¼‰
https://accu.org/journals/overload/28/159/kozlovski/
åŒæ—¶æ§åˆ¶external topic ä»¥åŠinternal topic` __consumer_offsets`å’Œ`__transaction_state`ï¼Œ
transaction.state.\*åªæ§åˆ¶`__transaction_state`ï¼ˆtransaction.state.log.min.isr overriden min.insync.replicasï¼‰ï¼Œ
offsets.topic.replication.factoræ§åˆ¶offsets topicä¹Ÿå°±æ˜¯`__consumer_offsets`ï¼Œå¿…é¡»è·Ÿbrokerä¸ªæ•°ä¸€è‡´(å°äºç­‰äºï¼Œé»˜è®¤å€¼ä¸º3ï¼Œå¦‚æœæ˜¯ä¸¤ä¸ªèŠ‚ç‚¹å°±ä¸è¡Œäº†ï¼Œæ‰€ä»¥ä¸è¦è½»æ˜“ä½¿ç”¨é»˜è®¤å€¼ï¼‰ï¼Œå¦åˆ™æ— æ³•å¯åŠ¨
default.replication.factoræ§åˆ¶external topicï¼ˆæœ‰æ—¶å€™ç§°ä¸ºautomatically created topicsï¼Œæ‰‹åŠ¨æˆ–è‡ªåŠ¨åˆ›å»ºauto.create.topics.enableé»˜è®¤æ˜¯trueï¼‰


+ if living/avaliable brokers < default.replication.factor
æ— æ³•åˆ›å»ºtopicï¼ŒæŠ¥é”™ï¼šInvalidReplicationFactorException

+ if offsets.topic.replication.factor > brokersæ•°é‡ï¼Œ~~kafka clientæ— æ³•å¯åŠ¨ï¼ˆæ— æ³•Discover group coordinatorï¼‰~~åº”è¯¥æ˜¯kafka severæ— æ³•æ­£å¸¸åˆ›å»ºinternal topic consumer_offset, kafka serveræŠ¥é”™ï¼š

```
ERROR [KafkaApi-0] Number of alive brokers '2' does not meet the required replication factor '3' for the offsets topic (configured via 'offsets.topic.replication.factor'). This error can be ignored if the cluster is starting up and not all brokers are up yet. (kafka.server.KafkaApis)
```



+ if æŒ‚æ‰çš„èŠ‚ç‚¹==default.replication.factorï¼Œæ¯”å¦‚ï¼š

default.replication.factor=1 åˆ™ä»£è¡¨external topicæ²¡æœ‰replicationï¼Œè¿™æ ·æŒ‚æ‰ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹clientéƒ½ä¼šæŠ¥é”™ï¼š

```
2021-06-08 17:06:01.892 ^[[33m WARN^[[m ^[[35m23610GG^[[m [TEST-MANAGER] ^[[36mk.c.NetworkClient$DefaultMetadataUpdater^[[m : [Consumer clientId=consumer-1, groupId=TEST-SZL] 1 partitions have leader brokers without a matching listener, including [T-TEST-1]
```

 

+ if (live isr æ´»ç€çš„èŠ‚ç‚¹ä¸­å¹¶ä¸”æ˜¯isrçš„èŠ‚ç‚¹æ•°) < transaction.state.log.min.isr:

```
[2021-06-09 09:31:14,285] ERROR [ReplicaManager broker=0] Error processing append operation on partition __transaction_state-28 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition __transaction_state-28
```

æ³¨æ„å¦‚æœä¸åœæ‰kafka producerç¨‹åºï¼Œä¸Šè¿°æ—¥å¿—ä¼šå¿«é€Ÿçš„åœ¨kafka/logs/server.log ä¸­åˆ·å…¥ï¼Œæ½œåœ¨å¯èƒ½ä¼šé€ æˆç£ç›˜é—®é¢˜

+ if living/avaliable brokers <min.insync.replicas && producer.properties.acks=allï¼š
produceræŠ¥é”™ NotEnoughReplicasException

+ if (live isr æ´»ç€çš„èŠ‚ç‚¹ä¸­å¹¶ä¸”æ˜¯isrçš„èŠ‚ç‚¹æ•°) <min.insync.replicas of` __consumer_offsets`:

kafka consumer client discover groupä¹‹åæ— æ³•join groupï¼Œåœ¨revokeä¹‹åï¼Œrejoining groupåœé¡¿å‡ åˆ†é’Ÿåç‹‚åˆ·æ—¥å¿—ï¼š

```
2021-06-09 10:17:23.076 ^[[32m INFO^[[m ^[[35m26210GG^[[m [TEST-MANAGER] ^[[36mo.a.k.c.c.i.AbstractCoordinator^[[m : [Consumer clientId=consumer-1, groupId=TEST-REALTIME-SZL] Group
coordinator XXXX:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery                                                                        
2021-06-09 10:17:23.186 ^[[32m INFO^[[m ^[[35m26210GG^[[m [TEST-MANAGER] ^[[36mordinator$FindCoordinatorResponseHandler^[[m : [Consumer clientId=consumer-1, groupId=TEST-REALTIME-SZL] Discovered group coordinator XXXX:9092 (id: 2147483647 rack: null)
2021-06-09 10:17:23.187 ^[[32m INFO^[[m ^[[35m26210GG^[[m [TEST-MANAGER] ^[[36mo.a.k.c.c.i.AbstractCoordinator^[[m : [Consumer clientId=consumer-1, groupId=TEST-REALTIME-SZL] Group
coordinator XXXX8:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery                                                                        
2021-06-09 10:17:23.288 ^[[32m INFO^[[m ^[[35m26210GG^[[m [TEST-MANAGER] ^[[36mordinator$FindCoordinatorResponseHandler^[[m : [Consumer clientId=consumer-1, groupId=TEST-REALTIME-SZL] Discovered group coordinator XXXX:9092 (id: 2147483647 rack: null)
2021-06-09 10:17:23.289 ^[[32m INFO^[[m ^[[35m26210GG^[[m [TEST-MANAGER] ^[[36mo.a.k.c.c.i.AbstractCoordinator^[[m : [Consumer clientId=consumer-1, groupId=TEST-REALTIME-SZL] (Re-)joining group
```



kafka serverç«¯ç‹‚åˆ·æ—¥å¿—ï¼š

```
[2021-06-09 10:18:43,146] INFO [GroupCoordinator 0]: Preparing to rebalance group TEST-REALTIME-SZL in state PreparingRebalance with old generation 393 (__consumer_offsets-49) (reaso
n: error when storing group assignment during SyncGroup (member: consumer-1-0c90d042-0326-4cf2-a870-bb2ae055d140)) (kafka.coordinator.group.GroupCoordinator)                          
[2021-06-09 10:18:43,349] INFO [GroupCoordinator 0]: Stabilized group TEST-REALTIME-SZL generation 394 (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)             
[2021-06-09 10:18:43,349] INFO [GroupCoordinator 0]: Assignment received from leader for group TEST-REALTIME-SZL for generation 394 (kafka.coordinator.group.GroupCoordinator)        
[2021-06-09 10:18:43,349] ERROR [ReplicaManager broker=0] Error processing append operation on partition __consumer_offsets-49 (kafka.server.ReplicaManager)                           
org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition __consumer_offsets-
49  
```

æœåŠ¡ç«¯borkerèŠ‚ç‚¹ä¸Štopic æ­£å¸¸çŠ¶æ€ï¼ˆæ¯ä¸ªtopicçš„partitionçš„leaderå’ŒreplicaçŠ¶æ€ï¼‰
```
[2022-03-16 15:55:15,899] TRACE [Controller id=0] Leader imbalance ratio for broker 2 is 0.0 (kafka.controller.KafkaController)
[2022-03-16 15:55:15,899] DEBUG [Controller id=0] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2022-03-16 15:55:15,899] TRACE [Controller id=0] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2022-03-16 15:55:15,899] DEBUG [Controller id=0] Topics not in preferred replica for broker 0 Map() (kafka.controller.KafkaController)
[2022-03-16 15:55:15,899] TRACE [Controller id=0] Leader imbalance ratio for broker 0 is 0.0 (kafka.controller.KafkaController)
```
éæ­£å¸¸çŠ¶æ€
```
failed to complete preferred replica leader election 

[2022-03-16 10:58:02,837] ERROR [Controller id=1] Error completing preferred replica leader election for partition T-TRADE-1 (kafka.controller.KafkaController)
kafka.common.StateChangeFailedException: Failed to elect leader for partition T-TRADE-1 under strategy PreferredReplicaPartitionLeaderElectionStrategy
        at kafka.controller.PartitionStateMachine.$anonfun$doElectLeaderForPartitions$9(PartitionStateMachine.scala:390)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at kafka.controller.PartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:388)
        at kafka.controller.PartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:315)
        at kafka.controller.PartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:225)
        at kafka.controller.PartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:141)
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$onPreferredReplicaElection(KafkaController.scala:649)
        at kafka.controller.KafkaController.$anonfun$checkAndTriggerAutoLeaderRebalance$6(KafkaController.scala:1008)
        at scala.collection.immutable.Map$Map3.foreach(Map.scala:195)
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$checkAndTriggerAutoLeaderRebalance(KafkaController.scala:989)
        at kafka.controller.KafkaController$AutoPreferredReplicaLeaderElection$.process(KafkaController.scala:1020)
        at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:94)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
        at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:94)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2022-03-16 10:58:02,837] WARN [Controller id=1] Partition T-TEST-SNP-2 failed to complete preferred replica leader election to 2. Leader is still 0 (kafka.controller.KafkaController)
[2022-03-16 10:58:02,837] WARN [Controller id=1] Partition T-TEST-0 failed to complete preferred replica leader election to 2. Leader is still 0 (kafka.controller.KafkaController)
[2022-03-16 10:58:02,838] INFO [Controller id=1] Partition T-TEST2-SNP-2 completed preferred replica leader election. New leader is 2 (kafka.controller.KafkaController)
[2022-03-16 10:58:02,838] WARN [Controller id=1] Partition T-TEST2-1 failed to complete preferred replica leader election to 2. Leader is still 0 (kafka.controller.KafkaController)
```

+ ä¸¢æ•°æ®ï¼šmin.insync.replicas=2 && unclean.leader.election.enable=true (It is default value)
https://stackoverflow.com/questions/57277370/min-insync-replicas-vs-unclean-leader-election

##### é€šç”¨é…ç½®

```
############################# Server Basics #############################
# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

############################# Socket Server Settings #############################
port=9092
host.name=X.X.X.48
advertised.host.name=X.X.X.48
advertised.port=9092
listeners = PLAINTEXT://your.host.name:9092
#advertised.listeners=PLAINTEXT://your.host.name:9092 //This is the metadata thatâ€™s passed back to clients.
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
#Kafka brokers communicate between themselves, usually on the internal network (e.g., Docker network, AWS VPC, etc.). To define which listener to use, specifyï¼š
inter.broker.listener.name //https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic

You need to set advertised.listeners (or KAFKA_ADVERTISED_LISTENERS if youâ€™re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise, theyâ€™ll try to connect to the internal host addressâ€”and if thatâ€™s not reachable, then problems ensue.

https://stackoverflow.com/questions/42998859/kafka-server-configuration-listeners-vs-advertised-listeners
https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
https://cwiki.apache.org/confluence/display/KAFKA/KIP-291%3A+Separating+controller+connections+and+requests+from+the+data+plane

############################# Group Coordinator Settings #############################
# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0

è¿˜çœ‹åˆ°é…ç½® scheduled.rebalance.max.delay.msï¼Œ
https://medium.com/streamthoughts/apache-kafka-rebalance-protocol-or-the-magic-behind-your-streams-applications-e94baf68e4f2
ä½†æ˜¯è¿™å¥½åƒæ˜¯confluenceæä¾›çš„äº§å“ï¼Œå¹¶ä¸æ˜¯kafkaé»˜è®¤çš„

############################# Log Retention Policy #############################
# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=336
# The maximum size of a log segment file. When this size is reached a new log segment will be created.
#log.segment.bytes=1073741824
log.segment.bytes=2147483647
https://stackoverflow.com/questions/65507232/kafka-log-segment-bytes-vs-log-retention-hours

```



##### æµ‹è¯• listenerå·¥å…·ï¼š

å…³äºhost

[Kafka Listeners â€“ Explained](https://www.confluent.io/blog/kafka-listeners-explained/)

+ kafkacat:

  https://github.com/edenhill/kafkacat

  https://docs.confluent.io/platform/current/app-development/kafkacat-usage.html

  ```
  kafkacat -b kafka0:9092 -L
  ```

+ python scripts

  https://github.com/lyhistory/kafka-listeners/blob/master/python/python_kafka_test_client.py

  https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/

+ nc

  ```
   nc -vz 1.1.1.1 9092
  ```


##### replica factor

å¾ˆé‡è¦ï¼Œå¯¹äºæ™®é€šçš„topic replica factoræ¥è¯´ï¼Œreplicaå¤šä¸€äº›æ²¡æœ‰é—®é¢˜ï¼Œä½†æ˜¯å¯¹internal topicè¦ç‰¹åˆ«æ³¨æ„ï¼Œå°¤å…¶æ˜¯å¯¹äº __transaction_stateæ¥è¯´ï¼Œå¦‚æœmin.isrè®¾ç½®è·Ÿreplication.factorè®¾ç½®ä¸€æ ·ï¼Œé‚£ä¹ˆä»»ä½•ä¸€ä¸ªkafkaèŠ‚ç‚¹downæ‰ï¼Œéƒ½ä¼šé€ æˆæ— æ³•å†™å…¥kafkaï¼ˆtransactional producerå†™å…¥ä¼šæŠ¥é”™ NotEnoughReplicasExceptionï¼‰

https://stackoverflow.com/questions/47483016/recommended-settings-for-kafka-internal-topics-after-upgrade-to-1-0

```
############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"                                                                        
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.                                                            
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2
```



#### Client Config

```
--- auto.create.topics.enable

Enable auto creation of topic on the server
Type:	boolean
Default:	true
Valid Values:	
Importance:	high
Update Mode:	read-only

--- request.timeout.ms
The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.

Type:	int
Default:	30000 (30 seconds)

--- scheduled.rebalance.max.delay.ms
The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned

Type:	int
Default:	300000 (5 minutes)

--- session.timeout.ms
 After every rebalance, all members of the current generation begin sending periodic heartbeats to the group coordinator. As long as the coordinator continues receiving heartbeats, it assumes that members are healthy. On every received heartbeat, the coordinator starts (or resets) a timer. If no heartbeat is received when the timer expires, the coordinator marks the member dead and signals the rest of the group that they should rejoin so that partitions can be reassigned. The duration of the timer is known as the session timeout and is configured on the client with the setting session.timeout.ms. 
  The only problem with this is that a spurious rebalance might be triggered if the consumer takes longer than the session timeout to process messages. You should therefore set the session timeout large enough to make this unlikely. The default is 30 seconds, but itâ€™s not unreasonable to set it as high as several minutes. The only downside of a larger session timeout is that it will take longer for the coordinator to detect genuine consumer crashes.
```

â€œThe message is 1626232 bytes when serialized which is larger than the maximum request size you have configured with the max.request.size configuration.â€

producer.properties.max.request.size=838860800 800M

### 4.1 Consumer Indepth

è·³è½¬è‡³ [æ·±å…¥Kafka Consumeræ¶ˆè´¹è€…è§£æ](/docs/software/buildingblock/kafka_consumer)

### 4.2 Producer Indepth

è·³è½¬è‡³ [æ·±å…¥Kafka Producerç”Ÿäº§è€…è§£æ](/docs/software/buildingblock/kafka_producer)

#### è·Ÿbrokeräº¤äº’å…³é”®é…ç½®

acks=all

if the producer receives an  acknowledgement (ack) from the Kafka broker and acks=all, it means that  the message has been written exactly once to the Kafka topic

When a producer sets acks to "all" (or "-1"), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend).
When used together, min.insync.replicas and acks allow you to enforce greater durability guarantees. A typical scenario would be to create a topic with a replication factor of 3, set min.insync.replicas to 2, and produce with acks of "all". This will ensure that the producer raises an exception if a majority of replicas do not receive a write.


### 4.3 Exactly-Once

#### 4.3.1 Exactly-Once-Message-Processing

> there are only two hard problems in distributed systems: 
>
> 1. Guaranteed order of messages 
> 2. Exactly-once delivery

https://www.confluent.io/online-talk/introducing-exactly-once-semantics-in-apache-kafka/

https://www.confluent.io/blog/transactions-apache-kafka/

https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging

https://blog.csdn.net/alex_xfboy/article/details/82988259

KIP-129: Streams Exactly-Once Semantics https://cwiki.apache.org/confluence/display/KAFKA/KIP-129%3A+Streams+Exactly-Once+Semantics

**é‡ç‚¹ï¼š**

+ Producerï¼š

  å¼€å¯å¹‚ç­‰enable.idempotenceå’Œäº‹åŠ¡ transactional.idï¼Œå¹¶ç»´æŠ¤offset

+ Consumerï¼š

  è®¾ç½®isolation.level=read_committedï¼ˆé…åˆäº‹åŠ¡å‹Producerï¼‰

+ è·Ÿ kafka borkeräº¤äº’ï¼š

  consumeræ ¹æ®kafka brokerçš„rebalanceæ¥ä¸ºæ¯ä¸ªpartitionåˆ›å»ºproducerï¼Œäº‹åŠ¡å‹ produceré€šè¿‡initTransactionæ“ä½œæ¥fence zombieï¼ˆä»ç„¶æ˜¯ä¾é kafka brokerï¼‰ï¼Œä»è€Œå±è”½æ‰å…¶ä»–è¿‡æ—¶çš„producerï¼ˆrebalanceè¿‡ç¨‹ä¸­è¢«æ”¶å›äº†partitionçš„consumerä¹‹å‰æ‰€åˆ›å»ºçš„producerï¼‰æ¶ˆè´¹æ¶ˆæ¯çš„å¯èƒ½æ€§ï¼Œç„¶åconsumerå¯ä»¥æ”¾å¿ƒçš„restore



The first generation of stream  processing applications could tolerate inaccurate processing. For  instance, applications which consumed a stream of web page impressions  and produced aggregate counts of views per web page could tolerate some  error in the counts. 

However, the demand for stream  processing applications with stronger semantics has grown along with the popularity of these applications. For instance, some financial  institutions use stream processing applications to process debits and  credits on user accounts. In these situations, there is no tolerance for errors in processing: we need every message to be processed exactly  once, without exception.



ä»æŸä¸ªTopicçš„æŸä¸ªPartitionçš„æ•°æ®æµçœ‹ atomic read-process-write patternï¼ˆå¯¹äºä½¿ç”¨kafka stream çš„åº”ç”¨æ¥è¯´å°±æ˜¯ consume-transform-produceï¼‰ï¼š

More formally, if a stream processing application consumes message *A* and produces message *B* such that *B = F(A)*, then exactly  once processing means that *A* is considered consumed if and only if *B* is successfully produced, and vice versa.

message *A* will be considered consumed from topic-partition *tp0* only when its offset *X* is marked as consumed. Marking an offset as consumed is called *committing an offset.* In Kafka, we record offset commits by writing to an internal Kafka topic called the *offsets topic*. A message is considered consumed only when its offset is committed to the offsets topic

Thus since an offset commit is just  another write to a Kafka topic, and since a message is considered  consumed only when its offset is committed, atomic writes across  multiple topics and partitions also enable atomic read-process-write  cycles: the commit of the offset *X* to the offsets topic and the write of message *B* to *tp1* will be part of a single transaction, and hence atomic.

è®¾è®¡ï¼š

æ¯ä¸ªapplicationè®¢é˜…ä¸€ä¸ªä¸»é¢˜ï¼Œåˆ›å»ºä¸€ä¸ªKafkaConsumerï¼Œå‘ç”Ÿrebalanceä¹‹åæ ¹æ®åˆ†é…çš„partitionï¼Œæ¯ä¸ªpartitionéƒ½åˆ›å»ºä¸€ä¸ªTransactional KafkaProducer

1.ä¸Šæ¸¸KafkaProducerï¼š

+ ä¿è¯ä¸ä¼šé‡å¤å‘é€ï¼š

  1) retry é‡å‘åœºæ™¯ï¼šThe producer.send() could result in duplicate writes of message *B* due to internal retries. 

  è§£å†³æ–¹æ³•ï¼šThis is addressed by the idempotent producer: enable.idempotence=true

  2) Reprocessing may happen if the stream processing application crashes after writing *B* but before marking *A* as consumed. Thus when it resumes, it will consume *A* again and write *B* again, causing a duplicate. 

  è§£å†³æ–¹æ³•ï¼šä½¿ç”¨transactionï¼Œå°† write Bå’Œmark A consumedï¼ˆself main consumer offset on kafka topic partitionï¼‰ä½œä¸ºä¸€ä¸ªtransaction

  3) æŒ‚èµ·ååˆæ¢å¤åœºæ™¯ï¼Œapplications will crash  orâ€”worse!â€”temporarily lose connectivity to the rest of the system.  Typically, new instances are automatically started to replace the ones  which were deemed lost. Through this process, we may have multiple  instances processing the same input topics and writing to the same  output topics, causing duplicate outputs and violating the exactly once  processing semantics. We call this the problem of â€œzombie instances.â€

  æ¯”å¦‚Producerå‘é€Kafkaæ¶ˆæ¯ï¼ˆcommitTransactionä¹‹å‰ï¼‰çªç„¶æŒ‚èµ·ï¼Œç„¶åtransaction.idè¢«æ–°èµ·çš„Producerå ç”¨ï¼Œå½“ä¹‹å‰çš„Produceråˆæ¢å¤çš„æ—¶å€™å†commitTransactionä¼šè¢«Fenceå±è”½æ‰ï¼Œå¹¶ä¸”KafkaConsumerçš„read_committedéš”ç¦»çº§åˆ«ä¹Ÿå¯ä»¥ä¿è¯ä¸è¯»å–è¿™äº›å°šæœªæäº¤çš„äº‹åŠ¡æ¶ˆæ¯ï¼ˆThe Kafka consumer will only deliver  transactional messages to the application if the transaction was  actually committed. Put another way, the consumer will not deliver  transactional messages which are part of an open transaction, and nor  will it deliver messages which are part of an aborted transaction.

  when using a Kafka consumer to consume  messages from a topic, an application will not know whether these  messages were written as part of a transaction, and so they do not know  when transactions start or end. 

  In short: Kafka guarantees that a  consumer will eventually deliver only non-transactional messages or  committed transactional messages. It will withhold messages from open  transactions and filter out messages from aborted transactions.ï¼‰

  è§£å†³æ–¹æ³•ï¼šé€šè¿‡ transaction.id è¿›è¡Œ zombie Fence

  4ï¼‰consumer groupä¸­å¤šä¸ªæœåŠ¡å‘ç”Ÿrebalanceåé‡å¯çš„åœºæ™¯ï¼šæœåŠ¡Aå¤„ç†Paritition 0å’Œ1ï¼Œç„¶åæœåŠ¡Bè¿›å…¥å¼•èµ·rebalanceï¼Œåˆ†é…ç»“æœä¸ºBæ¥ç®¡Partition 0ï¼Œå½“Båˆ›å»ºKafkaProducerå¤„ç†partition 0æ—¶ï¼Œé¦–å…ˆè¦å®šä½åˆ°partition 0ä¹‹å‰è¢«å¤„ç†åˆ°å“ªé‡Œäº†ï¼Œè¿™ç§åœºæ™¯ä»ç„¶æ˜¯å…ˆä½¿ç”¨ Transaction.idè¿›è¡Œzombie fenceï¼Œä»¥é˜²æœåŠ¡Açš„å¤„ç†Partition 0çš„kafkaproducerè¿˜åœ¨å·¥ä½œï¼ˆæ¯”å¦‚ä½¿ç”¨äº†disruptoræœ‰ringbufferç¼“å­˜ï¼Œå³ä½¿æ²¡æœ‰ç”¨disruptorè¿™ç§ä¸œè¥¿ï¼Œç”±äºAå’ŒBæ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„è¿›ç¨‹ï¼ŒBä¹Ÿæ— æ³•ä¿è¯Açš„kafka produceræ²¡æœ‰ç»§ç»­å†å¤„ç†æŸæ¡æ¶ˆæ¯ï¼Œæ‰€ä»¥å…ˆå°äººåå›å­ï¼‰ï¼Œç„¶åæ‰è¯»å–offsetï¼Œå®šä½åˆ°ä¹‹å‰Aåœ¨Partition 0æˆåŠŸæäº¤çš„æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆæ¢å¥è¯å°±æ˜¯è¢«zombie fenceä¹‹å‰æˆåŠŸçš„æœ€åä¸€æ¡æ¶ˆæ¯ï¼‰

+ ä¿è¯ä¸æ¼å‘ï¼š
  + KafkaProduceréœ€è¦ä½¿ç”¨å¹‚ç­‰æ€§ï¼Œå¹¶ä¸”åƒä¸‡ä¸è¦è®¾ç½® ETRIES_CONFIG = "retries"ï¼Œä½¿ç”¨é»˜è®¤çš„Integer.MAX_VALUE
  + é¦–å…ˆä¸Šæ¸¸ä¸è¦æ¼æ‰å¤„ç†æŸä¸ªä¿¡æ¯ï¼Œæ­¤æ—¶å¦‚æœä¸Šæ¸¸çš„è§’è‰²å°±æ˜¯kafkaConsumerï¼Œé—®é¢˜è¿›ä¸€æ­¥åˆ†è§£ä¸ºï¼š
    + ä¸Šæ¸¸ä¸è¦æ¼æ‰æŸæ¡kafkaæ¶ˆæ¯ï¼Œè¿™ä¸ªå®Œå…¨ç”±kafkaä¿è¯
    + æ”¶åˆ°kafkaæ¶ˆæ¯åï¼Œå¤„ç†åˆ°å‘é€ç»™ä¸‹æ¸¸çš„è¿‡ç¨‹ä¸è¦â€œæ¼â€ï¼Œå³è¿™ä¸ªè¿‡ç¨‹è¦ä¿è¯é²æ£’æ€§å’Œä¸€è‡´æ€§
  + å…¶æ¬¡å¤„ç†å®Œåè°ƒç”¨kafkaProduceræ¥å£sendç»™ä¸‹æ¸¸ï¼Œè¿™ä¸ªè°ƒç”¨å®Œå…¨æ˜¯kafkaæ¥ä¿è¯ä¸ä¼šæ¼

2.ä¸‹æ¸¸KafkaConsumerï¼š

+ ä¿è¯ä¸è¦é‡å¤å¤„ç†ï¼š

  + é»˜è®¤è¿™ä¸ªæ˜¯ç”±kafka offsetæ¥æ§åˆ¶ï¼Œ_consumer_offsetï¼Œä½†æ˜¯åœ¨æŸäº›åœºæ™¯ä¸‹ä½¿ç”¨ _consumer_offset ä¼šæœ‰é—®é¢˜ï¼š

    æ¯”å¦‚KafkaConsumerå¤„ç†æ•°æ®çš„è¿‡ç¨‹åŒ…æ‹¬ä½œä¸ºKafkaProducerç»§ç»­å¾€ä¸‹æ¸¸å‘é€æ¶ˆæ¯ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡KafkaProducerçš„transactionå¤„ç†ï¼ŒåŒæ—¶å‘é€ä¸€ä¸ªå¢é‡å¿«ç…§ä¿å­˜èµ·å½“å‰çš„offsetï¼Œè¿™æ ·å°±å¯ä»¥è„±ç¦»å¼€ç³»ç»Ÿé»˜è®¤çš„__consumer_offset

  + å¦å¤–ä¸€ä¸ªæ¯”è¾ƒéšå«çš„åœºæ™¯å°±æ˜¯ä¸Šæ¸¸å‡ºäº†é—®é¢˜ï¼Œå‚è€ƒä¸Šæ¸¸çš„åœºæ™¯3ï¼‰å’Œ4ï¼‰

+ ä¿è¯ä¸è¦ä¸¢å¤±æ¶ˆæ¯ï¼š

  1ï¼‰ä¸è¦ä½¿ç”¨ auto commitï¼Œå› ä¸ºè°ƒç”¨pollä¹‹åï¼Œæ¶ˆæ¯offsetå¯èƒ½å·²ç»è¢«è‡ªåŠ¨æäº¤ï¼Œä½†æ˜¯æ­¤æ—¶å¦‚æœæ²¡æœ‰å¤„ç†å®Œç¨‹åºå´©æºƒï¼Œå†å¯åŠ¨å°±ä¼šä¸¢å¤±æ¶ˆæ¯ï¼›

  è€Œæ˜¯é‡‡ç”¨ atomic read-process-writeæ¨¡å‹ï¼Œå°†writeå’ŒMark readä½œä¸ºä¸€ä¸ªtransactionï¼Œè¿™æ ·ä¸ä¼šå‡ºç°å‰é¢çš„ï¼š æ¶ˆæ¯è¢«æ ‡è®°ä¸ºå·²è¯»ï¼Œä½†æ˜¯è¿˜æ²¡å¤„ç†å®Œç¨‹åºå´©æºƒï¼Œé‡å¯ä¹‹åä¸¢å¤±çš„é—®é¢˜ï¼›

  2ï¼‰ä¸€æ—¦produceræˆåŠŸå°†äº‹åŠ¡å‹çš„æ¶ˆæ¯æˆ–éäº‹åŠ¡å‹çš„æ¶ˆæ¯å‘é€ç»™äº†kafkaï¼ˆæˆåŠŸè°ƒç”¨send apiæˆ–commit transactionï¼‰ï¼Œæ¥ä¸‹æ¥å°±æ˜¯kafkaçš„é›†ç¾¤æ¥ä¿è¯æ¶ˆæ¯ä¸ä¼šä¸¢æ‰äº†ï¼ˆæ¯”å¦‚å…³äºreplication high watermarkï¼‰

![](/docs/docs_image/software/buildingblock/kafka/kafka_exactly_once01.png)

å¦‚å›¾ä¸­æ‰€ç¤ºï¼Œsolution Aä¸å®Œç¾ï¼Œå› ä¸ºè§£å†³ä¸äº†æœåŠ¡Aå› ä¸ºç½‘ç»œè·Ÿkafkaé›†ç¾¤æ–­å¼€åˆæ¢å¤çš„åœºæ™¯ä¸‹æœ‰å¯èƒ½åœ¨æçŸ­çš„æ—¶é—´çª—å£å‘ç”Ÿçš„é‡å¤æ¶ˆè´¹é—®é¢˜ï¼Œsolution Bæ˜¯æœ€å®Œç¾çš„è®¾è®¡ï¼Œå……åˆ†åˆ©ç”¨äº†kafkaçš„exactly onceèƒ½åŠ›

![](/docs/docs_image/software/buildingblock/kafka/kafka_exactly_once02.png)


#### 4.3.2 Exactly-Once-Stream-Processsing

or stream processing applications built  using Kafkaâ€™s Streams API, we leverage the fact that the source of truth for the state store and the input offsets are Kafka topics. Hence we  can transparently fold this data into transactions that atomically write to multiple partitions, and thus provide the exactly-once guarantee for streams across the read-process-write operations.

```
processing.guarantee=exactly_once

Note that exactly-once semantics is guaranteed within the scope of Kafka Streamsâ€™ internal processing only; for example, if the event streaming app written in Streams makes an RPC call to update some remote stores, or if it uses a customized client to directly read or write to a Kafka topic, the resulting side effects would not be guaranteed exactly once. 
```

### 4.4 Diving into Kafka

å‰é¢4.1 4.2 4.3 ä¸»è¦æ˜¯å°†kafkaå½“åšé»‘ç›’ï¼Œç„¶åé€šè¿‡kafkaå¼€æ”¾çš„APIæ¥è¾¾åˆ°è·Ÿkafkaäº¤äº’çš„exactly-onceï¼Œ

ä½†æ˜¯kafkaæœ¬èº«çš„å¾ˆå¤šç»†èŠ‚ä¹Ÿä¼šå½±å“åˆ°ä½¿ç”¨æ€§èƒ½ç”šè‡³æ˜¯å¯ç”¨æ€§ï¼Œæ‰€ä»¥è¿˜éœ€è¦æ·±å…¥kafkaï¼Œäº†è§£æ¯”å¦‚ï¼š

å‰¯æœ¬ replication ç­‰ç»†èŠ‚

https://www.cnblogs.com/luozhiyun/p/12079527.html

#### leader epoch & high watermark

Kafka is a highly available, persistent, durable system where every message written to a partition is persisted  and replicated some number of times (we will call it *n*). As a result, Kafka can tolerate *n-1* broker failures, meaning that a partition is available as long as there is at least one broker available. Kafkaâ€™s replication protocol  guarantees that once a message has been written successfully to the  leader replica, it will be replicated to all available replicas. 

https://rongxinblog.wordpress.com/2016/07/29/kafka-high-watermark/

https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation
Kafkaæ•°æ®ä¸¢å¤±åŠæœ€æ–°æ”¹è¿›ç­–ç•¥ http://lday.me/2017/10/08/0014_kafka_data_loss_and_new_mechanism/
kafka ISRè®¾è®¡åŠæ°´å°ä¸leader epochå‰¯æœ¬åŒæ­¥æœºåˆ¶æ·±å…¥å‰–æ-kafka å•†ä¸šç¯å¢ƒå®æˆ˜ https://juejin.im/post/5bf6b0acf265da612d18e931
leader epochä¸watermark https://www.cnblogs.com/huxi2b/p/7453543.html
High watermark
If you want to improve the reliability of the data, set the request.required.acks = -1, but also min.insync.replicas this parameter (which can be set in the broker or topic level) to achieve maximum effectiveness. 
https://medium.com/@mukeshkumar_46704/in-depth-kafka-message-queue-principles-of-high-reliability-42e464e66172

#### Consumer coordinator & Group coordinator & Rebalance

https://matt33.com/2017/10/22/consumer-join-group/

https://cloud.tencent.com/developer/news/19958

> While the old consumer depended on Zookeeper for group management, the new consumer uses a group coordination protocol built into Kafka itself. For each group, one of the brokers is selected as the *group coordinator*. The coordinator is responsible for managing the state of the group. Its main job is to mediate partition assignment when new members arrive, old members depart, and when topic metadata changes. The act of reassigning partitions is known as *rebalancing* the group.
>
> https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/

keyword: 

+ GroupCoordinator  æœåŠ¡ç«¯ borkerï¼ˆKafka ä¼šä¾æ®è¯·æ±‚çš„ group çš„ ID æŸ¥æ‰¾å¯¹åº” offset topic [__consumer_offsets?]åˆ†åŒº leader å‰¯æœ¬æ‰€åœ¨çš„ broker èŠ‚ç‚¹ï¼š__

  æ¯ä¸ª Group éƒ½ä¼šé€‰æ‹©ä¸€ä¸ª Coordinator æ¥å®Œæˆè‡ªå·±ç»„å†…å„ Partition çš„ Offset ä¿¡æ¯ï¼Œé€‰æ‹©çš„è§„åˆ™å¦‚ä¸‹ï¼š 1. è®¡ç®— Group å¯¹åº”åœ¨ __consumer_offsets` ä¸Šçš„ `Partition 2. æ ¹æ®å¯¹åº”çš„Partitionå¯»æ‰¾è¯¥Partitionçš„leaderæ‰€å¯¹åº”çš„Brokerï¼Œè¯¥Brokerä¸Šçš„Group Coordinatorå³å°±æ˜¯è¯¥Groupçš„Coordinatorï¼‰

+ ConsumerCoordinator 

+ Consumer leader/follower ç¬¬ä¸€ä¸ªç”³è¯·åŠ å…¥groupçš„consumerä½œä¸ºleaderï¼Œåœ¨æœåŠ¡ç«¯ç¡®å®šå¥½åˆ†åŒºåˆ†é…ç­–ç•¥ä¹‹åï¼Œå…·ä½“æ‰§è¡Œåˆ†åŒºåˆ†é…çš„å·¥ä½œåˆ™äº¤ç”± leader æ¶ˆè´¹è€…è´Ÿè´£ï¼Œå¹¶åœ¨å®Œæˆåˆ†åŒºåˆ†é…ä¹‹åå°†åˆ†é…ç»“æœåé¦ˆç»™æœåŠ¡ç«¯

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Client-side+Assignment+Proposal

https://www.zhenchao.org/2019/06/25/kafka/kafka-group-coordinator/

åœ¨ kafka-0.10 ç‰ˆæœ¬ï¼ŒKafka åœ¨**æœåŠ¡ç«¯å¼•å…¥äº†ç»„åè°ƒå™¨(GroupCoordinator)**ï¼Œæ¯ä¸ª Kafka Server å¯åŠ¨æ—¶éƒ½ä¼šåˆ›å»ºä¸€ä¸ª GroupCoordinator å®ä¾‹ï¼Œ**ç”¨äºç®¡ç†éƒ¨åˆ†æ¶ˆè´¹è€…ç»„å’Œè¯¥æ¶ˆè´¹è€…ç»„ä¸‹çš„æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹åç§»é‡**ã€‚åŒæ—¶**åœ¨å®¢æˆ·ç«¯å¼•å…¥äº†æ¶ˆè´¹è€…åè°ƒå™¨(ConsumerCoordinator)**ï¼Œå®ä¾‹åŒ–ä¸€ä¸ªæ¶ˆè´¹è€…å°±ä¼šå®ä¾‹åŒ–ä¸€ä¸ª ConsumerCoordinator å¯¹è±¡ï¼ŒConsumerCoordinator **è´Ÿè´£åŒä¸€ä¸ªæ¶ˆè´¹è€…ç»„ä¸‹å„æ¶ˆè´¹è€…ä¸æœåŠ¡ç«¯çš„ GroupCoordinator è¿›è¡Œé€šä¿¡**ã€‚

##### å®¢æˆ·ç«¯-æ¶ˆè´¹è€…åè°ƒå™¨(ConsumerCoordinator)

To control this assignment, users can either write an implementation of the [`PartitionAssignor`](https://github.com/apache/kafka/blob/2.2/clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignor.java) interface or use one of the three provided implementations (configured through the `partition.assignment.strategy` config):

- [`RangeAssignor`](https://github.com/apache/kafka/blob/b6d1450012734a841fdf85400636593092cf7e2b/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java#L29): For each topic, divide its partitions by the number of consumers subscribed to it and assign X to each (lexicographically sorted) consumer. If it does not evenly divide, the first consumers will have more partitions.
- [`RoundRobinAssignor`](https://github.com/apache/kafka/blob/2.2/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java): Assign all partitions from all the subscribed topics to each consumer sequentially, one by one.
- [`StickyAssignor`](https://github.com/apache/kafka/blob/2.2/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java): Assign partitions so that they are distributed as evenly as possible. During rebalances, partitions stay with their previously assigned consumers as much as possible.

Additionally, the `PartitionAssignor` interface exposes a `metadata()` method. Every consumer in the group can use this method to send generic metadata about itself to the broker when joining a group. Once a rebalance is in the works, every consumerâ€™s metadata is propagated to the group leader. This enables the leader to make a well-informed decision about assigning partitions (e.g., by considering a consumer applicationâ€™s datacenter rack).

```java
public class KafkaConsumer<K, V> implements Consumer<K, V> {
    private final ConsumerCoordinator coordinator;
}
public final class ConsumerCoordinator extends AbstractCoordinator {
    private final List<PartitionAssignor> assignors;
    private final OffsetCommitCallback defaultOffsetCommitCallback;
    private final SubscriptionState subscriptions;
    private final ConsumerInterceptors<?, ?> interceptors;
    private boolean isLeader = false;
    private MetadataSnapshot metadataSnapshot;
    private MetadataSnapshot assignmentSnapshot;
    
    çœç•¥äº†éƒ¨åˆ†ä»£ç ....
}


public abstract class AbstractCoordinator implements Closeable {
    private enum MemberState {
        UNJOINED,    // the client is not part of a group
        REBALANCING, // the client has begun rebalancing
        STABLE,      // the client has joined and is sending heartbeats
    }

    private final Heartbeat heartbeat;
    protected final ConsumerNetworkClient client;
    private HeartbeatThread heartbeatThread = null;
    private MemberState state = MemberState.UNJOINED;
    private RequestFuture<ByteBuffer> joinFuture = null;
    
    çœç•¥äº†éƒ¨åˆ†ä»£ç ....
}
```

ConsumerCoordinator æ˜¯ KafkaConsumer çš„ä¸€ä¸ªç§æœ‰çš„æˆå‘˜å˜é‡ï¼Œå› æ­¤ ConsumerCoordinator ä¸­å­˜å‚¨çš„ä¿¡æ¯ä¹Ÿåªæœ‰ä¸ä¹‹å¯¹åº”çš„æ¶ˆè´¹è€…å¯è§ï¼Œä¸åŒæ¶ˆè´¹è€…ä¹‹é—´æ˜¯çœ‹ä¸åˆ°å½¼æ­¤çš„ ConsumerCoordinator ä¸­çš„ä¿¡æ¯çš„ã€‚

ConsumerCoordinator çš„ä½œç”¨ï¼š

- å¤„ç†æ›´æ–°æ¶ˆè´¹è€…ç¼“å­˜çš„ Metadata è¯·æ±‚
- å‘ç»„åè°ƒå™¨å‘èµ·åŠ å…¥æ¶ˆè´¹è€…ç»„çš„è¯·æ±‚
- å¯¹æœ¬æ¶ˆè´¹è€…åŠ å…¥æ¶ˆè´¹è€…å‰åçš„ç›¸åº”å¤„ç†
- è¯·æ±‚ç¦»å¼€æ¶ˆè´¹è€…ç»„(ä¾‹å¦‚å½“æ¶ˆè´¹è€…å–æ¶ˆè®¢é˜…æ—¶)
- å‘ç»„åè°ƒå™¨å‘é€æäº¤åç§»é‡çš„è¯·æ±‚
- é€šè¿‡ä¸€ä¸ªå®šæ—¶çš„å¿ƒè·³æ£€æµ‹ä»»åŠ¡æ¥è®©ç»„åè°ƒå™¨æ„ŸçŸ¥è‡ªå·±çš„è¿è¡ŒçŠ¶æ€
- Leaderæ¶ˆè´¹è€…çš„ ConsumerCoordinator è¿˜è´Ÿè´£æ‰§è¡Œåˆ†åŒºçš„åˆ†é…ï¼Œä¸€ä¸ªæ¶ˆè´¹è€…ç»„ä¸­æ¶ˆè´¹è€… leader ç”±ç»„åè°ƒå™¨é€‰å‡ºï¼Œleader æ¶ˆè´¹è€…çš„ ConsumerCoordinator è´Ÿè´£æ¶ˆè´¹è€…ä¸åˆ†åŒºçš„åˆ†é…ï¼Œç„¶åæŠŠåˆ†é…ç»“æœå‘é€ç»™ç»„åè°ƒå™¨ï¼Œç„¶åç»„åè°ƒå™¨å†æŠŠåˆ†é…ç»“æœè¿”å›ç»™å…¶ä»–æ¶ˆè´¹è€…çš„æ¶ˆè´¹è€…åè°ƒå™¨ï¼Œè¿™æ ·å‡è½»äº†æœåŠ¡ç«¯çš„è´Ÿæ‹…

ConsumerCoordinator å®ç°ä¸Šè¿°åŠŸèƒ½çš„ç»„ä»¶æ˜¯ ConsumerCoordinator ç±»çš„ç§æœ‰æˆå‘˜æˆ–è€…æ˜¯å…¶çˆ¶ç±»çš„ç§æœ‰æˆå‘˜ï¼š

![](/docs/docs_image/software/buildingblock/kafka/kafka_consumercoordinator.png)

##### æœåŠ¡ç«¯-ç»„åè°ƒå™¨(GroupCoordinator)

```
class GroupCoordinator(
                       val brokerId: Int, // æ‰€å±çš„ broker èŠ‚ç‚¹çš„ ID
                       val groupConfig: GroupConfig, // Group é…ç½®å¯¹è±¡ï¼Œè®°å½•äº† group ä¸­ session è¿‡æœŸçš„æœ€å°æ—¶é•¿å’Œæœ€å¤§æ—¶é•¿ï¼Œå³è¶…æ—¶æ—¶é•¿çš„åˆæ³•åŒºé—´
                       val offsetConfig: OffsetConfig, // è®°å½• OffsetMetadata ç›¸å…³çš„é…ç½®é¡¹
                       val groupManager: GroupMetadataManager, // è´Ÿè´£ç®¡ç† group å…ƒæ•°æ®ä»¥åŠå¯¹åº”çš„ offset ä¿¡æ¯
                       val heartbeatPurgatory: DelayedOperationPurgatory[DelayedHeartbeat], // ç®¡ç† DelayedHeartbeat å»¶æ—¶ä»»åŠ¡çš„ç‚¼ç‹±
                       val joinPurgatory: DelayedOperationPurgatory[DelayedJoin], // ç®¡ç† DelayedJoin å»¶æ—¶ä»»åŠ¡çš„ç‚¼ç‹±
                       time: Time) extends Logging {

    /** æ ‡è¯†å½“å‰ GroupCoordinator å®ä¾‹æ˜¯å¦å¯åŠ¨ */
    private val isActive = new AtomicBoolean(false)

    // ... çœç•¥æ–¹æ³•å®šä¹‰

}

å…¶ä¸­ GroupMetadataManager ç±»ä¸»è¦ç”¨äºç®¡ç†æ¶ˆè´¹è€… group çš„å…ƒæ•°æ®ä¿¡æ¯å’Œ offset ç›¸å…³ä¿¡æ¯

Kafka æœåŠ¡åœ¨å¯åŠ¨æ—¶é’ˆå¯¹æ¯ä¸€ä¸ª broker èŠ‚ç‚¹éƒ½ä¼šåˆ›å»ºä¸€ä¸ª GroupCoordinator å®ä¾‹ï¼Œå¹¶è°ƒç”¨ GroupCoordinator#startup æ–¹æ³•å¯åŠ¨è¿è¡Œã€‚GroupCoordinator åœ¨å¯åŠ¨æ—¶ä¸»è¦æ˜¯è°ƒç”¨äº† GroupMetadataManager#enableMetadataExpiration æ–¹æ³•å¯åŠ¨ delete-expired-group-metadata å®šæ—¶ä»»åŠ¡

å®šæ—¶ä»»åŠ¡ delete-expired-group-metadata çš„ä¸»è¦ä½œç”¨åœ¨äºä» group çš„å…ƒæ•°æ®ä¿¡æ¯ä¸­ç§»é™¤é‚£äº›å·²ç»è¿‡æœŸçš„ topic åˆ†åŒºå¯¹åº”çš„ offset å…ƒæ•°æ®ï¼Œå¹¶å°†è¿™äº›å…ƒæ•°æ®ä»¥æ¶ˆæ¯çš„å½¢å¼è®°å½•åˆ° offset topic ä¸­ï¼Œå…·ä½“æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š

1.ä¾æ®å½“å‰æ—¶é—´æˆ³è®¡ç®—å¹¶è·å–å·²ç»è¿‡æœŸçš„ topic åˆ†åŒºå¯¹åº”çš„ offset å…ƒæ•°æ®ä¿¡æ¯ï¼›
2.å°†çŠ¶æ€ä¸º Empty ä¸”åä¸‹è®°å½•çš„æ‰€æœ‰ offset å…ƒæ•°æ®éƒ½å·²ç»è¿‡æœŸçš„ group åˆ‡æ¢æˆ Dead çŠ¶æ€ï¼›
3.å¦‚æœ group å·²ç»å¤±æ•ˆï¼Œåˆ™ä» GroupCoordinator æœ¬åœ°ç§»é™¤å¯¹åº”çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œå¹¶ä¸æ­¥éª¤ 1 ä¸­è·å–åˆ°çš„ offset å…ƒæ•°æ®ä¿¡æ¯ä¸€èµ·å°è£…æˆæ¶ˆæ¯è®°å½•åˆ° offset topic ä¸­ã€‚
å…·ä½“é€»è¾‘ç”± GroupMetadataManager#cleanupGroupMetadata æ–¹æ³•å®ç°


GroupState ç‰¹è´¨å®šä¹‰äº† group çš„çŠ¶æ€ï¼Œå¹¶ç”± GroupCoordinator è¿›è¡Œç»´æŠ¤ã€‚å›´ç»• GroupState ç‰¹è´¨ï¼ŒKafka å®ç°äº† 5 ä¸ªæ ·ä¾‹å¯¹è±¡ï¼Œåˆ†åˆ«ç”¨äºæè¿° group çš„ 5 ç§çŠ¶æ€ï¼š

PreparingRebalance ï¼šè¡¨ç¤º group æ­£åœ¨å‡†å¤‡æ‰§è¡Œåˆ†åŒºå†åˆ†é…æ“ä½œã€‚
AwaitingSync ï¼šè¡¨ç¤º group æ­£åœ¨ç­‰å¾… leader æ¶ˆè´¹è€…çš„åˆ†åŒºåˆ†é…ç»“æœï¼Œæ–°ç‰ˆæœ¬å·²æ›´åä¸º CompletingRebalanceã€‚
Stable ï¼šè¡¨ç¤º group å¤„äºæ­£å¸¸è¿è¡ŒçŠ¶æ€ã€‚
Dead ï¼šè¡¨ç¤º group åä¸‹å·²ç»æ²¡æœ‰æ¶ˆè´¹è€…ï¼Œä¸”å¯¹åº”çš„å…ƒæ•°æ®å·²ç»ï¼ˆæˆ–æ­£åœ¨ï¼‰è¢«åˆ é™¤ã€‚
Empty ï¼šè¡¨ç¤º group åä¸‹å·²ç»æ²¡æœ‰æ¶ˆè´¹è€…ï¼Œå¹¶ä¸”æ­£åœ¨ç­‰å¾…è®°å½•çš„æ‰€æœ‰ offset å…ƒæ•°æ®è¿‡æœŸã€‚


```



GroupCoordinator çš„ä½œç”¨ï¼š

- è´Ÿè´£å¯¹å…¶ç®¡ç†çš„ç»„å‘˜(æ¶ˆè´¹è€…)æäº¤çš„ç›¸å…³è¯·æ±‚è¿›è¡Œå¤„ç†
- ä¸æ¶ˆè´¹è€…ä¹‹é—´å»ºç«‹è¿æ¥ï¼Œå¹¶ä»ä¸ä¹‹è¿æ¥çš„æ¶ˆè´¹è€…ä¹‹é—´é€‰å‡ºä¸€ä¸ª leader
- å½“ leader åˆ†é…å¥½æ¶ˆè´¹è€…ä¸åˆ†åŒºçš„è®¢é˜…å…³ç³»åï¼Œä¼šæŠŠç»“æœå‘é€ç»™ç»„åè°ƒå™¨ï¼Œç»„åè°ƒå™¨å†æŠŠç»“æœè¿”å›ç»™å„ä¸ªæ¶ˆè´¹è€…
- ç®¡ç†ä¸ä¹‹è¿æ¥çš„æ¶ˆè´¹è€…çš„æ¶ˆè´¹åç§»é‡çš„æäº¤ï¼Œå°†æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹åç§»é‡ä¿å­˜åˆ°kafkaçš„å†…éƒ¨ä¸»é¢˜ä¸­
- é€šè¿‡å¿ƒè·³æ£€æµ‹æ¶ˆè´¹è€…ä¸è‡ªå·±çš„è¿æ¥çŠ¶æ€
- å¯åŠ¨ç»„åè°ƒå™¨çš„æ—¶å€™åˆ›å»ºä¸€ä¸ªå®šæ—¶ä»»åŠ¡ï¼Œç”¨äºæ¸…ç†è¿‡æœŸçš„æ¶ˆè´¹ç»„å…ƒæ•°æ®ä»¥åŠè¿‡å»çš„æ¶ˆè´¹åç§»é‡ä¿¡æ¯

GroupCoordinator ä¾èµ–çš„ç»„ä»¶åŠå…¶ä½œç”¨ï¼š

![](/docs/docs_image/software/buildingblock/kafka/kafka_groupcoordinator.png)

- KafkaConfigï¼šå®ä¾‹åŒ– OffsetConfig å’Œ GroupConfig
- ZkUtilsï¼šåˆ†æ¶ˆè´¹è€…åˆ†é…ç»„åè°ƒå™¨æ—¶ä»Zookeeperè·å–å†…éƒ¨ä¸»é¢˜çš„åˆ†åŒºå…ƒæ•°æ®ä¿¡æ¯ã€‚
- GroupMetadataManagerï¼šè´Ÿè´£ç®¡ç† GroupMetadataä»¥åŠæ¶ˆè´¹åç§»é‡çš„æäº¤ï¼Œå¹¶æä¾›äº†ä¸€ç³»åˆ—çš„ç»„ç®¡ç†çš„æ–¹æ³•ä¾›ç»„åè°ƒå™¨è°ƒç”¨ã€‚GroupMetadataManager ä¸ä»…æŠŠ GroupMetadata å†™åˆ°kafkaå†…éƒ¨ä¸»é¢˜ä¸­ï¼Œè€Œä¸”è¿˜åœ¨å†…å­˜ä¸­ç¼“å­˜äº†ä¸€ä»½GroupMetadataï¼Œå…¶ä¸­åŒ…æ‹¬äº†ç»„å‘˜(æ¶ˆè´¹è€…)çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œä¾‹å¦‚æ¶ˆè´¹è€…çš„ memberIdã€leaderIdã€åˆ†åŒºåˆ†é…å…³ç³»ï¼ŒçŠ¶æ€å…ƒæ•°æ®ç­‰ã€‚çŠ¶æ€å…ƒæ•°æ®å¯ä»¥æ˜¯ä»¥ä¸‹äº”ç§çŠ¶æ€ï¼š 
  - PreparingRebalanceï¼šæ¶ˆè´¹ç»„å‡†å¤‡è¿›è¡Œå¹³è¡¡æ“ä½œ
  - AwaitingSyncï¼šç­‰å¾…leaderæ¶ˆè´¹è€…å°†åˆ†åŒºåˆ†é…å…³ç³»å‘é€ç»™ç»„åè°ƒå™¨
  - Stableï¼šæ¶ˆè´¹è€…æ­£å¸¸è¿è¡ŒçŠ¶æ€ï¼Œå¿ƒè·³æ£€æµ‹æ­£å¸¸
  - Deadï¼šå¤„äºè¯¥çŠ¶æ€çš„æ¶ˆè´¹ç»„æ²¡æœ‰ä»»ä½•æ¶ˆè´¹è€…æˆå‘˜ï¼Œä¸”å…ƒæ•°æ®ä¿¡æ¯ä¹Ÿå·²ç»è¢«åˆ é™¤
  - Emptyï¼šå¤„äºè¯¥çŠ¶æ€çš„æ¶ˆè´¹ç»„æ²¡æœ‰ä»»ä½•æ¶ˆè´¹è€…æˆå‘˜ï¼Œä½†å…ƒæ•°æ®ä¿¡æ¯ä¹Ÿæ²¡æœ‰è¢«åˆ é™¤ï¼ŒçŸ¥é“æ‰€æœ‰æ¶ˆè´¹è€…å¯¹åº”çš„æ¶ˆè´¹åç§»é‡å…ƒæ•°æ®ä¿¡æ¯è¿‡æœŸã€‚
- ReplicaManagerï¼šGroupMetadataManageréœ€è¦æŠŠæ¶ˆè´¹ç»„å…ƒæ•°æ®ä¿¡æ¯ä»¥åŠæ¶ˆè´¹è€…æäº¤çš„å·²æ¶ˆè´¹åç§»é‡ä¿¡æ¯å†™å…¥ Kafka å†…éƒ¨ä¸»é¢˜ä¸­ï¼Œå¯¹å†…éƒ¨ä¸»é¢˜çš„æ“ä½œä¸å¯¹å…¶ä»–ä¸»é¢˜çš„æ“ä½œä¸€æ ·ï¼Œå…ˆé€šè¿‡ ReplicaManager å°†æ¶ˆæ¯å†™å…¥ leader å‰¯æœ¬ï¼ŒReplicaManager è´Ÿè´£ leader å‰¯æœ¬ä¸å…¶ä»–å‰¯æœ¬çš„ç®¡ç†ã€‚
- DelayedJoinï¼šå»¶è¿Ÿæ“ä½œç±»ï¼Œç”¨äºç›‘è§†å¤„ç†æ‰€æœ‰æ¶ˆè´¹ç»„æˆå‘˜ä¸ç»„åè°ƒå™¨ä¹‹é—´çš„å¿ƒè·³è¶…æ—¶
- GroupConfigï¼šå®šä¹‰äº†ç»„æˆå‘˜ä¸ç»„åè°ƒå™¨ä¹‹é—´sessionè¶…æ—¶æ—¶é—´é…ç½®

##### æ¶ˆè´¹è€…åè°ƒå™¨å’Œç»„åè°ƒå™¨çš„äº¤äº’ -(æ ¸å¿ƒ rebalance)

https://chrzaszcz.dev/2019/06/kafka-rebalancing/

https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol

https://www.slideshare.net/ConfluentInc/the-silver-bullet-for-endless-rebalancing

![](/docs/docs_image/software/buildingblock/kafka/kafka_rebalance.png)

(1) å¿ƒè·³

æ¶ˆè´¹è€…åè°ƒå™¨é€šè¿‡å’Œç»„åè°ƒå™¨å‘é€å¿ƒè·³æ¥ç»´æŒå®ƒä»¬å’Œç¾¤ç»„çš„ä»å±å…³ç³»ä»¥åŠå®ƒä»¬å¯¹åˆ†åŒºçš„æ‰€æœ‰æƒå…³ç³»ã€‚åªè¦æ¶ˆè´¹è€…ä»¥æ­£å¸¸çš„æ—¶é—´é—´éš”å‘é€å¿ƒè·³ï¼Œå°±è¢«è®¤ä¸ºæ˜¯æ´»è·ƒçš„ï¼Œè¯´æ˜å®ƒè¿˜åœ¨è¯»å–åˆ†åŒºé‡Œçš„æ¶ˆæ¯ã€‚æ¶ˆè´¹è€…ä¼šåœ¨è½®è¯¢è·å–æ¶ˆæ¯æˆ–æäº¤åç§»é‡æ—¶å‘é€å¿ƒè·³ã€‚

å¦‚æœæ¶ˆè´¹è€…åœæ­¢å‘é€å¿ƒè·³çš„æ—¶é—´è¶³å¤Ÿé•¿ï¼Œä¼šè¯å°±ä¼šè¿‡æœŸï¼Œç»„åè°ƒå™¨è®¤ä¸ºå®ƒå·²ç»æ­»äº¡ï¼Œå°±ä¼šè§¦å‘ä¸€æ¬¡å†å‡è¡¡ã€‚

åœ¨ 0.10 ç‰ˆæœ¬é‡Œï¼Œå¿ƒè·³ä»»åŠ¡ç”±ä¸€ä¸ªç‹¬ç«‹çš„å¿ƒè·³çº¿ç¨‹æ¥æ‰§è¡Œï¼Œå¯ä»¥åœ¨è½®è¯¢è·å–æ¶ˆæ¯çš„ç©ºæ¡£å‘é€å¿ƒè·³ã€‚è¿™æ ·ä¸€æ¥ï¼Œå‘é€å¿ƒè·³çš„é¢‘ç‡ï¼ˆä¹Ÿå°±æ˜¯ç»„åè°ƒå™¨ç¾¤æ£€æµ‹æ¶ˆè´¹è€…è¿è¡ŒçŠ¶æ€çš„æ—¶é—´ï¼‰ä¸æ¶ˆæ¯è½®è¯¢çš„é¢‘ç‡ï¼ˆç”±å¤„ç†æ¶ˆæ¯æ‰€èŠ±è´¹çš„æ—¶é—´æ¥ç¡®å®šï¼‰ä¹‹é—´å°±æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚åœ¨0.10 ç‰ˆæœ¬çš„ Kafka é‡Œï¼Œå¯ä»¥æŒ‡å®šæ¶ˆè´¹è€…åœ¨ç¦»å¼€ç¾¤ç»„å¹¶è§¦å‘å†å‡è¡¡ä¹‹å‰å¯ä»¥æœ‰å¤šé•¿æ—¶é—´ä¸è¿›è¡Œæ¶ˆæ¯è½®è¯¢ï¼Œè¿™æ ·å¯ä»¥é¿å…å‡ºç°æ´»é”ï¼ˆlivelockï¼‰ï¼Œæ¯”å¦‚æœ‰æ—¶å€™åº”ç”¨ç¨‹åºå¹¶æ²¡æœ‰å´©æºƒï¼Œåªæ˜¯ç”±äºæŸäº›åŸå› å¯¼è‡´æ— æ³•æ­£å¸¸è¿è¡Œã€‚è¿™ä¸ªé…ç½®ä¸ session.timeout.ms æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œåè€…ç”¨äºæ§åˆ¶æ£€æµ‹æ¶ˆè´¹è€…å‘ç”Ÿå´©æºƒçš„æ—¶é—´å’Œåœæ­¢å‘é€å¿ƒè·³çš„æ—¶é—´ã€‚

(2) åˆ†åŒºå†å‡è¡¡

å‘ç”Ÿåˆ†åŒºå†å‡è¡¡çš„3ç§æƒ…å†µï¼š

- ä¸€ä¸ªæ–°çš„æ¶ˆè´¹è€…åŠ å…¥ç¾¤ç»„æ—¶ï¼Œå®ƒè¯»å–çš„æ˜¯åŸæœ¬ç”±å…¶ä»–æ¶ˆè´¹è€…è¯»å–çš„æ¶ˆæ¯ã€‚
- å½“ä¸€ä¸ªæ¶ˆè´¹è€…è¢«å…³é—­æˆ–å‘ç”Ÿå´©æºƒæ—¶ï¼Œå®ƒå°±ç¦»å¼€ç¾¤ç»„ï¼ŒåŸæœ¬ç”±å®ƒè¯»å–çš„åˆ†åŒºå°†ç”±ç¾¤ç»„é‡Œçš„å…¶ä»–æ¶ˆè´¹è€…æ¥è¯»å–ã€‚å¦‚æœä¸€ä¸ªæ¶ˆè´¹è€…ä¸»åŠ¨ç¦»å¼€æ¶ˆè´¹ç»„ï¼Œæ¶ˆè´¹è€…ä¼šé€šçŸ¥ç»„åè°ƒå™¨å®ƒå°†è¦ç¦»å¼€ç¾¤ç»„ï¼Œç»„åè°ƒå™¨ä¼šç«‹å³è§¦å‘ä¸€æ¬¡å†å‡è¡¡ï¼Œå°½é‡é™ä½å¤„ç†åœé¡¿ã€‚å¦‚æœä¸€ä¸ªæ¶ˆè´¹è€…æ„å¤–å‘ç”Ÿå´©æºƒï¼Œæ²¡æœ‰é€šçŸ¥ç»„åè°ƒå™¨å°±åœæ­¢è¯»å–æ¶ˆæ¯ï¼Œç»„åè°ƒå™¨ä¼šç­‰å¾…å‡ ç§’é’Ÿï¼Œç¡®è®¤å®ƒæ­»äº¡äº†æ‰ä¼šè§¦å‘å†å‡è¡¡ã€‚åœ¨è¿™å‡ ç§’é’Ÿæ—¶é—´é‡Œï¼Œæ­»æ‰çš„æ¶ˆè´¹è€…ä¸ä¼šè¯»å–åˆ†åŒºé‡Œçš„æ¶ˆæ¯ã€‚
- åœ¨ä¸»é¢˜å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ¯”å¦‚ç®¡ç†å‘˜æ·»åŠ äº†æ–°çš„åˆ†åŒºï¼Œä¼šå‘ç”Ÿåˆ†åŒºé‡åˆ†é…ã€‚

åˆ†åŒºçš„æ‰€æœ‰æƒä»ä¸€ä¸ªæ¶ˆè´¹è€…è½¬ç§»åˆ°å¦ä¸€ä¸ªæ¶ˆè´¹è€…ï¼Œè¿™æ ·çš„è¡Œä¸ºè¢«ç§°ä¸ºåˆ†åŒºå†å‡è¡¡ã€‚å†å‡è¡¡éå¸¸é‡è¦ï¼Œå®ƒä¸ºæ¶ˆè´¹è€…ç¾¤ç»„å¸¦æ¥äº†é«˜å¯ç”¨æ€§å’Œä¼¸ç¼©æ€§ï¼ˆæˆ‘ä»¬å¯ä»¥æ”¾å¿ƒåœ°æ·»åŠ æˆ–ç§»é™¤æ¶ˆè´¹è€…ï¼‰ï¼Œä¸è¿‡åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¹¶ä¸å¸Œæœ›å‘ç”Ÿè¿™æ ·çš„è¡Œä¸ºã€‚åœ¨å†å‡è¡¡æœŸé—´ï¼Œæ¶ˆè´¹è€…æ— æ³•è¯»å–æ¶ˆæ¯ï¼Œé€ æˆæ•´ä¸ªç¾¤ç»„ä¸€å°æ®µæ—¶é—´çš„ä¸å¯ç”¨ã€‚å¦å¤–ï¼Œå½“åˆ†åŒºè¢«é‡æ–°åˆ†é…ç»™å¦ä¸€ä¸ªæ¶ˆè´¹è€…æ—¶ï¼Œæ¶ˆè´¹è€…å½“å‰çš„è¯»å–çŠ¶æ€ä¼šä¸¢å¤±ï¼Œå®ƒæœ‰å¯èƒ½è¿˜éœ€è¦å»åˆ·æ–°ç¼“å­˜ï¼Œåœ¨å®ƒé‡æ–°æ¢å¤çŠ¶æ€ä¹‹å‰ä¼šæ‹–æ…¢åº”ç”¨ç¨‹åºã€‚

(3) leader æ¶ˆè´¹è€…åˆ†é…åˆ†åŒºçš„ç­–ç•¥

å½“æ¶ˆè´¹è€…è¦åŠ å…¥ç¾¤ç»„æ—¶ï¼Œå®ƒä¼šå‘ç¾¤ç»„åè°ƒå™¨å‘é€ä¸€ä¸ª JoinGroup è¯·æ±‚ã€‚ç¬¬ä¸€ä¸ªåŠ å…¥ç¾¤ç»„çš„æ¶ˆè´¹è€…å°†æˆä¸ºleaderæ¶ˆè´¹è€…ã€‚leaderæ¶ˆè´¹è€…ä»ç»„åè°ƒå™¨é‚£é‡Œè·å¾—ç¾¤ç»„çš„æˆå‘˜åˆ—è¡¨ï¼ˆåˆ—è¡¨ä¸­åŒ…å«äº†æ‰€æœ‰æœ€è¿‘å‘é€è¿‡å¿ƒè·³çš„æ¶ˆè´¹è€…ï¼Œå®ƒä»¬è¢«è®¤ä¸ºæ˜¯æ´»è·ƒçš„ï¼‰ï¼Œå¹¶è´Ÿè´£ç»™æ¯ä¸€ä¸ªæ¶ˆè´¹è€…åˆ†é…åˆ†åŒºã€‚

æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹è€…åè°ƒå™¨åœ¨å‘ç»„åè°ƒå™¨è¯·æ±‚åŠ å…¥ç»„æ—¶ï¼Œéƒ½ä¼šæŠŠè‡ªå·±æ”¯æŒçš„åˆ†åŒºåˆ†é…ç­–ç•¥æŠ¥å‘Šç»™ç»„åè°ƒå™¨(è½®è¯¢æˆ–è€…æ˜¯æŒ‰è·¨åº¦åˆ†é…æˆ–è€…å…¶ä»–)ï¼Œç»„åè°ƒå™¨é€‰å‡ºè¯¥æ¶ˆè´¹ç»„ä¸‹æ‰€æœ‰æ¶ˆè´¹è€…éƒ½æ”¯æŒçš„çš„åˆ†åŒºåˆ†é…ç­–ç•¥å‘é€ç»™leaderæ¶ˆè´¹è€…ï¼Œleaderæ¶ˆè´¹è€…æ ¹æ®è¿™ä¸ªåˆ†åŒºåˆ†é…ç­–ç•¥è¿›è¡Œåˆ†é…ã€‚

å®Œæ¯•ä¹‹åï¼Œleaderæ¶ˆè´¹è€…æŠŠåˆ†é…æƒ…å†µåˆ—è¡¨å‘é€ç»™ç»„åè°ƒå™¨ï¼Œæ¶ˆè´¹è€…åè°ƒå™¨å†æŠŠè¿™äº›ä¿¡æ¯å‘é€ç»™æ‰€æœ‰æ¶ˆè´¹è€…ã€‚æ¯ä¸ªæ¶ˆè´¹è€…åªèƒ½çœ‹åˆ°è‡ªå·±çš„åˆ†é…ä¿¡æ¯ï¼Œåªæœ‰leaderæ¶ˆè´¹è€…çŸ¥é“ç¾¤ç»„é‡Œæ‰€æœ‰æ¶ˆè´¹è€…çš„åˆ†é…ä¿¡æ¯ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šåœ¨æ¯æ¬¡å†å‡è¡¡æ—¶é‡å¤å‘ç”Ÿã€‚

(4) æ¶ˆè´¹è€…å…¥ç»„è¿‡ç¨‹

- æ¶ˆè´¹è€…åˆ›å»ºåï¼Œæ¶ˆè´¹è€…åè°ƒå™¨ä¼šé€‰æ‹©ä¸€ä¸ªè´Ÿè½½è¾ƒå°çš„èŠ‚ç‚¹ï¼Œå‘è¯¥èŠ‚ç‚¹å‘é€å¯»æ‰¾ç»„åè°ƒå™¨çš„è¯·æ±‚

-  KafkaApis å¤„ç†è¯·æ±‚ï¼Œè°ƒç”¨è¿”å›ç»„åè°ƒå™¨æ‰€åœ¨çš„èŠ‚ç‚¹ï¼Œè¿‡ç¨‹å¦‚ä¸‹ï¼š

  ```
  def partitionFor(group: String): Int = groupManager.partitionFor(group)
  https://github.com/a0x8o/kafka/blob/master/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala
  =>
  groupIdçš„å“ˆå¸Œå€¼çš„ç»å¯¹å€¼å¯¹ __consumer_offset è¿™ä¸ªtopicçš„partitionçš„ä¸ªæ•°ï¼ˆé»˜è®¤50ï¼‰å–ä½™ å¾—åˆ°ä¸€ä¸ªåˆ†åŒºçš„id
  def partitionFor(groupId: String): Int = Utils.abs(groupId.hashCode) % groupMetadataTopicPartitionCount
  https://github.com/a0x8o/kafka/blob/master/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala
  =>
  è¯¥åˆ†åŒºçš„leaderå‰¯æœ¬æ‰€åœ¨çš„èŠ‚ç‚¹å°±æ˜¯ç»„åè°ƒå™¨æ‰€åœ¨çš„èŠ‚ç‚¹ï¼Œè¯¥æ¶ˆè´¹ç»„çš„å…ƒæ•°æ®ä¿¡æ¯ä»¥åŠæ¶ˆè´¹è€…æ¶ˆè´¹åç§»é‡ä¿¡æ¯éƒ½ä¼šå†™åˆ°__consumer_offsetçš„è¿™ä¸ªåˆ†åŒºä¸­
  ```

  

- æ‰¾åˆ°ç»„åè°ƒå™¨åï¼Œæ¶ˆè´¹è€…åè°ƒå™¨ç”³è¯·åŠ å…¥æ¶ˆè´¹ç»„ï¼Œå‘é€ JoinGroupRequestè¯·æ±‚

- KafkaApis è°ƒç”¨ handleJoinGroup() æ–¹æ³•å¤„ç†è¯·æ±‚ 

  - æŠŠæ¶ˆè´¹è€…æ³¨å†Œåˆ°æ¶ˆè´¹ç»„ä¸­
  - æŠŠæ¶ˆè´¹è€…çš„clientIdä¸ä¸€ä¸ªUUIDå€¼ç”Ÿæˆä¸€ä¸ªmemberIdåˆ†é…ç»™æ¶ˆè´¹è€…
  - æ„é€ å™¨è¯¥æ¶ˆè´¹è€…çš„MemberMetadataä¿¡æ¯
  - æŠŠè¯¥æ¶ˆè´¹è€…çš„MemberMetadataä¿¡æ¯æ³¨å†Œåˆ°GroupMetadataä¸­
  - ç¬¬ä¸€ä¸ªåŠ å…¥ç»„çš„æ¶ˆè´¹è€…å°†æˆä¸ºleader

- æŠŠå¤„ç†JoinGroupRequestè¯·æ±‚çš„ç»“æœè¿”å›ç»™æ¶ˆè´¹è€…

- åŠ å…¥ç»„æˆåŠŸåï¼Œè¿›è¡Œåˆ†åŒºå†å‡è¡¡



#### kafka idempotent åŸç†

http://matt33.com/2018/10/24/kafka-idempotent/

#### kafka Transaction åŸç† Transaction Coordinator and Transaction Log

https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging

The components introduced with the  transactions API in Kafka 0.11.0 are the Transaction Coordinator and the Transaction Log on the right hand side of the diagram above.

The transaction coordinator is a  module running inside every Kafka broker. The transaction log is an  internal kafka topic. Each coordinator owns some subset of the  partitions in the transaction log, ie. the partitions for which its  broker is the leader.

Every transactional.id is mapped to a specific partition of the transaction log through a simple hashing  function. This means that exactly one coordinator owns a given  transactional.id.

This way, we leverage Kafkaâ€™s rock  solid replication protocol and leader election processes to ensure that  the transaction coordinator is always available and all transaction  state is stored durably. 

It is worth noting that the transaction log just stores the latest *state* of a transaction and not the actual messages in the transaction. The  messages are stored solely in the actual topic-partitions. The  transaction could be in various states like â€œOngoing,â€ â€œPrepare commit,â€ and â€œCompleted.â€ It is this state and associated metadata that is  stored in the transaction log.

![](/docs/docs_image/software/buildingblock/kafka/kafka_transaction_coordinator.png)

**data flow**

A: the producer and transaction coordinator interaction

When executing transactions, the producer makes requests to the transaction coordinator at the following points:

1. The  initTransactions API registers a transactional.id with the coordinator.  At this point, the coordinator closes any pending transactions with that transactional.id and bumps the epoch to fence out zombies. This happens only once per producer session.
2. When the  producer is about to send data to a partition for the first time in a  transaction, the partition is registered with the coordinator first.
3. When the application calls *commitTransaction* or *abortTransaction,* a request is sent to the coordinator to begin the two phase commit protocol.

B: the coordinator and transaction log interaction

As the transaction progresses, the  producer sends the requests above to update the state of the transaction on the coordinator. The transaction coordinator keeps the state of each transaction it owns in memory, and also writes that state to the  transaction log (which is replicated three ways and hence is durable). 

The transaction coordinator is the  only component to read and write from the transaction log. If a given  broker fails, a new coordinator is elected as the leader for the  transaction log partitions the dead broker owned, and it reads the  messages from the incoming partitions to rebuild its in-memory state for the transactions in those partitions.

C: the producer writing data to target topic-partitions

After registering new partitions in a transaction with the coordinator, the producer sends data to the actual partitions as normal. This is exactly the same *producer.send* flow, but with some extra validation to ensure that the producer isnâ€™t fenced.

D: the coordinator to topic-partition interaction

After the producer initiates a commit (or an abort), the coordinator begins the two phase commit protocol. 

In the first phase, the coordinator  updates its internal state to â€œprepare_commitâ€ and updates this state in the transaction log. Once this is done the transaction is guaranteed to be committed no matter what. 

The coordinator then begins phase 2, where it writes *transaction commit markers* to the topic-partitions which are part of the transaction. 

These *transaction markers* are not exposed to applications, but are used by consumers in *read_committed* mode to filter out messages from aborted transactions and to not return messages which are part of open transactions (i.e., those which are in  the log but donâ€™t have a *transaction marker* associated with them).

Once the markers are written, the  transaction coordinator marks the transaction as â€œcompleteâ€ and the  producer can start the next transaction.

**Performance of the transactional producer**

Letâ€™s turn our attention to how transactions perform. 

First, transactions cause only moderate write amplification. The additional writes are due to: 

1. For each  transaction, we have had additional RPCs to register the partitions with the coordinator. These are batched, so we have fewer RPCs than there  are partitions in the transaction.
2. When  completing a transaction, one transaction marker has to be written to  each partition participating in the transaction. Again, the transaction  coordinator batches all markers bound for the same broker in a single  RPC, so we save the RPC overhead there. But we cannot avoid one  additional write to each partition in the transaction.
3. Finally,  we write state changes to the transaction log. This includes a write for each batch of partitions added to the transaction, the â€œprepare_commitâ€ state, and the â€œcomplete_commitâ€ state. 

As we can see the overhead is independent of the *number* of messages written as part of a transaction. So the key to having  higher throughput is to include a larger number of messages per  transaction.

In practice, for a producer producing 1KB records at maximum throughput, committing messages every 100ms  results in only a 3% degradation in throughput. Smaller messages or  shorter transaction commit intervals would result in more severe  degradation. 

The main tradeoff when increasing the transaction duration is that it increases end-to-end latency. Recall  that a consumer reading transactional messages will not deliver messages which are part of open transactions. So the longer the interval between commits, the longer consuming applications will have to wait,  increasing the end-to-end latency.

**Performance of the transactional consumer**

The transactional consumer is much simpler than the producer, since all it needs to do is:

1. Filter out messages belonging to aborted transactions. 
2. Not return transactional messages which are part of open transactions.

As such, the transactional consumer shows no degradation in throughput when reading transactional messages in *read_committed* mode. The main reason for this is that we preserve zero copy reads when reading transactional messages. 

Further, the consumer does not need  to any buffering to wait for transactions to complete. Instead, the  broker does not allow it to advance to offsets which include open  transactions.



```
[main] [org.apache.kafka.clients.producer.internals.TransactionManager : [Producer clientId=producer-1, transactionalId=TID-PARTITION-0] ProducerId set to -1 with epoch -1
[ad | producer-1] [org.apache.kafka.clients.Metadata : Cluster ID: uEekh0baSnKon5ENwtY9dg
[ad | producer-1] [org.apache.kafka.clients.producer.internals.TransactionManager : [Producer clientId=producer-1, transactionalId=TID-PARTITION-0] ProducerId set to 16871 with epoch 85

```

ä¹‹å‰ä¸€ç›´å›°æƒ‘äºè¿™ä¸ª -1 -1ï¼Œä¸è¿‡å› ä¸ºè¿™ä¸‰æ¡logå…¨éƒ¨æ˜¯org.apache.kafkaçš„ï¼Œæ‰€ä»¥åˆšå¼€å§‹æŠ±ç€å®Œå…¨ä¿¡ä»»kafkaçš„æƒ³æ³•å°±å…ˆæ”¾ä¸€è¾¹äº†ï¼Œåæ¥å¿ä¸ä½å¤§æ¦‚debugè¿›å»ç…äº†ä¸‹ï¼Œç¡®è®¤æ˜¯kafkaæ­£å¸¸çš„è®¾è®¡ï¼Œç¬¬ä¸€æ¡logç›´æ¥å°±èƒ½debugåˆ°ï¼Œstep into initTransactionså¾ˆå®¹æ˜“çœ‹åˆ°ï¼Œæ‰€ä»¥æ„æ€æ˜¯é»˜è®¤å°±ä¼šå…ˆåˆå§‹åŒ–ä¸€ä¸ª-1 -1ï¼Œæˆ‘ä¼°è®¡æ˜¯å…ˆç»™ä¸ªè´Ÿå€¼çš„epochï¼Œç­‰åˆ°å®Œå…¨æ³¨å†Œå¥½æ‰ç»™åé¢é‚£ä¸ªæ­£æ•°çš„epochï¼Œæ˜¯åˆç†çš„ï¼Œä¸ç„¶è¿˜æ²¡æœ‰æ³¨å†Œå¥½äº‹åŠ¡å‹çš„producerï¼Œç›´æ¥ç»™ä¸€ä¸ªåˆæ³•çš„epochï¼Œä¼šå½±å“åˆ°ç°åœ¨æ­£å¸¸å·¥ä½œçš„å…¶ä»–producerï¼ˆæ¯”å¦‚ä¸‡ä¸€å› ä¸ºé—®é¢˜è¿™ä¸ªæ–°çš„produceråˆå§‹åŒ–å¤±è´¥ä¹Ÿä¸ä¼šå½±å“åˆ°/zombie fenceä½¿ç”¨ç›¸åŒTransaction.idçš„å…¶ä»–producerï¼‰

#### NIO Selector

https://stackoverflow.com/questions/46185430/kafka-source-understanding-the-semantics-of-selector-poll

#### Log Compaction 
https://kafka.apache.org/22/documentation/#compaction
KafkaæŠ€æœ¯å†…å¹•-æ—¥å¿—å‹ç¼© https://segmentfault.com/a/1190000005312891

Note however that there cannot be more consumer instances(task) in a consumer group than partitions. 
https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client

#### Kafka Fetch Sessionå‰–æ
https://stackoverflow.com/questions/54823733/kafka-invalid-fetch-session-epoch
https://www.cnblogs.com/smartloli/p/14352489.html

å¯¹äºå®¢æˆ·ç«¯æ¥è¯´ï¼Œä»€ä¹ˆæ—¶å€™ä¸€ä¸ªåˆ†åŒºä¼šè¢«åŒ…å«åˆ°å¢é‡çš„æ‹‰å–è¯·æ±‚ä¸­ï¼š

Clienté€šçŸ¥Brokerï¼Œåˆ†åŒºçš„maxBytesï¼ŒfetchOffsetï¼ŒLogStartOffsetæ”¹å˜äº†ï¼›
åˆ†åŒºåœ¨ä¹‹å‰çš„å¢é‡æ‹‰å–ä¼šè¯ä¸­ä¸å­˜åœ¨ï¼ŒClientæƒ³è¦å¢åŠ è¿™ä¸ªåˆ†åŒºï¼Œä»è€Œæ¥æ‹‰å–æ–°çš„åˆ†åŒºï¼›
åˆ†åŒºåœ¨å¢é‡æ‹‰å–ä¼šè¯ä¸­ï¼ŒClientè¦åˆ é™¤ã€‚
å¯¹äºæœåŠ¡ç«¯æ¥è¯´ï¼Œå¢é‡åˆ†åŒºåŒ…å«åˆ°å¢é‡çš„æ‹‰å–å“åº”ä¸­ï¼š

Brokeré€šçŸ¥Clientåˆ†åŒºçš„HighWaterMarkæˆ–è€…brokerLogStartOffsetæ”¹å˜äº†ï¼›
åˆ†åŒºæœ‰æ–°çš„æ•°æ®

#### retention / delete

log.retention.check.interval.ms:default 5 minutes. So the broker log-segments are checked every 5 minutes to see if they can be deleted according to the retention policies.

topic1 configuration had retention policy set (retention.ms=60000), so if there was at least one existing message in an active segment of topic1, that segment would get closed and deleted if it was idle for long enough. Since log.retention.check.interval.ms is broker configuration, it's not affected by changes on the topic. Also retention.ms has to pass after the last message is produced to the segment. So after the last message is produced to that segment, segment will be deleted in not less than retention.ms milliseconds and not more than retention.ms+log.retention.check.interval.ms.

So the "segment of just 35 bytes, which contained just one message, was deleted after the minute (maybe a little more)" happened because retention check by chance happened almost immediately after the message was produced to that segment. Broker then had just to wait 60 seconds to be sure no new message will be produced to that segment (in which case deletion would't happen) and since there was none, it deleted the segment

https://stackoverflow.com/questions/41048041/kafka-deletes-segments-even-before-segment-size-is-reached


### 4.5 Nodes expansion

https://kafka.apache.org/documentation/#basic_ops_cluster_expansion

æ³¨æ„ï¼Œkafkaæ˜¯ä¸å¯ä»¥åŠ¨æ€æ‰©å¢çš„ï¼Œä¸€æ—¦é…ç½®å¥½å¹¶ä¸”å¼€å¯nä¸ªèŠ‚ç‚¹ï¼Œç¬¬ä¸€ä¸ªconsumeræ¥å…¥çš„æ—¶å€™å°±ä¼šåˆ›å»º50ä¸ªpartitionçš„`__consumer_offsets`å¹¶ä¸”åœ¨nä¸ªèŠ‚ç‚¹ä¸Šå¹³å‡åˆ†é…ï¼Œç¬¬ä¸€ä¸ªproduceræ¥å…¥çš„æ—¶å€™ä¼šåˆ›å»º50ä¸ª`__transaction_state`å¹¶ä¸”åœ¨nä¸ªèŠ‚ç‚¹ä¸Šå¹³å‡åˆ†é…ï¼Œåˆ›å»ºçš„å¤–éƒ¨topicä¹ŸåŒæ ·å¹³å‡åˆ†é…åœ¨nä¸ªèŠ‚ç‚¹ä¸Šï¼Œå¦‚æœæ‰©å¢nodeåˆ°n+1ï¼Œå¹¶ä¸ä¼šè‡ªåŠ¨çš„é‡æ–°åˆ†é…ï¼Œéœ€è¦æ‰‹åŠ¨migrationï¼›

å¦å¤–næœ€å¥½æ˜¯ç­‰äºreplica factorçš„è®¾ç½®ï¼Œä½äºreplica factoræ— æ³•æ­£å¸¸å¯åŠ¨å·¥ä½œï¼Œé«˜äºreplica factoråˆ™ä¼šé€ æˆèŠ‚ç‚¹æµªè´¹ï¼Œæ¯”å¦‚

replica factor=2ï¼Œå¯åŠ¨3ä¸ªnodeï¼Œé‚£ä¹ˆå°±å‡ºç°æ¯”å¦‚ `__consumer_offsets_49`çš„replicaåˆ†å¸ƒåœ¨node 1å’Œ2ä¸Šï¼Œæ‰€ä»¥min.isr=1çš„æƒ…å†µä¸‹åªæ”¯æŒnode1å’Œ2åªèƒ½æŒ‚æ‰ä¸€ä¸ªï¼ŒåŒç†å…¶ä»–çš„åˆ†å¸ƒåœ¨node1å’Œnode3ï¼Œnode2å’Œnode3ä¸Šï¼Œæ‰€ä»¥æ€»çš„æ¥è¯´ï¼Œåªèƒ½æŒ‚æ‰3ä¸ªèŠ‚ç‚¹çš„ä¸€ä¸ªï¼Œå‡è®¾æŒ‚æ‰ä¸¤ä¸ªnode1å’Œ2ï¼Œnode3ä¸Šæœ‰replicaçš„topicæ²¡é—®é¢˜ï¼Œä½†æ˜¯`__consumer_offsets_49`æ˜¾ç„¶å°±æŒ‚äº†


## Performance metric monitoring
[Benchmarking Apache Kafka: 2 Million Writes Per Second (On Three Cheap Machines)](https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines)

[Apache KafkaÂ® Performance](https://developer.confluent.io/learn/kafka-performance/)

[Monitoring Kafka performance metrics](https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/)

[scripting approach to run performance tests](https://github.com/gkoenig/kafka-benchmarking)

## 5. Troubleshooting

### Caused by: org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor is below 1 or larger than the number of available brokers.
é…ç½®
```
offsets.topic.replication.factor=3
min.insync.replicas=1
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=1
default.replication.factor=3
```
ä¸€ä¸ªèŠ‚ç‚¹æŒ‚äº†ï¼Œç„¶åå¯åŠ¨kafka clientï¼ˆå¹¶ä¸”æ­¤æ—¶topicè¿˜æœªåˆ›å»ºï¼‰ï¼ŒæŠ¥é”™
```
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2022-06-27 06:20:09.825 [31mERROR[m [35m2506GG[m [main] [36mo.s.b.SpringApplication[m : Application run failed

org.apache.kafka.common.KafkaException: Unexpected error fetching metadata for topic T-TEST
	at org.apache.kafka.clients.consumer.internals.Fetcher.getTopicMetadata(Fetcher.java:327)
	at org.apache.kafka.clients.consumer.KafkaConsumer.partitionsFor(KafkaConsumer.java:1803)
	at org.apache.kafka.clients.consumer.KafkaConsumer.partitionsFor(KafkaConsumer.java:1771)
	....
	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49)
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:108)
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:58)
	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:88)
Caused by: org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor is below 1 or larger than the number of available brokers.
```
å¾ˆæ­£å¸¸ï¼Œå› ä¸ºåˆ›å»ºtopicéœ€è¦æ´»ç€çš„èŠ‚ç‚¹>=replication.factor,å¯¹åº”è¿™æ®µä»£ç æ˜¯åœ¨è¯·æ±‚ getTopicMetadata è§¦å‘kafkaæœåŠ¡ç«¯åˆ›å»ºtopic
```
@Override
    public List<PartitionInfo> partitionsFor(String topic, Duration timeout) {
        acquireAndEnsureOpen();
        try {
            Cluster cluster = this.metadata.fetch();
            List<PartitionInfo> parts = cluster.partitionsForTopic(topic);
            if (!parts.isEmpty())
                return parts;

            Timer timer = time.timer(timeout);
            Map<String, List<PartitionInfo>> topicMetadata = fetcher.getTopicMetadata(
                    new MetadataRequest.Builder(Collections.singletonList(topic), true), timer);
            return topicMetadata.get(topic);
        } finally {
            release();
        }
    }

auto.create.topics.enable

MetadataRequest auto create kafka topic InvalidReplicationFactorException

https://cwiki.apache.org/confluence/display/KAFKA/KIP-361%3A+Add+Consumer+Configuration+to+Disable+Auto+Topic+Creation

```
ç„¶åè¿˜æœ‰ä¸€ä¸ªkafka clientç¨‹åºä¹Ÿæ˜¯ç±»ä¼¼é—®é¢˜ï¼Œåªä¸è¿‡è¿™ä¸ªç¨‹åºä½¿ç”¨äº†å¤šä¸ªtopic: Topic1 Topic2ï¼ŒTopic1äº‹å…ˆå·²ç»åˆ›å»ºäº†æ‰€ä»¥æ²¡æœ‰é—®é¢˜ï¼Œåœ¨è¯·æ±‚ Topic2çš„ metadataçš„æ—¶å€™ï¼ˆendoffsetï¼‰ä¹Ÿæ˜¯è‡ªåŠ¨åˆ›å»ºå¤±è´¥äº§ç”Ÿç±»ä¼¼æŠ¥é”™

### Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)

```
[2019-05-07 21:45:48,648] ERROR [KafkaServer id=0] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
```

org.apache.kafka.common.KafkaException: Failed to acquire lock on file .lock in /tmp/kafka-logs. A Kafka instance in another process or thread is using this directory.

### UNKOWN_PRDOCUER_ID

got error produce response with correlation id on topic-partition UNKOWN_PRDOCUER_ID

https://stackoverflow.com/questions/51036351/kafka-unknown-producer-id-exception

åŸºæœ¬ä¸Šå°±æ˜¯å› ä¸ºè¿‡æœŸäº†

```
log.retention.hours=720
transactional.id.expiration.ms=2073600000

```

### kafka segment.bytes different for each topic

ä¿®æ”¹äº†å…¶ä¸­ä¸€ä¸ªbrokerèŠ‚ç‚¹çš„configï¼Œå¿˜è®°åŒæ­¥åˆ°æ‰€æœ‰çš„èŠ‚ç‚¹

### Kafka Client Compatibility

https://spring.io/projects/spring-kafka
org.springframework.kafka org.apache.kafka
https://www.cnblogs.com/wangb0402/p/6187796.html

### Replica factor

```
############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"                                                                        
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.          
offsets.topic.num.partitions = 50 ï¼ˆdefaultï¼‰
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2

kafka-topics.sh -describe --bootstrap-server ip:9092 --topic __consumer_offsets
kafka-topics.sh -describe --bootstrap-server ip:9092 --topic __transaction_state
```

### æ— æ³•åˆ é™¤topic



```
bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic T-*
å¯ä»¥é€šè¿‡zookeeperåˆ é™¤
bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic T-.*
```



### åˆ é™¤topicåæ— æ³•åˆ›å»ºï¼Œæç¤ºå·²å­˜åœ¨ï¼Œä½†æ˜¯æ‰¾ä¸åˆ° org.apache.kafka.common.errors.TopicExistsException
```

æ˜¯zookeeperè·Ÿkafkaæ•°æ®ä¸ä¸€è‡´ï¼Œ

./bin/zookeeper-shell.sh :2181 get /brokers/topics

æˆ–è€…

bin/zookeeper-shell.sh localhost:2181

â€‹	ls /brokers/topics

â€‹	ls /admin/delete_topics

ä¼šçœ‹åˆ°å¾ˆå¤štopicå­˜åœ¨è¿™é‡Œ

ç›´æ¥é€šè¿‡å‘½ä»¤deleteï¼Œæˆ–è€…æ›´ç®€å•çš„é€šè¿‡åˆ é™¤ ../zookeeper/logs/version-2/ å’Œ ../zookeeper/zkdata/version-2/

rm -rf ../zookeeper/zkdata/version-2/* 
rm -rf ../zookeeper/logs/version-2/*
rm -rf kafka-logs/*
rm -rf logs/*
```
### æ—¶é’Ÿæ¼‚ç§»
clientç«¯æ—¥å¿—
2022-03-14 09:00:56.202 ^[[32m INFO^[[m ^[[35m16686GG^[[m [JOB-MANAGER] ^[[36mordinator$FindCoordinatorResponseHandler^[[m : [Consumer clientId=consumer-2, groupId=TEST-SZL] Discovered group coordinator HOST2:9092 (id: 2147483646 rack: null)
ä½†æ˜¯åœ¨æœºå™¨HOST2ä¸Šæ²¡æœ‰æ‰¾åˆ°å¯¹åº”æ—¶é—´æ®µæ—¥å¿—ï¼Œæœ€åå‘ç° HOST2æ¯”HOST1å’Œ3æ—¶é’Ÿå¿«äº†å‡ åˆ†é’Ÿï¼Œå¯¹åº”kafkaæ—¥å¿—åˆšå¥½æ˜¯å››åˆ†é’Ÿ[2022-03-14 09:04:23,689]
[2022-03-14 09:04:23,689] INFO [GroupCoordinator 1]: 
	Preparing to rebalance group TEST-SZL in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member consumer-2-ab88af5d-b206-48fb-a38b-ead5e50ad76e) (kafka.coordinator.group.GroupCoordinator)


### ç½‘ç»œæ•…éšœ / kafkaé›†ç¾¤æœ‰èŠ‚ç‚¹æŒ‚æ‰ï¼ˆä¸æ˜¯æ­£å¸¸åœèŠ‚ç‚¹ï¼Œè€Œæ˜¯brokerèŠ‚ç‚¹æ‰€åœ¨æœåŠ¡å™¨ç½‘ç»œæ–­å¼€æˆ–æš´åŠ›åœæœºï¼‰

#### é€ æˆkafka clientç«¯ç¨‹åºè¯»å– metadata è¶…è¿‡é»˜è®¤ 30s æŠ›é”™

public java.util.Map<TopicPartition,java.lang.Long> endOffsets(java.util.Collection<TopicPartition> partitions)

TimeoutException - if the offset metadata could not be fetched before the amount of time allocated by **request.timeout.ms expires**

å¼‚å¸¸æ—¥å¿—åˆ†æï¼š
```
åœºæ™¯1ï¼š
ç°åœ¨devä¸Š3ä¸ªèŠ‚ç‚¹é…ç½® borker 0 1 2ï¼š
offsets.topic.replication.factor=3
min.insync.isr=2
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2
default.replication.factor=3
æ­£å¸¸æŒ‚æ‰1ä¸ªæ˜¯æ²¡é—®é¢˜çš„ï¼Œä½†æ˜¯å±…ç„¶æŒ‚æ‰2ä¸ªå±…ç„¶éƒ½èƒ½å¯åŠ¨clientç«¯ï¼ˆèƒ½æˆåŠŸjoin groupï¼ŒkafkaæˆåŠŸrebalanceï¼‰ï¼š
è¿™ä¸ªæ¯”è¾ƒè¯¡å¼‚ï¼Œå°±æ˜¯æµ‹è¯•çš„æ—¶å€™å…¶å®åªæœ‰ä¸¤ä¸ªæ´»ç€çš„èŠ‚ç‚¹ï¼Œbroker 0ä¸çŸ¥é“è¢«è°ç”¨rootæ›´æ”¹äº†å‡ ä¸ªkafka-logsæ–‡ä»¶æƒé™ï¼Œé€ æˆborker 0åœäº†ï¼Œ
ï¼Œç„¶åæµ‹è¯•æ–­å¼€broker 2çš„ç½‘ç»œï¼Œåªæœ‰broker 1ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹ï¼Œclientç«¯å±…ç„¶èƒ½å¤Ÿå¯åŠ¨å¹¶ä¸”è®¢é˜…topicï¼Œåªä¸è¿‡æ¢å¤çš„æ—¶å€™ï¼ˆonPartitionAssignå†…éƒ¨è¿›ä¸€æ­¥è®¿é—®kafkaè¯»å–å¿«ç…§æ•°æ®ï¼‰æŠ›é”™ï¼Œæ¯”å¦‚å¯èƒ½æ˜¯æ¢å¤çš„æ—¶å€™åˆ›å»ºä¸´æ—¶consumerï¼ŒkafkaæœåŠ¡ç«¯å‘zookeeperæ³¨å†Œçš„æ—¶å€™å‡ºç°è¶…æ—¶ï¼ˆä¸‹é¢æœ‰zookeeperé—®é¢˜çš„logï¼‰ï¼Œ
2022-03-12 18:33:29.793 [31mERROR[m [35m15732GG[m [TEST-MANAGER] [36mo.a.k.c.c.i.ConsumerCoordinator[m : [Consumer clientId=consumer-2, groupId=TEST-TRADEFRONT-SZL] User provided listener com.lyhistory.core.boot.SimpleWorkBalancer failed on partition assignment

com.lyhistory.core.exception.RecoveryException: Failed Recovery Worker
	at com.lyhistory.core.boot.SimpleWorkBalancer.onPartitionsAssigned(SimpleWorkBalancer.java:54)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:292)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:410)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:344)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:342)
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1226)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1191)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1176)
	at com.lyhistory.core.boot.SimpleWorkerManager.doServe(SimpleWorkerManager.java:96)
	at com.lyhistory.core.boot.AdministrableService.serve(AdministrableService.java:98)
	at com.lyhistory.core.boot.AdministrableService.start(AdministrableService.java:36)
	at com.lyhistory.core.boot.Starter$$Lambda$826/315805187.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.lyhistory.core.exception.RecoveryException: Failed Recovery Worker
	at com.lyhistory.core.boot.SimpleWorkBalancer.getWorkState(SimpleWorkBalancer.java:64)
	at com.lyhistory.core.boot.SimpleWorkBalancer$$Lambda$872/1231621690.apply(Unknown Source)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at com.lyhistory.core.boot.SimpleWorkBalancer.onPartitionsAssigned(SimpleWorkBalancer.java:52)
	... 12 more
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to get offsets by times in 30000ms
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.lyhistory.core.boot.SimpleWorkBalancer.getWorkState(SimpleWorkBalancer.java:62)
	... 22 more
Caused by: org.apache.kafka.common.errors.TimeoutException: Failed to get offsets by times in 30000ms

ä½†æ˜¯æ­£å¸¸ä¸åº”è¯¥èƒ½å¤Ÿè®¢é˜…topicï¼Œåº”è¯¥æŠ›å‡ºé”™è¯¯ç±»ä¼¼ï¼š
1 partitions have leader brokers without a matching listener ï¼ˆå› ä¸ºfactor=2ï¼ŒæŒ‚äº†2å°äº†è‚¯å®šæœ‰èŠ‚ç‚¹çš„leaderä¸åœ¨äº†ï¼‰
æˆ–è€…
insufficient isr (min.isr=2>live broker=1)
ä»è€Œå¯¼è‡´clientæ— æ³•join groupï¼š
Group coordinator XXXX:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery

çŒœæµ‹1ï¼š broker 0å¹¶éçœŸçš„æŒ‚ï¼Œåªæ˜¯å› ä¸ºkafka logæ–‡ä»¶æƒé™ä¸ºrootï¼Œé€ æˆkafkaæœåŠ¡å¤„äºå¼‚å¸¸çŠ¶æ€ï¼ˆè¿˜å¯ä»¥è·Ÿå…¶ä»–æœºå™¨æ²Ÿé€šï¼‰ï¼Œæ‰€ä»¥æ­¤æ—¶ä»ç„¶æ»¡è¶³min.isr=2çš„è¦æ±‚
å¦å®šï¼šæ ¹æ®broker 0ä¸Šé¢çš„æ—¥å¿—ï¼Œå¯ä»¥çœ‹åˆ°kafkaæ ¹æœ¬æ²¡æœ‰å¯¹åº”æ—¶é—´æ®µçš„ä»»ä½•æ—¥å¿—
çŒœæµ‹2: è™½ç„¶broker 2ç½‘ç»œæ–­å¼€ï¼Œæ­¤æ—¶ broker 2 å¯¹äº broker 1 æ¥è¯´å±äºå‡æ­»çŠ¶æ€ï¼Œå°šæœªæ›´æ–°metadataï¼Œæ‰€ä»¥æ²¡æœ‰æ£€æµ‹å‡ºinsufficient isr
éªŒè¯ï¼šå‘ç°æ–­ç½‘åå‡ºç°å¦‚ä¸‹é”™è¯¯ Opening socket connection to server sgkc2-devclr-v07/x.x.x.47:2181. Will not attempt to authenticate using SASL (unknown error)
æ¯”è¾ƒå¯é çš„çŒœæµ‹ï¼šçŒœæµ‹2åŸºæœ¬å¯¹çš„ï¼Œä¸è¿‡ä¸æ˜¯å‡æ­»ï¼Œè€Œæ˜¯æœ¬èº«å°±å·²ç»æˆäº†å­¤ç«‹èŠ‚ç‚¹ï¼Œåˆæ— æ³•ä¸broker 0å’Œ2çš„zookeeperé€šä¿¡æ›´æ–°ä¿¡æ¯ï¼Œä»è€Œé€ æˆkafkaæœåŠ¡ç«¯å¼‚å¸¸ï¼Œäº§ç”Ÿäº†ç»•è¿‡min.isré™åˆ¶çš„å‡è±¡
[2022-03-14 08:59:39,144] WARN Client session timed out, have not heard from server in 4002ms for sessionid 0x17862f2bedc0004 (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:39,145] INFO Client session timed out, have not heard from server in 4002ms for sessionid 0x17862f2bedc0004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:39,826] INFO Opening socket connection to server sgkc2-devclr-v07/x.x.x.47:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:41,829] WARN Client session timed out, have not heard from server in 2583ms for sessionid 0x17862f2bedc0004 (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:41,829] INFO Client session timed out, have not heard from server in 2583ms for sessionid 0x17862f2bedc0004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:42,820] INFO Opening socket connection to server sgkc2-devclr-v05/x.x.x.45:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:42,821] INFO Socket error occurred: sgkc2-devclr-v05/x.x.x.45:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2022-03-14 09:00:00,597] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=2095832195, epoch=8913816) to node 2: java.io.IOException: Connection to 2 was disconnected before the response was read. (org.apache.kafka.clients.FetchSessionHandler)
[2022-03-14 09:00:00,598] WARN [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=1, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=2095832195, epoch=8913816)) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 2 was disconnected before the response was read
        at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
        at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:100)
        at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:193)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:280)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:132)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:131)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
ã€‚ã€‚ã€‚
[2022-03-14 09:00:32,612] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=2095832195, epoch=INITIAL) to node 2: java.net.SocketTimeoutException: Failed to connect within 30000 ms. (org.apache.kafka.clients.FetchSessionHandler)
[2022-03-14 09:00:32,612] WARN [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=1, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={T-TRADE-CHK-0=(fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[0]), T-EOD-SNP-1=(fetchOffset=86, logStartOffset=86, maxBytes=1048576, currentLeaderEpoch=Optional[0]), T-DBMS-CHK-0=(fetchOffset=2002, logStartOffset=1992, maxBytes=1048576, currentLeaderEpoch=Optional[0]), T-CAPTURE-2=(fetchOffset=2002, logStartOffset=1992, maxBytes=1048576, currentLeaderEpoch=Optional[0])}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=2095832195, epoch=INITIAL)) (kafka.server.ReplicaFetcherThread)
java.net.SocketTimeoutException: Failed to connect within 30000 ms
        at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:96)
        at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:193)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:280)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:132)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:131)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)

åœºæ™¯2ï¼šç±»ä¼¼ä¸Šé¢åœºæ™¯
ç°åœ¨devä¸Š3ä¸ªèŠ‚ç‚¹é…ç½® borker 0 1 2ï¼š
offsets.topic.replication.factor=3 
min.insync.replicas=1 
transaction.state.log.replication.factor=3 
transaction.state.log.min.isr=1 
default.replication.factor=3 

åŒæ—¶æ–­å¼€äº†broker 0 å’Œ 2çš„ç½‘ç»œï¼Œç„¶åå¯åŠ¨kafka clientï¼Œæ­¤æ—¶kafka clientç«¯æ‰€åœ¨çš„ç½‘ç»œåˆ†åŒºå†…åªèƒ½è¿æ¥åˆ° borker 1ï¼ŒåŒæ ·å‡ºç°äº†åœºæ™¯1ä¸­çš„æƒ…å½¢ï¼Œ
å¯ä»¥æ­£å¸¸subscribe topicå¹¶ä¸”kafka serverç«¯borker 1æ­£å¸¸è¿›è¡Œäº†rebalanceï¼Œä½†æ˜¯æ¥ä¸‹æ¥clientç«¯åœ¨onPartitionAssignedå†…è¿›ä¸€æ­¥è®¿é—®kafkaè·å–ä¸€äº›å¿«ç…§æ•°æ®æ—¶äº§ç”Ÿè¶…æ—¶é”™è¯¯

ä¸è¿‡è·Ÿåœºæ™¯1ä¸å¤ªä¸€æ ·çš„æ˜¯ï¼Œåæ¥broker 2æ¢å¤äº†ï¼Œå¹¶ä¸”broker 1æˆåŠŸè¿æ¥äº†broker 2ï¼ˆä»¥åŠå…¶åŒä¸€æœºå™¨ä¸Šçš„zookeeperï¼‰ï¼Œåªä¸è¿‡è¿‡ç¨‹ä¸­fetchçš„æ—¶å€™å‡ºé”™
è™½ç„¶ 18:09:48,061 è¿æ¥ä¸Šäº†ï¼Œä½†æ˜¯åé¢fetch ioé”™è¯¯
[2022-03-12 18:10:07,087] INFO Opening socket connection to server vm-v01/x.x.x.x:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2022-03-12 18:10:07,089] INFO Socket connection established to vm-v01/x.x.x.x:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2022-03-12 18:10:07,093] INFO Session establishment complete on server vm-v01/x.x.x.x:2181, sessionid = 0x27b6bf74fac000e, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2022-03-12 18:10:07,093] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)

æ³¨æ„åˆ°å…³é”®è¯ï¼šShrinking
[2022-03-12 18:10:07,139] INFO [Partition __consumer_offsets-43 broker=3] Shrinking ISR from 3,1,4 to 3. Leader: (highWatermark: 115, endOffset: 120). Out of sync replicas: (brokerId: 1, endOffset: 115) (brokerId: 4, endOffset: 116). (kafka.cluster.Partition)
ç´§æ¥ç€åˆexpandï¼Ÿ
[2022-03-12 18:10:07,381] INFO [Partition __consumer_offsets-43 broker=3] Expanding ISR from 3 to 3,4 (kafka.cluster.Partition)

ä½†æ˜¯fetché”™è¯¯
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread)
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] 
	Error sending fetch request (sessionId=346995240, epoch=1677663) to node 4: 
	java.io.IOException: Client was shutdown before response was read. (org.apache.kafka.clients.FetchSessionHandler)
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread)

å¯è§ç½‘ç»œè¿˜æ˜¯ä¸ç¨³å®šæˆ–è€…broker 1 å’Œ 2ä¹‹é—´çš„é€šä¿¡åŒæ­¥å‡ºç°é”™è¯¯

åœºæ™¯3ï¼š
æ—¢ç„¶å‰é¢ä¸¤ä¸ªåœºæ™¯ä¸­éƒ½æ˜¯æœ‰ä¸¤ä¸ªèŠ‚ç‚¹ä¸æ­£å¸¸ï¼Œå†æµ‹è¯•ä¸€æ¬¡ï¼š
broker 0 1 2ï¼Œæ–­ç½‘broker 2ï¼ˆä»¥åŠå…¶æœåŠ¡å™¨ä¸Šé¢çš„zookeeperèŠ‚ç‚¹ï¼‰
åŒæ ·å¯åŠ¨kafka-clientï¼Œä»ç„¶è·Ÿå‰é¢ç±»ä¼¼ï¼Œclientç«¯èƒ½å¤Ÿå¯åŠ¨å¹¶ä¸”è®¢é˜…topicï¼Œåªä¸è¿‡æ¢å¤çš„æ—¶å€™ï¼ˆonPartitionAssignå†…éƒ¨è¿›ä¸€æ­¥è®¿é—®kafkaè¯»å–å¿«ç…§æ•°æ®ï¼‰æŠ›é”™ï¼›
ä½†æ˜¯å†è¯•äº†å‡ æ¬¡å‘ç°è¿™ä¸ªé”™è¯¯æœ‰ç‚¹æ„æ€ï¼Œåˆšå¼€å§‹çš„æ—¶å€™ï¼Œæ¯æ¬¡éƒ½ä¼šéšæœºæˆåŠŸæŸå‡ ä¸ªpartitionï¼Œç›´åˆ°2å°æ—¶åæ‰ç¨³å®šçš„æˆåŠŸï¼ˆåé¢å†æµ‹è¯•å‡ è½®å®é™…æ˜¯éšæœºæˆåŠŸå’Œå¤±è´¥ï¼‰
ç¨‹åºçš„main consumer subscribe topicåŸºæœ¬éƒ½æ²¡æœ‰é—®é¢˜ ï¼ˆä¸­é—´åªæœ‰ä¸€æ¬¡å‡ºç°äº†åœºæ™¯5ä¸­çš„æ— æ³•å¯åŠ¨çš„é—®é¢˜ï¼‰ï¼Œ
ç„¶åonPartitionAssignå†…éƒ¨è¿›ä¸€æ­¥è®¿é—®kafkaè¯»å–å¿«ç…§æ•°æ®çš„æ—¶å€™ä¸ç¨³å®š
è¯»å–å¿«ç…§è‡ªç„¶ç”¨çš„ä¸æ˜¯subscribeè€Œæ˜¯
        TopicPartition topicPartition = new TopicPartition(config.getSnapshotTopic(), partition);
        Consumer<String, Snapshot> consumer = createConsumer(partition);
        long lastOffset = consumer.endOffsets(Collections.singleton(topicPartition)).get(topicPartition) - 2;
        if (lastOffset < 0) {
            lastOffset = 0;
        }
        consumer.assign(Collections.singleton(topicPartition));
        consumer.seek(topicPartition, lastOffset);
        ConsumerRecords<String, Snapshot> records = consumer.poll(Duration.ofMillis(10_000L));
        Snapshot snapshot = null;
        for (ConsumerRecord<String, Snapshot> record : records) {
            snapshot = record.value();
        }
        consumer.close();
```
é€šè¿‡åŠ æ—¥å¿—ï¼Œå‘ç°é”™è¯¯éƒ½æ˜¯å‘ç”Ÿåœ¨endOffsetsè¿™é‡Œï¼Œé»˜è®¤çš„timeoutåº”è¯¥æ˜¯30sï¼Œ 
è¿™ä¸ªé—®é¢˜å°±æ˜¯è¯»å–metadataå‡ºç°é—®é¢˜ï¼Œ
æœ€ç»ˆåå¤æµ‹è¯•ç¡®å®šæ ¹æºï¼š
å¦‚æœæŸä¸ªbrokerèŠ‚ç‚¹æ˜¯ç›´æ¥æ€æ­»ï¼Œåªè¦è¯¥èŠ‚ç‚¹ç½‘ç»œé€šçš„ï¼Œclientç«¯è¯»å–metadataå°±ä¸ä¼šè¶…æ—¶ï¼Œ
ä½†æ˜¯å¦‚æœæŸä¸ªbrokerèŠ‚ç‚¹æ‰€åœ¨æœåŠ¡å™¨ç½‘ç»œæ–­å¼€ï¼Œclientç«¯çš„kafka clusteré…ç½®ä¸­ä»æœ‰è¯¥brokerçš„ä¿¡æ¯ï¼Œé‚£ä¹ˆclientç«¯è¯»å–metadataçš„æ—¶å€™å¯èƒ½ä¼šå°è¯•è¿æ¥æœ‰é—®é¢˜çš„èŠ‚ç‚¹ç”±äºç½‘ç»œé‡è¯•é€ æˆè¶…æ—¶
##### æ‰€ä»¥è§£å†³æ–¹æ¡ˆï¼š
1ï¼‰ä¿®æ”¹ consumer.properties.bootstrap.serversï¼Œç§»é™¤åèŠ‚ç‚¹
2ï¼‰ä¿®æ”¹apiè°ƒç”¨ï¼Œå¢åŠ timeoutæ—¶é—´åˆ°5åˆ†é’Ÿï¼ˆå®æµ‹è¶…æ—¶åœ¨2åˆ†é’Ÿå·¦å³ï¼‰ï¼šconsumer.endOffsets(Collections.singleton(topicPartition),Duration.ofMillis(300000)).get(topicPartition)
3ï¼‰ä¿®æ”¹é…ç½® consumer.properties.request.timeout.ms=300000ï¼Œæ³¨æ„è¿™ä¸ªåªå¯¹org.apache.kafka.clients.consumer.KafkaConsumerç”Ÿæ•ˆï¼Œæˆ‘ä»¬è¿™é‡Œç”¨çš„æ˜¯org.apache.kafka.clients.consumer.Consumer

```
åœºæ™¯4ï¼š
1. broker 0 1 2 éƒ½å¯åŠ¨
2.åœ broker 2ï¼Œå¯åŠ¨kafka clientç«¯æœåŠ¡æ­£å¸¸ï¼Œexpected
3.åœbroker 0ï¼Œæ— æ³•å¯åŠ¨kafka clientç«¯æœåŠ¡ï¼Œexpected
4.æ¢å¤broker2ï¼Œæ— æ³•å¯åŠ¨kafka clientç«¯æœåŠ¡ï¼ŒæŠ¥é”™ï¼Œunexpected
  clientç«¯ä»ç„¶æ˜¯æŠ¥é”™è·Ÿ3ä¸€æ ·ï¼Œ[Consumer clientId=consumer-2, groupId=TEST-SZL] 1 partitions have leader brokers without a matching listener, including [T-TRADE-1]ï¼Œ
  kafkaæœåŠ¡ç«¯æŠ¥é”™ï¼š
  [2022-03-16 10:58:02,825] TRACE [Controller id=1] Leader imbalance ratio for broker 2 is 0.1282051282051282 (kafka.controller.KafkaController)
[2022-03-16 10:58:02,825] INFO [Controller id=1] Starting preferred replica leader election for partitions T-TEST-SNP-2,T-TEST-0,T-TRADE-SNP-2,T-TRADE-1,T-CAPTURE-1 (kafka.controller.KafkaController)
[2022-03-16 10:58:02,836] ERROR [Controller id=1] Error completing preferred replica leader election for partition T-TEST-0 (kafka.controller.KafkaController)
kafka.common.StateChangeFailedException: Failed to elect leader for partition T-TEST-0 under strategy PreferredReplicaPartitionLeaderElectionStrategy
        at kafka.controller.PartitionStateMachine.$anonfun$doElectLeaderForPartitions$9(PartitionStateMachine.scala:390)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at kafka.controller.PartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:388)
        at kafka.controller.PartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:315)
        at kafka.controller.PartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:225)
        at kafka.controller.PartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:141)
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$onPreferredReplicaElection(KafkaController.scala:649)
        at kafka.controller.KafkaController.$anonfun$checkAndTriggerAutoLeaderRebalance$6(KafkaController.scala:1008)
        at scala.collection.immutable.Map$Map3.foreach(Map.scala:195)
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$checkAndTriggerAutoLeaderRebalance(KafkaController.scala:989)
        at kafka.controller.KafkaController$AutoPreferredReplicaLeaderElection$.process(KafkaController.scala:1020)
        at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:94)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
        at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:94)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2022-03-16 10:58:02,837] WARN [Controller id=1] Partition T-TEST-0 failed to complete preferred replica leader election to 2. Leader is still 0 (kafka.controller.KafkaController)
åŸå› åˆ†æï¼šåº”è¯¥æ˜¯ç”±äºæ“ä½œé¢‘ç‡è¿‡å¿«ï¼Œå¯åœkafka borkerçš„æ—¶å€™æ²¡æœ‰ç»™è¶³æ—¶é—´åšfailoverï¼Œå¯è§kafkaæœåŠ¡æœ¬èº«å¯ä»¥çœ‹åˆ°ä»–çš„å¥å£®æ€§æœ‰é—®é¢˜äº†ï¼Œå±…ç„¶æ— æ³•é€‰ä¸¾æ–°leader

é‡æ–°è®¾è®¡è¿ç»­æµ‹è¯•åœºæ™¯ï¼š
-----------------------------------
--- borker 0 1 2 alive
-----------------------------------
start kafka client
test kafka client
-----------------------------------
--- kill 2(both zookeeper&kafka), borker 0 1 alive
-----------------------------------
test kafka client
restart kafka client
test kafka client

-----------------------------------
--- resume 2, borker 0 1 2 alive
-----------------------------------
test kafka client
restart kafka client
test kafka client

-----------------------------------
--- shutdown network for borker 2, borker 0 1 alive
-----------------------------------
test kafka client	
restart kafka client
test kafka client


åœºæ™¯5ï¼šè·Ÿå‰é¢ç±»ä¼¼ï¼Œåªä¸è¿‡è¿™æ¬¡æ˜¯kafkaå®¢æˆ·ç«¯å¯åœå¤ªå¿«
æ–­å¼€ borker 2ç½‘ç»œ, borker 0 1 alive
ç¬¬ä¸€æ¬¡é‡å¯kafka clientç«¯ï¼ˆstop then startï¼‰å¤±è´¥ï¼
    2022-03-16 17:14:21.728  INFO 370GG [main] o.a.k.c.u.AppInfoParser$AppInfo : Kafka version: 2.2.0
		2022-03-16 17:14:21.728  INFO 370GG [main] o.a.k.c.u.AppInfoParser$AppInfo : Kafka commitId: 05fcfde8f69b0349
		2022-03-16 17:15:21.737  INFO 370GG [main] ConditionEvaluationReportLoggingListener : 

		Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
		2022-03-16 17:15:21.767 ERROR 370GG [main] o.s.b.SpringApplication : Application run failed

		org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

		2022-03-16 17:15:21.808  INFO 370GG [main] o.s.s.c.ExecutorConfigurationSupport : Shutting down ExecutorService 'applicationTaskExecutor'
		2022-03-16 17:15:21.809  INFO 370GG [main] o.a.k.c.p.KafkaProducer : [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
		2022-03-16 17:15:21.816  INFO 370GG [main] c.a.d.p.DruidDataSource : {dataSource-1} closing ...
		2022-03-16 17:15:21.831  INFO 370GG [main] c.a.d.p.DruidDataSource : {dataSource-1} closed
    possible reason:
		åœæ‰ kafka clientå¤§æ¦‚æ˜¯åœ¨ï¼š
		2022-03-16 17:13:51.918 ^[[32m INFO^[[m ^[[35m30256GG^[[m [QFJ Timer] ^[[36mc.q.c.f.f.s.AbstractApplication^[[m : fix server toAdmin: [8=FIX.4.4|9=60|35=0|34=683|49=EXEC|52=20220316-09:13:51.918|56=EXCHANGE_FS|10=167|]
		ç„¶åå¾ˆå¿«å¯åŠ¨äº† kafka clientï¼š
		2022-03-16 17:14:17.944 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.StartupInfoLogger^[[m : Starting TradeFrontMain v1.1.0-SNAPSHOT using Java 1.8.0_40 on XXXX with PID 370 (/lyhistory/kafka client.jar started by clear in /lyhistory)
		2022-03-16 17:14:17.955 ^[[32mDEBUG^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.StartupInfoLogger^[[m : Running with Spring Boot v2.4.5, Spring v5.3.6
		2022-03-16 17:14:17.956 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.SpringApplication^[[m : The following profiles are active: dev
		2022-03-16 17:14:19.821 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.w.e.t.TomcatWebServer^[[m : Tomcat initialized with port(s): 10102 (http)
		2022-03-16 17:14:19.835 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.a.j.l.DirectJDKLog^[[m : Initializing ProtocolHandler ["http-nio-10102"]
		2022-03-16 17:14:19.836 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.a.j.l.DirectJDKLog^[[m : Starting service [Tomcat]
		2022-03-16 17:14:19.836 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.a.j.l.DirectJDKLog^[[m : Starting Servlet engine: [Apache Tomcat/9.0.45]
		å¯¹åº”çš„group coordinatoråœ¨broker 1ä¸Šï¼š
		[2022-03-16 17:14:07,063] INFO [GroupCoordinator 1]: Member consumer-2-f852e86d-7db8-4943-b378-097fe08415f8 in group TEST-SZL has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
		[2022-03-16 17:14:07,064] INFO [GroupCoordinator 1]: Preparing to rebalance group TEST-SZL in state PreparingRebalance with old generation 1 (__consumer_offsets-43) (reason: removing member consumer-2-f852e86d-7db8-4943-b378-097fe08415f8 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
		[2022-03-16 17:14:07,065] INFO [GroupCoordinator 1]: Group TEST-SZL with generation 2 is now empty (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
		[2022-03-16 17:15:54,415] INFO [GroupMetadataManager brokerId=1] Group TEST-SZL transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
		å¯ä»¥çœ‹åˆ°åœ¨kafka clientå¯åŠ¨çš„æ—¶å€™ï¼Œåˆšå¥½æ˜¯group coordinatoråœ¨å¤„ç†removeä¹‹å‰çš„kafka client consumerçš„æ—¶å€™ï¼Œæ‰€ä»¥é€ æˆè¯»å–offsetæ—¶é—´è¶…æ—¶ï¼

ç»éªŒæ€»ç»“ï¼š
1. é‡åˆ°ç½‘ç»œæ•…éšœï¼Œå³ä½¿è‡ªåŠ¨æ¢å¤äº†ï¼Œä¹Ÿæœ€å¥½è¦å¯åœä¸€ä¸‹zookeeperå’Œkafka
2. å¯åœkafkaæœåŠ¡æ“ä½œæ—¶ä¸è¦åŠ¨ä½œå¤ªå¿«
3. å¯åœkafkaå®¢æˆ·ç«¯æ“ä½œæ—¶ä¹Ÿä¸è¦åŠ¨ä½œå¤ªå¿«

è¿™äº›å¼‚å¸¸æƒ…å†µå¯èƒ½ä¹Ÿæ˜¯kafkaæœ€ç»ˆæŠ›å¼ƒzookeeperçš„åŸå› ï¼
```

#### é€ æˆkafka clientç«¯ç¨‹åºpollè‡ªå·±ç»´æŠ¤çš„offset topicå‡ºç°é—®é¢˜
ç»“åˆå‰é¢ ã€4.1.3 ä¸ä¾èµ–interal offsetï¼Œè‡ªå·±ç»´æŠ¤offset exactly-onceã€‘
public ConsumerRecords<K,V> poll(java.time.Duration timeout)

This method returns immediately if there are records available. Otherwise, it will await the passed timeout. If the timeout expires, an empty record set will be returned. 

```
consumer.seek(topicPartition, snpoffset);
        ConsumerRecords<String, SimpleSnapshot> records = consumer.poll(Duration.ofMillis(10_000L));
        if(records!=null) {
            System.out.println("records not null, count=" + String.valueOf(records.count()));
        }
```
å’Œå‰é¢æƒ…å†µç±»ä¼¼ï¼Œç»è¿‡å®é™…æµ‹è¯•ï¼Œå‘ç°è¿™ä¸ª10sçš„è®¾ç½®ï¼ŒæŒ‡å®šçš„offsetæœ‰æ—¶å€™èƒ½è¿”å›æ­£ç¡®çš„messageï¼Œæœ‰æ—¶å€™è¿”å›empty recordï¼ï¼Œ
ç„¶åæ”¹æˆ 30000 å³5åˆ†é’Ÿåˆ™æ²¡æœ‰é—®é¢˜

#### é€ æˆkafka clientç«¯ç¨‹åºè¯»å– metadata è¶…è¿‡é»˜è®¤ 1åˆ†é’Ÿ æŠ›é”™

public java.util.List<PartitionInfo> partitionsFor(java.lang.String topic)
Get metadata about the partitions for a given topic. This method will issue a remote call to the server if it does not already have any metadata about the given topic.

TimeoutException - if the offset metadata could not be fetched before the amount of time allocated by **default.api.timeout.ms** expires.

### This member will leave the group because consumer poll timeout has expired

[36m.c.i.AbstractCoordinator$HeartbeatThread^[[m : 
[Consumer clientId=consumer-1, groupId=TEST-SZL] 
This member will leave the group because consumer poll timeout has expired. This means the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time processing messages. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.
[[36mo.a.k.c.c.i.AbstractCoordinator^[[m : [
Consumer clientId=consumer-1, groupId=TEST-SZL] Member consumer-1-7f40d109-cd66-4554-82a9-376f1922c1b5 sending LeaveGroup request to coordinator x.x.x.x:9092 (id: 2147483647 rack: null)

åˆ†æï¼š
é¦–å…ˆæŠ¥é”™çš„æ„æ€æ˜¯ç¨‹åºå¤„ç†ä¸€æ‰¹kafkaæ¶ˆæ¯ï¼ˆmax.poll.records é»˜è®¤500æ¡ï¼‰çš„æ—¶é—´è¶…è¿‡äº†max.poll.interval.msï¼Œæ‰€ä»¥ç›´è§‚çš„çœ‹æ¥å°±æ˜¯é™ä½max.poll.recordsæˆ–è€…æé«˜max.poll.interval.msï¼Œä¸è¿‡æˆ‘ä»¬è¿™é‡Œçš„åœºæ™¯å¹¶æ²¡æœ‰è¿™ä¹ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬å‡ºé”™çš„æ—¶å€™è¿˜æ²¡æœ‰å¼€å§‹è·å–å¹¶å¤„ç†kafkaæ¶ˆæ¯ï¼Œè€Œæ˜¯ç¬¬ä¸€æ¬¡pollè§¦å‘çš„rebalanceçš„è¿‡ç¨‹ä¸­å‡ºç°çš„é”™è¯¯ï¼›

è§‚å¯Ÿåˆ°çš„ç°è±¡æ˜¯ï¼Œæ¯æ¬¡æˆ‘ä»¬åˆ é™¤äº†æ‰€æœ‰topicï¼Œç„¶åé‡æ–°å¯åŠ¨ç¨‹åºçš„æ—¶å€™ï¼ˆç¨‹åºè®¾ç½®äº†auto.create.topics.enable=trueï¼‰ï¼Œç¬¬ä¸€æ¬¡consumer join groupåï¼Œè¿‡äº†äº”åˆ†é’Ÿåå°±æŠ¥ä¸Šé¢çš„é”™è¯¯ï¼›

è¯¥ç¨‹åºçš„ä¸»è¦é€»è¾‘ï¼š
```
private void start(ApplicationContext context) {
        if (managerThread == null) {
            workerManager = new SimpleWorkerManager(workContext);
            managerThread = new Thread(workerManager::start);
            managerThread.setDaemon(true);
            managerThread.setName(config.getTaskType() + "-MANAGER");
            managerThread.start();
        }
    }
SimpleWorkerManagerçš„æ„é€ æ–¹æ³•ä¸­è°ƒç”¨äº†kafkaçš„æ¶ˆæ¯è®¢é˜…
this.kafkaConsumer.subscribe(Collections.singleton(context.getConfig().getTaskTopic()), new SimpleWorkBalancer(context.getRestorer(), this::removeWorker, this::addWorker));

ç„¶åstrat()æ–¹æ³•æ˜¯å»pollæ¶ˆæ¯
ConsumerRecords<String, Info> records = kafkaConsumer.poll(pollDuration);
```
After subscribing to a set of topics, the consumer will automatically join the group when poll(Duration) is invoked.

æ‰€ä»¥é—®é¢˜å‡ºåœ¨ç¬¬ä¸€æ¬¡pollå¼•èµ·çš„rebalanceçš„å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œè¶…è¿‡äº†max.poll.interval.msï¼Œç„¶åæˆ‘ä»¬æŸ¥ä¸€ä¸‹è¿™ä¸ªé…ç½®ï¼š

> The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. For consumers using a non-null group.instance.id which reach this timeout, partitions will not be immediately reassigned. Instead, the consumer will stop sending heartbeats and partitions will be reassigned after expiration of session.timeout.ms. This mirrors the behavior of a static consumer which has shutdown.
> Type:	int
> Default:	300000 (5 minutes)
> Valid Values:	[1,...]
> Importance:	medium

æ­¤æ—¶æˆ‘æƒ³åˆ°äº†[å‰é¢çš„å¦å¤–ä¸€ä¸ªbugåšçš„ä¿®å¤](#ç½‘ç»œæ•…éšœ--kafkaé›†ç¾¤æœ‰èŠ‚ç‚¹æŒ‚æ‰ä¸æ˜¯æ­£å¸¸åœèŠ‚ç‚¹è€Œæ˜¯brokerèŠ‚ç‚¹æ‰€åœ¨æœåŠ¡å™¨ç½‘ç»œæ–­å¼€æˆ–æš´åŠ›åœæœº) request.timeout.ms æˆ‘è®¾ç½®æˆäº†5åˆ†é’Ÿï¼Œç„¶ååº”è¯¥æ˜¯åœ¨reblanceçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç¨‹åºçš„é€»è¾‘ä¼šå»ä»kafkaä¸­æ¢å¤æˆ‘ä»¬è‡ªå·±ç®¡ç†çš„å¿«ç…§ï¼Œä½†æ˜¯ç”±äºæ‰€æœ‰topicå·²ç»åˆ é™¤ï¼Œæ‰€ä»¥è¯»å–çš„æ—¶å€™å¯èƒ½ä¼šç­‰å¾…5åˆ†é’Ÿï¼Œç„¶å max.poll.interval.ms é»˜è®¤ä¹Ÿæ˜¯5åˆ†é’Ÿï¼Œè‡ªç„¶å°±timeoutäº†ï¼Œæ‰€ä»¥ä¿®å¤åŠæ³•æ˜¯å»¶é•¿ max.poll.interval.msåˆ°æ›´é•¿çš„æ—¶é—´

æ³¨æ„ï¼š
> Also as part of KIP-266, the default value of request.timeout.ms has been changed to 30 seconds. The previous value was a little higher than 5 minutes to account for maximum time that a rebalance would take. Now we treat the JoinGroup request in the rebalance as a special case and use a value derived from max.poll.interval.ms for the request timeout. All other request types use the timeout defined by request.timeout.ms
> Notable changes in 0.10.2.1
> The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer retries default value was changed from 0 to 10. The internal Kafka Streams consumer max.poll.interval.ms default value was changed from 300000 to Integer.MAX_VALUE.
> The new Java Consumer now supports heartbeating from a background thread. There is a new configuration max.poll.interval.ms which controls the maximum time between poll invocations before the consumer will proactively leave the group (5 minutes by default). The value of the configuration request.timeout.ms (default to 30 seconds) must always be smaller than max.poll.interval.ms(default to 5 minutes), since that is the maximum time that a JoinGroup request can block on the server while the consumer is rebalance. Finally, the default value of session.timeout.ms has been adjusted down to 10 seconds, and the default value of max.poll.records has been changed to 500.

## Reference
+ kafkaåŸç†

  + kafkaæ¶æ„/å·¥ä½œæ–¹å¼
    http://www.linkedkeeper.com/detail/blog.action?bid=1016
    https://www.cnblogs.com/qiu-hua/p/13388326.html
    https://blog.csdn.net/qq_37095882/article/details/81024048

    Kafka æ•°æ®å¯é æ€§æ·±åº¦è§£è¯» 

    https://www.infoq.cn/article/depth-interpretation-of-kafka-data-reliability
    éœ‡æƒŠäº†ï¼åŸæ¥è¿™æ‰æ˜¯kafkaï¼ 

    https://www.jianshu.com/p/d3e963ff8b70

  + kafka stream

    https://github.com/confluentinc/kafka-streams-examples/tree/4.0.0-post

    https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/
    https://www.youtube.com/watch?v=p9wcx3aTjuo
    http://benstopford.com/uploads/CraftMeetup.pdf
    https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html
    [Designing Event-Driven Systems Concepts and Patterns for Streaming Services with Apache Kafka --Ben Stopford](https://www.confluent.io/wp-content/uploads/confluent-designing-event-driven-systems.pdf)

    [Building Event Driven Services with Apache Kafka and Kafka Streams by Ben Stopford](https://www.youtube.com/watch?v=p9wcx3aTjuo)

    https://stackoverflow.com/questions/40274884/is-kafka-stream-statestore-global-over-all-instances-or-just-local
    https://stackoverflow.com/questions/52488070/difference-between-ktable-and-local-store
    https://stackoverflow.com/questions/50741186/how-to-filter-out-unnecessary-records-before-materializing-globalktable/50752095#50752095
    https://github.com/confluentinc/kafka-streams-examples/issues/126

  + Consumer

    consumer group:
    https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html
    https://www.confluent.io/blog/apache-kafka-data-access-semantics-consumers-and-membership

    Kafka Consumerè®¾è®¡è§£æ
    http://www.jasongj.com/2015/08/09/KafkaColumn4/

    kafka9é‡å¤æ¶ˆè´¹é—®é¢˜è§£å†³ 

    https://blog.csdn.net/u011637069/article/details/72899915

  + other detail

    partition message ordering:
    https://stackoverflow.com/questions/29820384/apache-kafka-order-of-messages-with-multiple-partitions

    ä¸€æ–‡çœ‹æ‡‚Kafkaæ¶ˆæ¯æ ¼å¼çš„æ¼”å˜ 

    https://blog.csdn.net/u013256816/article/details/80300225

    consuming and then producing messages based on what you consumed
    https://stackoverflow.com/questions/45195010/meaning-of-sendoffsetstotransaction-in-kafka-0-11

    seek seekToEnd

    https://stackoverflow.com/questions/48395934/how-to-get-last-committed-offset-from-read-committed-kafka-consumer

    assign() vs subscribe()

+ Dev doc

  https://www.tutorialspoint.com/apache_kafka/apache_kafka_quick_guide.htm

  https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/

+ best practice

  https://go.streamsets.com/wc-guide-best-practices-for-simplifying-apache-kafka.html?assetpushed=guide-simplifying-apache-kafka

  https://www.digitalocean.com/community/tutorial_collections/how-to-install-apache-kafka

+ use case

  Kafkaå®ç°æ·˜å®äº¿ä¸‡çº§æ•°æ®ç»Ÿè®¡ https://zhuanlan.zhihu.com/p/58747239

  [How Netflix Handles Data Streams Up to 8M Events/sec](https://www.youtube.com/watch?v=WuRazsX-MBY)

  å¦‚ä½•ä½¿ç”¨Kafka Streamsæ„å»ºå¹¿å‘Šæ¶ˆè€—é¢„æµ‹ç³»ç»Ÿ  https://mp.weixin.qq.com/s?spm=a2c4e.11153940.blogcont270421.21.7d142e7catwiNO&__biz=MzU1NDA4NjU2MA==&mid=2247486752&idx=1&sn=e9e789c18a6801af12db4c97dcb098e5&chksm=fbe9b4efcc9e3df9cb7d8f666335ed669a7339b27663fc223e5aacfbfa5d2d7404b431b3937d&scene=27#wechat_redirect
  https://medium.com/@Pinterest_Engineering/using-kafka-streams-api-for-predictive-budgeting-9f58d206c996

  Twitter

  Twitter is an online social networking service that provides a  platform to send and receive user tweets. Registered users can read and  post tweets, but unregistered users can only read tweets. Twitter uses  Storm-Kafka as a part of their stream processing infrastructure.

  LinkedIn

  Apache Kafka is used at LinkedIn for activity stream data and  operational metrics. Kafka mes-saging system helps LinkedIn with various products like LinkedIn Newsfeed, LinkedIn Today for online message  consumption and in addition to offline analytics systems like Hadoop.  Kafkaâ€™s strong durability is also one of the key factors in connection  with LinkedIn.

  Netflix

  Netflix is an American multinational provider of on-demand Internet  streaming media. Netflix uses Kafka for real-time monitoring and event  processing.

  Mozilla

  Mozilla is a free-software community, created in 1998 by members of  Netscape. Kafka will soon be replacing a part of Mozilla current  production system to collect performance and usage data from the  end-userâ€™s browser for projects like Telemetry, Test Pilot, etc.

  Oracle

  Oracle provides native connectivity to Kafka from its Enterprise  Service Bus product called OSB (Oracle Service Bus) which allows  developers to leverage OSB built-in mediation capabilities to implement  staged data pipelines.


+ products
  https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem

  schema registry https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html

+ Cheatsheet
  https://gist.github.com/filipefigcorreia/3db4c7e525581553e17442792a2e7489

+ related
https://medium.com/eleven-labs/cqrs-pattern-c1d6f8517314
https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs
https://docs.microsoft.com/en-us/azure/architecture/patterns/materialized-view

https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol

## Presentaion

---

ã€ŠIntroduction to kafkaã€‹

1.Basic Concepts
	Whatâ€™s Kafka
	Why use Kafka
	How it works
2.Exactly once Semantics

Whatâ€™s Kafka

Apache Kafka is a community distributed streaming platform capable of handling trillions of events a day. 
Initially conceived as a messaging queue, Kafka is based on an abstraction of a distributed commit log. 
Since being created and open sourced by LinkedIn in 2011, 
Kafka has quickly evolved from messaging queue to a full-fledged event streaming platform.

Why use Kafka

Microservice and kafka already became a de-facto industry standard

1.
basicaly kafka is a messaging system, compare to other messaging middleware like the one I used before called rabbit mq, with rabbit mq you can only process once,
after consuming the message, it's removed from the queue.
kafka provides durable storage of messages, sometimes kafka is used as a kind of database.
and now kafka has evolved from messaging queue to full-fleged event streaming platform, we're not using the streaming feature, so today the topic only cover messaging queue.

2.
why do we use kafka

![](/docs/docs_image/software/buildingblock/kafka/kafka21.png)

Let's have a look at architecuture diagram next slides microservices approach vs traditional approach, 
in traditional approach, application stack multiple layers and compononets together as a single unit.
we can see microservice segregates functionalities into a set of autonamous services,so the circle connecting microservices is message queue system.
there are some advantages for microservice approach,
there is no single point of failure, one service broke down doesn't impact other services;
its easier to scale up, all these services are deployed independently, esier to identify the bottle neck and scale up;
from developer standpoint, it can save a lot of time troubleshooting the microservices compared to debug into the traditional application,
micorservice is designed based on single reponsiblity principle, you can find the paticular service responsible for the cause straightforward.

3.

![](/docs/docs_image/software/buildingblock/kafka/kafka22.png)

kafka works like this:
producers publish message to the topics on brokers, the consumers subscribe to the topic will continously poll from the brokers.
in the middle is the brokers, we have 4 brokers, each broker represents one instance of the kafka server, we have 2 topics allocated on the brokers:
topic 1 and topic 2, topic 1 have 2 paritions, topic 2 have 1 parition, each topic has two replications, to publish a message, the producers has to specify 3 params:
the topic name, which partition and the message itself, the messsage will be published on to leader partition, and the followers will replicate from leader,

consumers can join in the same group by config the same application id,
each one partition can be consumed by consumers from different consumer group, but one partition can only be consumed by one consumer in the same consumer group, 
in another word, consumers in the same consumer group load balance the topic partitions, consumers from different consumer group are idenpendent from each other.

4.
one critical concerns is how do we achieve exactly once semantics, how do we guarantee there is no missing or duplicated messages, there is a misconception that 
develop using kafka API will inherently has the capbility to achieve exactly once senmantics, truth is we have to design properly.
to discuss this concern, let's look at a typical application.

![](/docs/docs_image/software/buildingblock/kafka/kafka23.png)



we post a message to APP-1, APP-1 extract the data,transform and produce the message to kafka, APP-2 will consume the message.
very simple but it can go wrong from many aspects.
first, the http call, when we make a http post, it may happen that APP-1 recevied the post data and processed, 
but somehow failed to return the reposonse back to the http client due to may be network issue, so the http client side will be timeout, 
normally the http client library will retry for this secnario, if the network recovered, APP-1 will recevie duplicated message, 
in this case from my own experience, what we would do is that we use redis on APP-1 to check duplication. 
the same may happen when APP-1 publish message to kafka, good news is that in the latest kafka version, 
it already help us handled this secnario, all we need to do is simply config enable idempotence to be ture.
go on the consumer side, unfortunately, consumer side is too much complicated, there is no easy way to solve it, 
before further discuss, let me clarify the verb 'processing', there are mainly two types of processing: in-memory processing, the other type is data persist(for example 
store into database, write to kafka), if it's purly in-memory processing there is nothing to worry about, whenever it's broken, so I'm talking about type 2,
, let's assume processing here means write to kafka.

![](/docs/docs_image/software/buildingblock/kafka/kafka24.png)

Transactional delivery allows producers to send data to multiple partitions such that either all messages are successfully delivered, or none of them are.

1) Producer
However, idempotent producers donâ€™t provide guarantees for writes across multiple TopicPartitions. For this, one needs stronger transactional guarantees, ie. the ability to write to several TopicPartitions atomically. By atomically, we mean the ability to commit a set of messages across TopicPartitions as a unit: either all messages are committed, or none of them are.
2) Consumer
maintain offset
Refer to https://kafka.apache.org/0102/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html
By default, kafka help maintains consumer offset in topic called "__consumer_offset", whenever the consumer dies and restart or failover to another consumer in the same consumer group, it will continue from last committed offset.For example, we have two consumers A and B in one consumer group, if A crashed, B will take over A and continue from where A left over(from last committed offset), sounds good? Actually no,  here is why? 
there are two ways to commit offset:
Automatically commit when receive the record by calling poll()[Let's say, you call poll each 10 ms, and set commit-interval to 100ms. Thus, in every 10th call to poll will commit (and this commit covers all messages from the last 10 poll calls).], the problem with this way is that the consumer may fail process the offset has already advanced(If auto-commit is enabled for the Kafka consumer, the event processor might already have commited the event offset and fail before finishing the event. The consumer must commit the event manually after all relevant sub-processes have completed.), the other way is manually commit after process the record, this method is still problematic as the consumer may crashed after processing the record(calculating some results and send it to the downstream consumers/send to kafka) but before sending the commit to the kafka broker.


**snapshot and recovery **

When the rebalance happen, for the example when a consumer crashed, the other consumer will take over the partition and continue from last committed offset, but hold on, what about the context, I mean the states stored in the memory, how do we recover the states.
So what we can do is that we reprocessing the partition from the beginning, but the problem is message processing is time consuming, and when the partition grows fast, there are too many messages, reprocessing the partition may take a long time.
So what we can improve is that we add a checkpoint to take snapshot of the memory state, we call it checkpoint, so when rebalance happens, we can recover from the latest checkpoint instead of beginning, so this will improve the performance, save time for recovery.
By design, the new clearing system process records from kafka one by one in sequence, during the data processing, the consumer generated many in memory results, for example currentPosition for each positionAccount, we want to have snapshot of the result from time to time, so in the case of recovery, the consumer can recover from snapshot, save a lot computing power to re-calculate the result.
By the way, Kafka do support stateful operation with kafka stream state store, but thatâ€™s mainly for aggregation operation, so in our framework we didnâ€™t take advantage of it


ä¸šåŠ¡æµæ°´çº¿å‡è®¾ä¸ºï¼š
å¾®æœåŠ¡A->å¾®æœåŠ¡B->å¾®æœåŠ¡C

å¾®æœåŠ¡Bå¤„ç†å®Œæ¥è‡ªå¾®æœåŠ¡Açš„æŸæ¡kafkaä¿¡æ¯ï¼Œç„¶åéœ€è¦å¾€ä¸‹æ¸¸ä¹Ÿå°±æ˜¯å¾®æœåŠ¡Cå‘é€kafkaæ¶ˆæ¯SendKafkaMsg_InfoToC()æ—¶ï¼ŒåŒæ—¶ä¼šä¿å­˜æ­¤æ—¶å¤„ç†çš„æ¥è‡ªå¾®æœåŠ¡Açš„æ¶ˆæ¯çš„offset SendKafkaMsg_SaveOffset()ï¼Œæˆ‘ä»¬æˆä¸ºå¢é‡å¿«ç…§ï¼Œ
SendKafkaMsg_InfoToC()å’ŒSendKafkaMsg_SaveOffset()å¯ä»¥ä½œä¸ºä¸€ä¸ªäº‹åŠ¡ä¸€èµ·æäº¤ï¼Œ
è¿™ç§æƒ…å†µä¸‹ï¼Œå¢é‡å¿«ç…§ä¿å­˜çš„æ˜¯ä¸€ä¸ªoffsetï¼Œå‡è®¾æ­¤æ—¶æœåŠ¡BæŒ‚æ‰ï¼Œé‡å¯åï¼ŒBä¼šå¯»æ‰¾offsetï¼Œç„¶åä»å¤´å¼€å§‹æ¢å¤å†…å­˜çŠ¶æ€ï¼Œç›´åˆ°è¿™ä¸ªoffsetï¼Œæ¢å¤åŒºæ®µä¸ºã€0, offsetã€‘ï¼›

ä¸ºäº†æ›´å¿«çš„æ¢å¤ï¼Œæˆ‘ä»¬ä¸æƒ³æ¯æ¬¡ä»0å¼€å§‹æ¢å¤ï¼Œæ‰€ä»¥å¼•å…¥å…¨é‡å¿«ç…§SendKafkaMsg_SaveMemory()ï¼Œä¿å­˜æ­¤æ—¶çš„å†…å­˜çŠ¶æ€ï¼Œå¹¶ä¸”å…¨é‡å¿«ç…§ä¸­è¿˜è¦ä¿å­˜æ­¤æ—¶å¯¹åº”çš„æ¥è‡ªå¾®æœåŠ¡Açš„æ¶ˆæ¯çš„offsetï¼Œä¹Ÿå°±æ˜¯è¯´è¿™æ˜¯å¤„ç†å®Œç¬¬å‡ æ¡æ¶ˆæ¯ä¹‹åçš„å†…å­˜çŠ¶æ€ï¼Œ
å¢é‡å¿«ç…§ä¸­ä¿å­˜äº†ä¸¤ä¸ªä¿¡æ¯ï¼šå…¨é‡å¿«ç…§çš„ä½ç½®å’Œæ¢å¤åŒºæ®µçš„ç»ˆæ­¢ä½ç½®å³offsetENDï¼Œ
ç„¶åæ ¹æ®å…¨é‡å¿«ç…§çš„ä½ç½®ï¼Œå†å»å–å‡ºå…¨é‡å¿«ç…§ï¼Œå…¨é‡å¿«ç…§ä¸­ä¿å­˜äº†å½“æ—¶çš„å†…å­˜ä¿¡æ¯ä»¥åŠæ¢å¤åŒºæ®µçš„èµ·å§‹ä½ç½®å³offsetSTARTï¼Œ
æ‰€ä»¥æ¢å¤æ—¶å…ˆæ‰¾åˆ°å¢é‡å¿«ç…§ï¼Œç„¶åæ ¹æ®å¢é‡å¿«ç…§å­˜çš„ä½ç½®ä¿¡æ¯æ‰¾åˆ°å…¨é‡å¿«ç…§ï¼Œæ¢å¤å†…å­˜ï¼Œå‰©ä¸‹çš„ä¸€ç‚¹å·®å¼‚å†é€šè¿‡æ¢å¤åŒºæ®µã€offsetSTART,offsetENDã€‘ï¼Œæ¢å¤åˆ°æœåŠ¡BæŒ‚æ‰ä¹‹å‰çš„å®Œæ•´å†…å­˜çŠ¶æ€ï¼›

**æ³¨æ„ä¸€ç‚¹**ï¼Œè·Ÿå‰é¢çš„â€œSendKafkaMsg_InfoToC()å’ŒSendKafkaMsg_SaveOffset()å¯ä»¥ä½œä¸ºä¸€ä¸ªäº‹åŠ¡ä¸€èµ·æäº¤â€ä¸åŒï¼Œ
SendKafkaMsg_SaveMemory()ä¸èƒ½è·ŸSendKafkaMsg_SaveOffset()ä¸ºä¸€ä¸ªäº‹åŠ¡ä¸€èµ·æäº¤ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªæ˜¯æœ‰å‰åä¾èµ–çš„ï¼Œå› ä¸ºå¢é‡å¿«ç…§SendKafkaMsg_SaveOffset()è¿˜éœ€è¦ä¿å­˜å…¨é‡å¿«ç…§çš„ä½ç½®ä¿¡æ¯ï¼Œè€Œå…¨é‡å¿«ç…§SendKafkaMsg_SaveMemory()
æœ¬èº«å‘é€ç»™Kafkaæ˜¯å¼‚æ­¥æ“ä½œï¼Œå›è°ƒä¸­æ‰èƒ½æ‹¿åˆ°è‡ªå·±çš„ä½ç½®ä¿¡æ¯ï¼Œæ‰€ä»¥æ— æ³•ä½œä¸ºåŒä¸€ä¸ªäº‹åŠ¡ä¸€èµ·æäº¤ï¼Œæ‰€ä»¥åªèƒ½ç­‰å¼‚æ­¥å›è°ƒä¹‹åæ‹¿åˆ°å…¨é‡çš„ä½ç½®å†å­˜åˆ°å¢é‡å¿«ç…§SendKafkaMsg_SaveOffset()æˆ–è€…å¦ä¸€ç§åšæ³•æ˜¯å°†å…¨é‡å¿«ç…§çš„ä½ç½®å…ˆå­˜åœ¨å†…å­˜ä¸­ï¼Œå¢é‡å¿«ç…§SendKafkaMsg_SaveOffset()åœ¨ä¸‹ä¸€æ¬¡æäº¤ï¼Œ
å³ä½¿æŒ‚æ‰ä¹Ÿå½±å“ä¸å¤§ï¼Œå¤§ä¸äº†ä»å¤´å¼€å§‹æˆ–è€…ä»æ›´æ—©çš„å…¨é‡å¿«ç…§å¼€å§‹æ¢å¤ï¼›

å› æ­¤å°±å¯ä»¥ç†è§£ä¸‹å›¾ï¼š

![](/docs/docs_image/software/buildingblock/kafka/kafka25.png)

<disqus/>