---
sidebar: auto
sidebarDepth: 3
footer: MIT Licensed | Copyright Â© 2018-LIU YUE
---

[Apache KafkaConsumer](https://kafka.apache.org/23/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html)

## 1.å…³é”®æ¦‚å¿µ

keyword: heartbeatï¼Œrebalance

### Offsets

![](/docs/docs_image/software/buildingblock/kafka/kafka_offsets-1.png)

å›¾ä¸­last committed logæ˜¯æŒ‡consumeræ¶ˆè´¹å®Œä¹‹åï¼Œè‡ªåŠ¨æäº¤çš„offsetï¼ˆ"then each **later rebalance** will reset the position to the last committed offset"ï¼‰

 high waterMarkå’Œlog end offsetæ˜¯ä¸Šæ¸¸producerå‘å¸ƒçš„æ¶ˆæ¯offsetï¼Œå…¶ä¸­high watermarkæ˜¯ä»£è¡¨å…¨éƒ¨replicateç»“æŸ+1ï¼Œæ‰€ä»¥consumeræœ€å¤šèƒ½è¯»å–åˆ°high watermarkä½ç½®-1ï¼Œ

+ HWM high watermark 
  the offset of the last successfully replicated message plus one)
+ LEO Log End Offset
  å½“å‰æ—¥å¿—æ–‡ä»¶ä¸­ä¸‹ä¸€æ¡å¾…å†™å…¥æ¶ˆæ¯çš„offsetã€‚åˆ†åŒºISRé›†åˆä¸­çš„æ¯ä¸ªå‰¯æœ¬éƒ½ä¼šç»´æŠ¤è‡ªèº«çš„LEOï¼Œè€ŒISRé›†åˆä¸­æœ€å°çš„LEOå³ä¸ºåˆ†åŒºçš„HWMã€‚
  è¿™ä¸ªoffsetæœªå¿…åœ¨ç¡¬ç›˜ä¸­ï¼Œå¯èƒ½ç›®å‰åªåœ¨å†…å­˜ä¸­è¿˜æ²¡æœ‰è¢«flushåˆ°ç¡¬ç›˜ã€‚

+ LWM Low Watermarkçš„
  ä»£è¡¨ARé›†åˆ(åˆ†åŒºä¸­çš„æ‰€æœ‰å‰¯æœ¬ç»Ÿç§°ä¸º Assigned Replicas)ä¸­æœ€å°çš„logStartOffsetå€¼ã€‚
  ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ—¥å¿—æ–‡ä»¶çš„èµ·å§‹åç§»é‡ logStartOffset ç­‰äºç¬¬ä¸€ä¸ªæ—¥å¿—åˆ†æ®µçš„ baseOffsetï¼Œä½†è¿™å¹¶ä¸æ˜¯ç»å¯¹çš„ï¼Œæ—§æ—¥å¿—çš„æ¸…ç†å’Œæ¶ˆæ¯åˆ é™¤éƒ½æœ‰å¯èƒ½ä¿ƒä½¿LWçš„å¢é•¿ã€‚

+ LSO Last Stable Offsetï¼Œ
  å®ƒä¸kafka äº‹åŠ¡Transactional produeræœ‰å…³ã€‚å¯¹äºæœªå®Œæˆçš„äº‹åŠ¡è€Œè¨€ï¼ŒLSOçš„å€¼ç­‰äºäº‹åŠ¡ä¸­çš„ç¬¬ä¸€æ¡æ¶ˆæ¯æ‰€åœ¨çš„ä½ç½®ï¼ˆfirstUnstableOffsetï¼‰ï¼›å¯¹äºå·²ç»å®Œæˆçš„äº‹åŠ¡è€Œè¨€ï¼Œå®ƒçš„å€¼ç­‰åŒäºHWMç›¸åŒã€‚

  Kafkaçš„ä¸€ä¸ªæ¶ˆè´¹ç«¯çš„å‚æ•°â€”â€”isolation.levelï¼Œè¿™ä¸ªå‚æ•°ç”¨æ¥é…ç½®æ¶ˆè´¹è€…çš„äº‹åŠ¡éš”ç¦»çº§åˆ«ã€‚å­—ç¬¦ä¸²ç±»å‹ï¼Œæœ‰æ•ˆå€¼ä¸ºâ€œread_uncommittedâ€å’Œ â€œread_committedâ€ï¼Œè¡¨ç¤ºæ¶ˆè´¹è€…æ‰€æ¶ˆè´¹åˆ°çš„ä½ç½®ï¼Œå¦‚æœè®¾ç½®ä¸ºâ€œread_committedâ€ï¼Œé‚£ä¹ˆæ¶ˆè´¹è€…å°±ä¼šå¿½ç•¥äº‹åŠ¡æœªæäº¤çš„æ¶ˆæ¯ï¼Œå³åªèƒ½æ¶ˆè´¹åˆ° LSO(LastStableOffset)çš„ä½ç½®ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¸º â€œread_uncommittedâ€ï¼Œå³å¯ä»¥æ¶ˆè´¹åˆ° HWM(High Watermark)å¤„çš„ä½ç½®ã€‚æ³¨æ„ï¼šfollowerå‰¯æœ¬çš„äº‹åŠ¡éš”ç¦»çº§åˆ«ä¹Ÿä¸ºâ€œread_uncommittedâ€ï¼Œå¹¶ä¸”ä¸å¯ä¿®æ”¹ã€‚

  è¿™ä¸ªLSOè¿˜ä¼šå½±å“Kafkaæ¶ˆè´¹æ»åé‡ï¼ˆä¹Ÿå°±æ˜¯Kafka Lagï¼Œå¾ˆå¤šæ—¶å€™ä¹Ÿä¼šè¢«ç§°ä¹‹ä¸ºæ¶ˆæ¯å †ç§¯é‡ï¼‰çš„è®¡ç®—ï¼š

  a) å¦‚æœæ²¡æœ‰äº‹åŠ¡ Lag=HWM â€“ ConsumerOffset:
  ![](/docs/docs_image/software/buildingblock/kafka/kafka_offsets-2.png)

  b) å¦‚æœä¸ºæ¶ˆæ¯å¼•å…¥äº†äº‹åŠ¡:
  - å¦‚æœæ¶ˆè´¹è€…å®¢æˆ·ç«¯çš„ isolation.level å‚æ•°é…ç½®ä¸ºâ€œread_uncommittedâ€(é»˜è®¤)ï¼Œé‚£ä¹ˆ Lagçš„è®¡ç®—æ–¹å¼ä¸å—å½±å“ Lag=HWM â€“ ConsumerOffset
  - å¦‚æœè¿™ä¸ªå‚æ•°é…ç½®ä¸ºâ€œread_committedâ€ï¼Œé‚£ä¹ˆ Lag = LSO â€“ ConsumerOffset :
    i) å¯¹æœªå®Œæˆçš„äº‹åŠ¡è€Œè¨€ï¼ŒLSO çš„å€¼ç­‰äºäº‹åŠ¡ä¸­ç¬¬ä¸€æ¡æ¶ˆæ¯çš„ä½ç½® firstUnstableOffset
    ![](/docs/docs_image/software/buildingblock/kafka/kafka_offsets-3.png)
    ii) å¯¹å·²å®Œæˆçš„äº‹åŠ¡è€Œè¨€ï¼Œå®ƒçš„å€¼åŒ HWM ç›¸åŒ
    ![](/docs/docs_image/software/buildingblock/kafka/kafka_offsets-4.png)
    æ³¨æ„ï¼Œå›¾ä¸­çš„ControlBatchå³[Control message](/software/buildingblock/kafka_producer.md#2-å®¢æˆ·ç«¯å…³é”®apiåŠæºç è§£è¯»)

æŸ¥çœ‹ï¼š
```
./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list xxxx --time -1 --topic xxx
```

**why offsets increment by 2 instead of 1?**

å¯¹äº Transactional produceræ¥è¯´ï¼Œé™¤äº†å†™å…¥msgä¹‹å¤–ï¼Œè¿˜ä¼šå†™å…¥ abort/commit marker

[Finally writes the COMMITTED (or ABORTED) message to transaction log](/software/buildingblock/kafka_producer.md#_5-2-writetxnmarkerrequest)

å¤§å®¶çš„åæ§½ï¼š

[For this reason, I realize that latest_available_offset is a misleading name. The function should probably return an offset one less than what it currently returns.](https://github.com/Parsely/pykafka/issues/494)

[If you use transactions, each commit (or abort) of a transaction writes a commit (or abort) marker into the topic -- those transactional markers also "consume" one offset](https://stackoverflow.com/questions/54636524/kafka-streams-does-not-increment-offset-by-1-when-producing-to-topic)

[Each time you commit or abort a transaction, a commit/abort marker is written into the corresponding partitions and requires one offset in the log.](https://groups.google.com/g/confluent-platform/c/IQKd3BKgvYw)

### Consumer Groups

#### é€‰æ‹©æ¨¡å¼ï¼š

1)	Having consumers as part of the same consumer group means providing theâ€œcompeting consumersâ€ pattern with whom the messages from topic partitions are spread across the members of the group.

2)	Having consumers as part of different consumer groups means providing the â€œpublish/subscribeâ€ pattern where the messages from topic partitions are sent to all the consumers across the different groups.
- [Don't Use Apache Kafka Consumer Groups the Wrong Way!](https://dzone.com/articles/dont-use-apache-kafka-consumer-groups-the-wrong-wa)

#### çº¿ç¨‹å®‰å…¨ï¼š

You canâ€™t have multiple consumers that belong to the same group in one thread and you canâ€™t have multiple threads safely use the same consumer. One consumer per thread is the rule. To run multiple consumers in the same group in one application, you will need to run each in its own thread. It is useful to wrap the consumer logic in its own object and then use Javaâ€™s ExecutorService to start multiple threads each with its own consumer:
```
public class ConsumerLoop implements Runnable {
  private final KafkaConsumer<String, String> consumer;
  private final List<String> topics;
  private final int id;

  public ConsumerLoop(int id,
                      String groupId, 
                      List<String> topics) {
    this.id = id;
    this.topics = topics;
    Properties props = new Properties();
    props.put(â€œgroup.idâ€, groupId);
    props.put(â€œkey.deserializerâ€, StringDeserializer.class.getName());
    props.put(â€œvalue.deserializerâ€, StringDeserializer.class.getName());
    this.consumer = new KafkaConsumer<>(props);
  }
  
  @Override
  public void run() {
    try {
      consumer.subscribe(topics);

      while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Long.MAX_VALUE);
        for (ConsumerRecord<String, String> record : records) {
          Map<String, Object> data = new HashMap<>();
          data.put("partition", record.partition());
          data.put("offset", record.offset());
          data.put("value", record.value());
          System.out.println(this.id + ": " + data);
        }
      }
    } catch (WakeupException e) {
      // ignore for shutdown 
    } finally {
      consumer.close();
    }
  }

  public void shutdown() {
    consumer.wakeup();
  }
}

public static void main(String[] args) {  
  int numConsumers = 3;
  String groupId = "consumer-tutorial-group"
  List<String> topics = Arrays.asList("consumer-tutorial");
  ExecutorService executor = Executors.newFixedThreadPool(numConsumers);

  final List<ConsumerLoop> consumers = new ArrayList<>();
  for (int i = 0; i < numConsumers; i++) {
    ConsumerLoop consumer = new ConsumerLoop(i, topics);
    consumers.add(consumer);
    executor.submit(consumer);
  }

  Runtime.getRuntime().addShutdownHook(new Thread() {
    @Override
    public void run() {
      for (ConsumerLoop consumer : consumers) {
        consumer.shutdown();
      }
    }
  });
}
-- https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/
```

#### Consumer GroupCoordinator

> The way consumers maintain membership in a consumer group and ownership of the partitions assigned to them is by sending *heartbeats* to a Kafka broker designated as the *group coordinator* (this broker can be different for different consumer groups). As long as the consumer is sending heartbeats at regular intervals, it is assumed to be alive, well, and processing messages from its partitions. ~~Heartbeats are sent when the consumer polls (i.e., retrieves records) and when it commits records it has consumed.~~
> https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html

ç¡®å®šä¸€ä¸ª Consumer Group çš„ GroupCoordinator çš„ä½ç½®ï¼š

1. abs (GroupId.hashCode) % NumPartitionï¼ŒNumPartition å°±æ˜¯__consumer_offsets çš„åˆ†åŒºæ•°
2. è®¡ç®—ç»“æœè¡¨ç¤ºäº†__consumer_offsets çš„ä¸€ä¸ª partitionæ¯”å¦‚`__consumer_offsets-10`
3.  æ‰¾åˆ°è¯¥`__consumer_offsets-10` çš„ leader æ‰€åœ¨çš„ brokerå¦‚broker id=3ï¼Œå³è¯¥consumer groupçš„ GroupCoordinatorï¼Œ 
4. å½“è¯¥consumer groupçš„GroupCoordinatoræŒ‚æ‰æ—¶ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªbrokeræŒ‚æ‰åï¼Œå…¶ä»–borkersï¼ˆä¿å­˜æœ‰`__consumer_offsets-10`çš„replicaçš„èŠ‚ç‚¹ï¼‰ä¼šé€‰ä¸€ä¸ªbrokerå¦‚broker id=1ä½œä¸ºæ–°çš„`__consumer_offsets-10`çš„leaderï¼Œç„¶åè¯¥brokerä¼šload æœ¬æœºä¿å­˜çš„`__consumer_offsets-10`replicaåˆ°å†…å­˜ä¸­ï¼Œå®Œæˆåï¼ŒClientç«¯å°±ä¼šdiscoverè¯¥brokerä½œä¸ºæ–°çš„GroupCoordinator
5. å½“broker id=3æ¢å¤æ­£å¸¸åï¼Œä¼šæŠ¢å›broker id=1ä¹‹å‰æ¥ç®¡çš„`__consumer_offsets-10`ï¼Œé‡æ–°ä½œä¸ºè¯¥topicçš„leaderï¼Œç„¶åclientç«¯å°±é‡æ–°discover broker id=3ä½œä¸ºgroup coordinatorï¼Œè¿™ç§æŠ¢å›çš„æ–¹å¼å¯ä»¥ä¿è¯kafkaèŠ‚ç‚¹ä»»åŠ¡å‡è¡¡ï¼ˆæ³¨æ„ï¼Œbroker id=3æ¢å¤ä¹‹åï¼Œé€šè¿‡kafka-topics.sh --list æŸ¥çœ‹ï¼Œ`__consumer_offsets-10`çš„leaderä»ç„¶ä¼šæ˜¯broker id 1ï¼Œéœ€è¦ç­‰åˆ°å†æ¥æ”¶ä¸€æ¡æ–°çš„kafkaæ¶ˆæ¯åï¼Œleaderæ‰ä¼šåˆ‡æ¢æˆbroker id 3ï¼Œå¤–éƒ¨topicä¹Ÿæ˜¯å¦‚æ­¤ï¼Œ`__transaction_state`ä¹Ÿæ˜¯ç±»ä¼¼ï¼Œå¯èƒ½æ˜¯ç”Ÿäº§ä¸€æ¡æ¶ˆæ¯æ—¶æ›´æ–°ï¼‰

## 2.å…³é”®é…ç½®

### Consumer Client Config

##### auto.create.topics.enable

Enable auto creation of topic on the server
Type:	boolean
Default:	true
Valid Values:	
Importance:	high
Update Mode:	read-only

##### request.timeout.ms & retries
request.timeout.ms:
The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.

Type:	int
Default:	30000 (30 seconds)

retries:
Setting a value greater than zero will cause the client to resend any request that fails with a potentially transient error. It is recommended to set the value to either zero or `MAX_VALUE` and use corresponding timeout parameters to control how long a client should retry a request.

Type:	int
Default:	0
Valid Values:	[0,...,2147483647]
Importance:	low

##### max.poll.interval.ms vs session.timeout.ms
max.poll.interval.ms:
The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. For consumers using a non-null group.instance.id which reach this timeout, partitions will not be immediately reassigned. Instead, the consumer will stop sending heartbeats and partitions will be reassigned after expiration of session.timeout.ms. This mirrors the behavior of a static consumer which has shutdown.

Type:	int
Default:	300000 (5 minutes)
Valid Values:	[1,...]
Importance:	medium

session.timeout.ms:
The timeout used to detect client failures when using Kafka's group management facility. The client sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this client from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.

Type:	int
Default:	45000 (45 seconds)
Valid Values:	
Importance:	high

> Before KIP-62, there is only session.timeout.ms (ie, Kafka 0.10.0 and earlier). max.poll.interval.ms is introduced via KIP-62 (part of Kafka 0.10.1).
> KIP-62, decouples heartbeats from calls to poll() via a background heartbeat thread, allowing for a longer processing time (ie, time between two consecutive poll()) than heartbeat interval.
> Assume processing a message takes 1 minute. If heartbeat and poll are coupled (ie, before KIP-62), you will need to set session.timeout.ms larger than 1 minute to prevent consumer to time out. However, if a consumer dies, it also takes longer than 1 minute to detect the failed consumer.
> KIP-62 decouples polling and heartbeat allowing to send heartbeats between two consecutive polls. Now you have two threads running, the heartbeat thread and the processing thread and thus, KIP-62 introduced a timeout for each. session.timeout.ms is for the heartbeat thread while max.poll.interval.ms is for the processing thread.
> Assume, you set session.timeout.ms=30000, thus, the consumer heartbeat thread must sent a heartbeat to the broker before this time expires. On the other hand, if processing of a single message takes 1 minutes, you can set max.poll.interval.ms larger than one minute to give the processing thread more time to process a message.
> If the processing thread dies, it takes max.poll.interval.ms to detect this. However, if the whole consumer dies (and a dying processing thread most likely crashes the whole consumer including the heartbeat thread), it takes only session.timeout.ms to detect it.
> The idea is, to allow for a quick detection of a failing consumer even if processing itself takes quite long.
> https://stackoverflow.com/questions/39730126/difference-between-session-timeout-ms-and-max-poll-interval-ms-for-kafka-0-10

##### scheduled.rebalance.max.delay.ms
The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned

Type:	int
Default:	300000 (5 minutes)

##### session.timeout.ms
 After every rebalance, all members of the current generation begin sending periodic heartbeats to the group coordinator. As long as the coordinator continues receiving heartbeats, it assumes that members are healthy. On every received heartbeat, the coordinator starts (or resets) a timer. If no heartbeat is received when the timer expires, the coordinator marks the member dead and signals the rest of the group that they should rejoin so that partitions can be reassigned. The duration of the timer is known as the session timeout and is configured on the client with the setting session.timeout.ms. 
  The only problem with this is that a spurious rebalance might be triggered if the consumer takes longer than the session timeout to process messages. You should therefore set the session timeout large enough to make this unlikely. The default is 30 seconds, but itâ€™s not unreasonable to set it as high as several minutes. The only downside of a larger session timeout is that it will take longer for the coordinator to detect genuine consumer crashes.

##### metadata.max.age.ms
ä¸æ˜¯è®¢é˜…æŸä¸ªtopic è€Œæ˜¯è®¢é˜…æŸç§pattern
repeat subscribe()
[Kafka pattern subscription. Rebalancing is not being triggered on new topic](https://stackoverflow.com/questions/38754865/kafka-pattern-subscription-rebalancing-is-not-being-triggered-on-new-topic/66758840#66758840)

### Broker Config
to do

## 3.å®¢æˆ·ç«¯å…³é”®APIåŠæºç è§£è¯»

### poll
[public ConsumerRecords<K,V> poll(long timeoutMs)](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll-long-)

The poll API returns fetched records based on the current position. 

> pollçš„è¡Œä¸ºï¼š

**On each poll, consumer will try to use the last consumed offset as the starting offset and fetch sequentially**. The last consumed offset can be manually set through [`seek(TopicPartition, long)`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#seek-org.apache.kafka.common.TopicPartition-long-) or automatically set as the last committed offset for the subscribed list of partitions å³å¦‚æœä¸æ˜¾ç¤ºè°ƒç”¨ seekæ¥è®¾ç½®å…¶ä½ç½®ï¼Œå°†ä¼šè‡ªåŠ¨ä½¿ç”¨interal offsetæ¥å®šä½å…¶æœ€åä¸€æ¬¡æ¶ˆè´¹çš„ä½ç½®ã€‚

> æ›´å®Œæ•´çš„ï¼š

When the group is **first created**, the position will be set according to the reset policy (which is typically either set to the earliest or latest offset for each partition defined by the auto.offset.reset). Once the consumer begins committing offsets, then each **later rebalance** will reset the position to the last committed offset. The parameter passed to poll controls the maximum amount of time that the consumer will block while it awaits records at the current position. The consumer returns immediately as soon as any records are available, but it will wait for the full timeout specified before returning if nothing is available.

> å…·ä½“çš„ï¼š

_ç¬¬ä¸€æ¬¡ï¼ˆè§¦å‘reblanceï¼‰pollçš„è¡Œä¸ºï¼š_

The poll loop does a lot more than just get data. The first time you call poll() with a new consumer, it is responsible for finding the GroupCoordinator, joining the consumer group, and receiving a partition assignment.[æ³¨æ„ï¼šåªæ˜¯subscribe topicå¹¶ä¸èƒ½ç«‹å³å¼•å‘rebalanceï¼Œå¯ä»¥åœ¨subscribeä¹‹åç¬¬ä¸€æ¬¡pollï¼Œä»è€Œç«‹å³å¼•å‘rebalance] If a rebalance is triggered, it will be handled inside the poll loop as well. ~~And of course the heartbeats that keep consumers alive are sent from within the poll loop. For this reason, we try to make sure that whatever processing we do between iterations is fast and efficient.~~
KIP-62, decouples heartbeats from calls to poll() via a background heartbeat thread, allowing for a longer processing time (ie, time between two consecutive poll()) than heartbeat interval.

_ç¬¬äºŒæ¬¡ä¹‹åpollçš„è¡Œä¸ºï¼š_

ä»ä¸Šä¸€æ¬¡çš„fetch positionsç»§ç»­å¾€ä¸‹æ‹‰å–

```
private ConsumerRecords<K, V> poll(final Timer timer, final boolean includeMetadataInTimeout) {
    acquireAndEnsureOpen();
    try {
        if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) {
            throw new IllegalStateException("Consumer is not subscribed to any topics or assigned any partitions");
        }

        do {
            client.maybeTriggerWakeup();

            if (includeMetadataInTimeout) {
                if (!updateAssignmentMetadataIfNeeded(timer)) {
                    return ConsumerRecords.empty();
                }
            } else {
                while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {
                    log.warn("Still waiting for metadata");
                }
            }

            final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);
            if (!records.isEmpty()) {
                if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {
                    client.pollNoWakeup();
                }

                return this.interceptors.onConsume(new ConsumerRecords<>(records));
            }
        } while (timer.notExpired());

        return ConsumerRecords.empty();
    } finally {
        release();
    }
}

boolean updateAssignmentMetadataIfNeeded(final Timer timer) {
    if (coordinator != null && !coordinator.poll(timer)) {
        return false;
    }

    return updateFetchPositions(timer);
}

1.Polling coordinator for updates â€” ensure weâ€™re up-to-date with our groupâ€™s coordinator.
2.Updating fetch positions â€” ensure every partition assigned to this consumer has a fetch position. If it is missing then consumer uses auto.offset.reset value to set it (set it to earliest, latest or throw exception).

```

Kafka maintains a numerical offset for each record in a partition. This offset acts as a unique identifier of a record within that partition, and also denotes the position of the consumer in the partition. For example, a consumer which is at position 5 has consumed records with offsets 0 through 4 and will next receive the record with offset 5. There are actually two notions of position relevant to the user of the consumer:

+ The position of the consumer gives **the offset of the next record** that will be given out. It will be one larger than the highest offset the consumer has seen in that partition. It automatically advances every time the consumer receives messages in a call to poll(Duration).
  The position of the consumer == consumed position == current position

+ The committed position is the last offset that has been stored securely. Should the process fail and restart, this is the offset that the consumer will recover to. The consumer can either automatically commit offsets periodically; or it can choose to control this committed position manually by calling one of the commit APIs (e.g. commitSync and commitAsync).
  The committed position == last committed offset

This distinction gives the consumer control over when a record is considered consumed. 

### endoffsets

* Get the end offsets for the given partitions. In the default {@code read_uncommitted} isolation level, the end
  * offset is the high watermark (that is, the offset of the last successfully replicated message plus one). For
  * {@code read_committed} consumers, the end offset is the last stable offset (LSO), which is the minimum of
  * the high watermark and the smallest offset of any open transaction. Finally, if the partition has never been
  * written to, the end offset is 0.

**How to get the msg of lastoffset?**

Solution: seek(endOffsets()-n) then poll, å¯¹äºæ™®é€šçš„æ¶ˆæ¯n=1ï¼Œä½†æ˜¯æ¶‰åŠåˆ°äº‹åŠ¡ transactional msg n=2

### ConsumerRebalanceListener 

onPartitionsRevoked && onPartitionsAssigned

> It is guaranteed that all the processes in a consumer group will execute their [`onPartitionsRevoked(Collection)`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html#onPartitionsRevoked-java.util.Collection-) callback before any instance executes its [`onPartitionsAssigned(Collection)`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html#onPartitionsAssigned-java.util.Collection-) callback. 

**<=kafka2.3 Stop the World:**

å‘ç”Ÿrebalanceæ—¶ï¼Œkafkaä¼šä¿è¯æ‰€æœ‰ä¹‹å‰çš„consumeræ— æ³•ç»§ç»­æ¶ˆè´¹æ¶ˆæ¯ï¼ˆè¿heartbeatéƒ½åœæ­¢äº†ï¼Œæç¤ºæ¶ˆæ¯ Attempt to heartbeat failed since group is rebalancingï¼‰ï¼Œç„¶åä¼šå…ˆé€šè¿‡ onPartitionsRevoked å›è°ƒæ‰€æœ‰çš„consumerï¼Œå¾…æ‰€æœ‰consumerçš„onPartitionsRevokedå®Œæˆä¹‹åï¼Œæ‰ä¼šç»§ç»­å›è°ƒonPartitionsAssignedï¼ˆç¬”è€…æµ‹è¯•åˆ°ä¸€ç§æƒ…å†µï¼Œå°±æ˜¯consumergroupæœ‰æœåŠ¡Aå’ŒBï¼ŒAå› ä¸ºç½‘ç»œé—®é¢˜ï¼Œå¯¼è‡´kafkaé›†ç¾¤å†³å®šå°†æ‰€æœ‰partitionåˆ†é…ç»™Bï¼Œæ‰€ä»¥kafkaé›†ç¾¤å‘é€revokeç»™Aå’ŒBï¼Œå› ä¸ºAæœ‰ç½‘ç»œé—®é¢˜ï¼ŒBå°±æ²¡æœ‰ç­‰å¾…Aå®Œæˆrevokeï¼Œç›´æ¥å¯åŠ¨äº†ï¼Œè€Œè¿‡äº†ä¸¤åˆ†é’Ÿï¼ŒAæ‰æ”¶åˆ°kafkaé›†ç¾¤çš„æ¶ˆæ¯ï¼Œåé¢exactly onceç¬”è€…ç»™å‡ºäº†åœºæ™¯å›¾ç¤ºï¼‰

**>=kafka2.4 Incremental rebalance**

## 4. Exactly-Once åœºæ™¯åˆ†æ

**ç†è§£è§’åº¦ï¼š**

+ kafkaæœ¬èº«çš„ä¿è¯
  - nothing to guarantee
  - at-least-once Messages are never lost but may be redelivered.
  - at-most-once Messages may be lost but are never redelivered.
  - exactly-once This is what people actually want, each message is delivered once and only once.

+ åŸºäºkafkaçš„ç”¨æˆ·ä»£ç çš„ä¿è¯
  ä¸šåŠ¡å¤„ç†é€»è¾‘å’Œoffsetç»´æŠ¤é€»è¾‘

**å…·ä½“åœºæ™¯ï¼š**

ä¸Šæ¸¸(consume topic 1-transform-produce to topic 2)->ä¸‹æ¸¸(consume topic 2....)

**ç›®æ ‡ï¼š**

å¯¹ä¸Šæ¸¸å’Œä¸‹æ¸¸éƒ½å®ç° atomic-read-process-write

### 4.1 ä¸Šæ¸¸(consume topic 1) -ä¾èµ–consumer internal offset
å…ˆæ¥çœ‹æ¯”è¾ƒç®€å•çš„åœºæ™¯å°±æ˜¯åªæœ‰ consumer topicï¼Œä¸é€šè¿‡ seekæ¥è®¾ç½®ä½ç½®ç›´æ¥pollï¼Œè‡ªåŠ¨ä½¿ç”¨interal offsetæ¥å®šä½å…¶æœ€åä¸€æ¬¡æ¶ˆè´¹çš„ä½ç½®ï¼Œæ³¨æ„ä¸‹é¢çš„ä¸¤ä¸ªä½¿ç”¨æ–¹æ³• at-least-once è‡³å°‘ä¸€æ¬¡å½“ç„¶å¯èƒ½ä¼šé‡å¤æ¶ˆè´¹ï¼Œ**ä½†æ˜¯ä¹Ÿå¯èƒ½ä¸¢å¤±ä¿¡æ¯**

#### 4.1.1 è‡ªåŠ¨æäº¤offsetï¼Œat-least-once

Setting `enable.auto.commit` **means that offsets are committed automatically with a frequency controlled by** the config `auto.commit.interval.ms`. 

```
  Properties props = new Properties();
     props.put("bootstrap.servers", "localhost:9092");
     props.put("group.id", "test");
     props.put("enable.auto.commit", "true");
     props.put("auto.commit.interval.ms", "1000");
     props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
     props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
     KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
     consumer.subscribe(Arrays.asList("foo", "bar"));
     while (true) {
         ConsumerRecords<String, String> records = consumer.poll(100);
         for (ConsumerRecord<String, String> record : records)
             System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
     }
```

When a partition gets reassigned to another consumer in the group, the initial position is set to the last committed offset. If the consumer in the example above suddenly crashed, then the group member taking over the partition would begin consumption from offset 1. In that case, it would have to reprocess the messages up to the crashed consumerâ€™s position of 6.

The diagram also shows two other significant positions in the log. The log end offset is the offset of the last message written to the log. The high watermark is the offset of the last message that was successfully copied to all of the logâ€™s replicas. From the perspective of the consumer, the main thing to know is that you can only read up to the high watermark. This prevents the consumer from reading unreplicated data which could later be lost.

#### 4.1.2 æ‰‹åŠ¨æäº¤offsetï¼Œat-least-once

Instead of relying on the consumer to periodically commit consumed  offsets, users can also control when records should be considered as consumed and hence commit their offsets. This is useful when the consumption of the messages is coupled with some processing logic and hence a message should not be considered as consumed until it is completed processing.  

```
 Properties props = new Properties();
     props.put("bootstrap.servers", "localhost:9092");
     props.put("group.id", "test");
     props.put("enable.auto.commit", "false");
     props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
     props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
     KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
     consumer.subscribe(Arrays.asList("foo", "bar"));
     final int minBatchSize = 200;
     List<ConsumerRecord<String, String>> buffer = new ArrayList<>();
     while (true) {
         ConsumerRecords<String, String> records = consumer.poll(100);
         for (ConsumerRecord<String, String> record : records) {
             buffer.add(record);
         }
         if (buffer.size() >= minBatchSize) {
             insertIntoDb(buffer);
             consumer.commitSync();
             buffer.clear();
         }
     }
```

In this example we will consume a batch of records and batch them up in memory. When we have enough records batched, we will insert them into a database. If we allowed offsets to auto commit as in the previous example, records would be considered consumed after they were returned to the user in [`poll`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll-long-). It would then be possible for our process to fail after batching the records, but before they had been inserted into the database. 

To avoid this, we will manually commit the offsets only after the corresponding records have been inserted into the database. This gives us exact control of when a record is considered consumed. This raises the opposite possibility: the process could fail in the interval after the insert into the database but before the commit (even though this would likely just be a few milliseconds, it is a possibility). In this case the process that took over consumption would consume from last committed offset and would repeat the insert of the last batch of data. Used in this way Kafka provides what is often called "at-least-once" delivery guarantees, as each record will likely be delivered one time but in failure cases could be duplicated. 

**ä¸Šé¢at-least-once ä¹Ÿä¸æ˜¯ç»å¯¹çš„ï¼Œä¹Ÿå¯èƒ½ä¸¢æ•°æ®(nothing to guarantee)ï¼š**

**Note: Using automatic offset commits can also give you "at-least-once" delivery, but the requirement is that you must consume all data returned from each call to [`poll(long)`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll-long-) before any subsequent calls, or before [`closing`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#close--) the consumer. If you fail to do either of these, it is possible for the committed offset to get ahead of the consumed position, which results in missing records. The advantage of using manual offset control is that you have direct control over when a record is considered "consumed."**

The above example uses [`commitSync`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#commitSync--) to mark all received records as committed. In some cases you may wish to have even finer control over which records have been committed by specifying an offset explicitly. In the example below we commit offset after we finish handling the records in each partition. 

```
     try {
         while(running) {
             ConsumerRecords<String, String> records = consumer.poll(Long.MAX_VALUE);
             for (TopicPartition partition : records.partitions()) {
                 List<ConsumerRecord<String, String>> partitionRecords = records.records(partition);
                 for (ConsumerRecord<String, String> record : partitionRecords) {
                     System.out.println(record.offset() + ": " + record.value());
                 }
                 long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();
                 consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)));
             }
         }
     } finally {
       consumer.close();
     }
Note: The committed offset should always be the offset of the next message that your application will read. Thus, when calling commitSync(offsets) you should add one to the offset of the last message processed. 
```

### 4.2 ä¸Šæ¸¸(consume topic 1-transform-produce to topic 2) - æ‰‹åŠ¨æäº¤/produceræäº¤ at-most-once

**idempotent producer:**

Prior to 0.11.0.0, if a producer failed to receive a response indicating that a message was committed, it had little choice but to resend the message. This provides at-least-once delivery semantics since the message may be written to the log again during resending if the original request had in fact succeeded. Since 0.11.0.0, the Kafka producer also supports an idempotent delivery option which guarantees that resending will not result in duplicate entries in the log. To achieve this, the broker assigns each producer an ID and deduplicates messages using a sequence number that is sent by the producer along with every message. 

### 4.3 ä¸Šæ¸¸(consume topic 1-transform-produce to topic 2) - æ‰‹åŠ¨æäº¤/produceræäº¤ exactly-once

Also beginning with 0.11.0.0, the producer supports the ability to send messages to multiple topic partitions using transaction-like semantics: i.e. either all messages are successfully written or none of them are. The main use case for this is exactly-once processing between Kafka topics

æ¥ç€çœ‹ä¸Šæ¸¸æ¯”è¾ƒå®Œæ•´çš„ consumer-transform-produce åœºæ™¯

#### 4.3.1 ä¾èµ– interal offset,exactly-once

**é‡ç‚¹ï¼š**
[å‰é¢çš„"ä¸Šæ¸¸(consume topic 1) -ä¾èµ–internal offset"](#1-ä¸Šæ¸¸consume-topic-1--ä¾èµ–internal-offset) æ˜¯ä¾èµ– consumeræäº¤offsetï¼Œè€Œå¯¹äºatomic-read-process-writeéœ€è¦Produceræäº¤offsetï¼Œ[Producer#sendOffsetsToTransaction](https://stackoverflow.com/questions/45195010/meaning-of-sendoffsetstotransaction-in-kafka-0-11)

å‚è€ƒ[æ·±å…¥Exactly-Onceè§£æ](/docs/software/buildingblock/kafka) 

example 1ï¼š

```
public class KafkaTransactionsExample {
  
  public static void main(String args[]) {
    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);
 
 
    // Note that the â€˜transactional.idâ€™ configuration _must_ be specified in the
    // producer config in order to use transactions.
    KafkaProducer<String, String> producer = new KafkaProducer<>(producerConfig);
 
    // We need to initialize transactions once per producer instance. To use transactions,
    // it is assumed that the application id is specified in the config with the key
    // transactional.id.
    //
    // This method will recover or abort transactions initiated by previous instances of a
    // producer with the same app id. Any other transactional messages will report an error
    // if initialization was not performed.
    //
    // The response indicates success or failure. Some failures are irrecoverable and will
    // require a new producer  instance. See the documentation for TransactionMetadata for a
    // list of error codes.
    producer.initTransactions();
     
    while(true) {
      ConsumerRecords<String, String> records = consumer.poll(CONSUMER_POLL_TIMEOUT);
      if (!records.isEmpty()) {
        // Start a new transaction. This will begin the process of batching the consumed
        // records as well
        // as an records produced as a result of processing the input records.
        //
        // We need to check the response to make sure that this producer is able to initiate
        // a new transaction.
        producer.beginTransaction();
         
        // Process the input records and send them to the output topic(s).
        List<ProducerRecord<String, String>> outputRecords = processRecords(records);
        for (ProducerRecord<String, String> outputRecord : outputRecords) {
          producer.send(outputRecord);
        }
         
        // To ensure that the consumed and produced messages are batched, we need to commit
        // the offsets through
        // the producer and not the consumer.
        //
        // If this returns an error, we should abort the transaction.
         
        sendOffsetsResult = producer.sendOffsetsToTransaction(getUncommittedOffsets());
         
      
        // Now that we have consumed, processed, and produced a batch of messages, let's
        // commit the results.
        // If this does not report success, then the transaction will be rolled back.
        producer.endTransaction();
      }
    }
  }
}
```


example 2ï¼š

```
KafkaProducer producer = createKafkaProducer(
  â€œbootstrap.serversâ€, â€œlocalhost:9092â€,
  â€œtransactional.idâ€, â€œmy-transactional-idâ€);

producer.initTransactions();

KafkaConsumer consumer = createKafkaConsumer(
  â€œbootstrap.serversâ€, â€œlocalhost:9092â€,
  â€œgroup.idâ€, â€œmy-group-idâ€,
  "isolation.level", "read_committed");

consumer.subscribe(singleton(â€œinputTopicâ€));

while (true) {
  ConsumerRecords records = consumer.poll(Long.MAX_VALUE);
  producer.beginTransaction();
  for (ConsumerRecord record : records)
    producer.send(producerRecord(â€œoutputTopicâ€, record));
  producer.sendOffsetsToTransaction(currentOffsets(consumer), group);  
  producer.commitTransaction();
}
```

#### 4.3.2 ä¸ä¾èµ–interal offsetï¼Œè‡ªå·±ç»´æŠ¤offset exactly-once

The consumer application need not use Kafka's built-in offset storage, it can store offsets in a store of its own choosing, example usage:

+ If the results of the consumption are being stored in a relational database, storing the offset in the database as well can allow committing both the results and offset in a single transaction. Thus either the transaction will succeed and the offset will be updated based on what was consumed or the result will not be stored and the offset won't be updated. 
+ If the results are being stored in a local store it may be possible to store the offset there as well. For example a search index could be built by subscribing to a particular partition and storing both the offset and the indexed data together. If this is done in a way that is atomic, it is often possible to have it be the case that even if a crash occurs that causes unsync'd data to be lost, whatever is left has the corresponding offset stored as well. This means that in this case the indexing process that comes back having lost recent updates just resumes indexing from what it has ensuring that no updates are lost. 

æ¯”å¦‚å­˜å‚¨offsetåˆ°è‡ªå·±ç»´æŠ¤çš„ä¸€ä¸ªtopic T-SNP ä½œä¸ºå¢é‡æ•°æ®ä¸»é¢˜

æ¶ˆè´¹æ—¶ï¼š

Configure `enable.auto.commit=false` 

å› ä¸ºæ¯æ¡record éƒ½æºå¸¦å…¶offsetä¿¡æ¯æ ¹æ®æ¨¡å‹
![atomic-read-process-write](/docs/docs_image/software/buildingblock/kafka/kafka_exactly_once01.png)ï¼Œå°†writeå’Œmark readï¼ˆUse the offset provided with each [`ConsumerRecord`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html) to save your positionï¼‰ä½œä¸ºä¸€ä¸ªtransactionæäº¤ï¼›

å¯åŠ¨æˆ–â€œé‡å¯â€æ—¶ï¼š

åˆ™æ‰¾åˆ°æœ€åä¸€ä¸ªæ¶ˆæ¯ï¼Œå³å­˜å‚¨çš„æœ€åä¸€ä¸ªoffsetï¼Œæ–¹æ³•:

endOffsetsï¼ˆè¿”å›the offset of the upcoming message, i.e. the offset of the last available message + 1. æ‰€ä»¥-1å°±æ˜¯åˆ°äº†last available messageçš„ä½ç½®ï¼Œè¿˜è¦å†-1æ‰èƒ½å†åé¢pollåˆ°è¿™æ¡æ¶ˆæ¯ï¼‰ --> assign ---> seekï¼ˆä¸èƒ½ç”¨seekToEndï¼Œç”¨äº†åˆ™pollä¸åˆ°ä»»ä½•æ¶ˆæ¯ï¼Œåªèƒ½ç­‰å¾…æ–°æ¶ˆæ¯ï¼‰ ---> poll

ï¼Œç„¶åé€šè¿‡è·å–çš„offsetå®šä½æ¢å¤restoreåˆ°ä¸Šä¸€æ¬¡è¿™ä¸ªtopicçš„positionå¤„ç†ä½ç½® [`seek(TopicPartition, long)`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#seek-org.apache.kafka.common.TopicPartition-long-)ï¼Œç„¶åå†poll

æ³¨æ„ï¼š

+ å¦‚æœæƒ³è¦zombie fenceç”Ÿæ•ˆï¼Œé™¤äº†ç”¨å¯¹transaction.idï¼Œè¿™ä¸ªé¡ºåºä¹Ÿå¾ˆé‡è¦ï¼Œè¦å…ˆå»initTransactionæ³¨å†Œ transaction.idï¼ˆå½¢è±¡çš„è¯´å°±æ˜¯äº‰å–åˆ°åˆæ³•èº«ä»½å…ˆï¼‰ï¼Œç„¶åæ‰æ˜¯å»restoreï¼ˆè¯»å–å¢é‡å¿«ç…§åšæ¢å¤ï¼‰ï¼Œå¦åˆ™ï¼Œå¦‚æœä½ å…ˆå»restoreï¼Œå†å»æ³¨å†Œï¼ˆåˆ›å»ºTransactional Producerå¹¶initTransactionsï¼‰ï¼Œæœ‰å¯èƒ½restoreçš„æ—¶å€™è¯»å–åˆ°å¢é‡å¿«ç…§æ˜¯ 1000ï¼Œæ—§çš„Producerä»ç„¶å¯ä»¥ç»§ç»­å†™å…¥kafkaï¼š
  - case 1ï¼šé‡åˆ°è¿‡ä¸€æ¬¡disruptor ringbufferåœ¨rebalanceä¹‹åç»§ç»­å·¥ä½œé‡å¤è½åº“çš„äº‹æƒ…ï¼Œè§£å†³åŠæ³•ï¼šonrevokeçš„æ—¶å€™å…³é—­producer
  - case 2ï¼šcase 1åªé€‚ç”¨äºå•å®ä¾‹æ¨¡å¼ï¼Œå¦‚æœæ˜¯å¤šæ´»æ¨¡å¼ï¼Œåœ¨ç½‘ç»œæŠ–åŠ¨çš„æ—¶é—´çª—å†…ï¼Œä»ç„¶æœ‰æœºä¼šäº§ç”Ÿè¯¥é—®é¢˜
ï¼Œå› æ­¤ç­‰åˆ° å»æ³¨å†Œï¼ˆäº‰å–åˆæ³•èº«ä»½ï¼‰çš„æ—¶å€™ï¼Œå¢é‡å¿«ç…§å¯èƒ½å·²ç»åˆ°äº†2000ï¼Œç„¶åå› ä¸ºä½ å…ˆåšçš„restoreï¼Œä½ ä¼šå®šä½åˆ°1000ï¼Œå°†1001å¼€å§‹çš„éƒ½å½“åšæ–°æ¶ˆæ¯ï¼š

æ¯ä¸ªconsumer groupä¸­çš„æœåŠ¡rebalanceçš„æ­£ç¡®å¯åŠ¨é¡ºåºåº”è¯¥æ˜¯ï¼š
  1. å…ˆæ ¹æ® kafkaåˆ†é…çš„partitionåˆ›å»ºå¥½workerï¼ˆä¸»è¦æ˜¯Transactional Producerï¼‰ï¼Œè¿™ä¸ªåšå®Œåï¼Œä¼šç«‹å³è®©fenceç”Ÿæ•ˆï¼Œä¸ç”¨å†æ‹…å¿ƒ å…¶ä»–æœåŠ¡ä¸Šä»åœ¨ç­‰å¾…shutdownçš„disruptorWorkerç»§ç»­æ¶ˆè´¹ringbufferçš„ç¼“å­˜æ¶ˆæ¯
  2. è¯»å–å¢é‡å¿«ç…§è¿›è¡Œrestoreï¼Œç”±äºç¬¬1æ­¥åšå®Œï¼Œæˆ‘ä»¬å®Œå…¨ç›¸ä¿¡kafkaå¯ä»¥å±¥è¡Œzombie fenceï¼Œæ‰€ä»¥è¿™é‡Œå¯ä»¥100%ç¡®å®šå¯ä»¥æ‹¿åˆ°å‡†ç¡®çš„ last offsetï¼Œä»è€Œå‡†ç¡®çš„æ¢å¤

+ æœ‰ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œè™½ç„¶æˆ‘ä»¬å¯åŠ¨æ—¶å¯ä»¥åˆ¤æ–­ï¼Œæ¯”å¦‚[0,1000]æ˜¯ä¹‹å‰å¤„ç†è¿‡çš„ï¼Œ1001å¼€å§‹æ˜¯æ–°çš„æ•°æ®ï¼Œä½†æ˜¯ä¸ºäº†ä½¿å†…å­˜æ¢å¤åˆ°ä¹‹å‰çš„çŠ¶æ€ï¼Œä»ç„¶éœ€è¦å¯¹[0,1000]è¿™ä¸ªåŒºé—´çš„æ•°æ®è¿›è¡Œè®¡ç®—ï¼ˆåªä¸è¿‡ä¸è¿›è¡Œä»»ä½•äº‹åŠ¡å¤„ç†æ¯”å¦‚è½æ•°æ®åº“ï¼Œåªæ˜¯å•çº¯ä¸ºäº†restore memoryï¼‰ï¼Œæ‰€ä»¥ä¸€ä¸ªæ”¹è¿›ç­–ç•¥å°±æ˜¯ï¼Œå¢åŠ å…¨é‡å¿«ç…§ï¼Œç³»ç»Ÿåœæ­¢ä¹‹å‰æˆ–å®šæœŸå°†å†…å­˜åºåˆ—åŒ–å­˜èµ·æ¥ï¼Œæ³¨æ„å­˜çš„æ—¶å€™åŒæ—¶å­˜ä¸‹å½“æ—¶çš„offsetï¼Œæ¯”å¦‚1000ï¼Œç„¶ååœ¨å¢é‡å¿«ç…§ä¸­è®°å½•ä¸‹è¿™ä¸ªå…¨é‡å¿«ç…§çš„ä½ç½®ï¼ˆå½“ç„¶è¿˜æœ‰æˆ‘ä»¬è¦ä¿å­˜çš„offsetï¼‰å³å¯ï¼Œç”±äºä¸ºäº†è®°å½•ä¸‹å…¨é‡å¿«ç…§çš„kafkaä½ç½®ï¼Œéœ€è¦ç­‰å¾…kafka sendçš„å›è°ƒï¼Œæ‰€ä»¥è®°å½•åˆ°å¢é‡å¿«ç…§æ²¡æœ‰åŠæ³•è·Ÿä¿å­˜å…¨é‡å¿«ç…§ä½œä¸ºä¸€ä¸ªäº‹åŠ¡å¤„ç†ï¼Œä¸è¿‡æ²¡å…³ç³»ï¼š

  æ¯”å¦‚ä¸»é¢˜T-TARGET ï¼Œç°åœ¨å¤„ç†åˆ°äº†offset=1000ï¼Œå†³å®šåšä¸€æ¬¡å…¨é‡å¿«ç…§ï¼Œæ­¤æ—¶å…¨é‡å¿«ç…§ä¸­ä¿å­˜ä¸‹å†…å­˜çŠ¶æ€å’Œstart offset=1000ï¼Œkafka sendå…¨é‡å¿«ç…§åˆ° T-QuanLiangä¸­ï¼Œç„¶ååœ¨callbackæ—¶ï¼Œå¯ä»¥è·å–åˆ°å…¨é‡å¿«ç…§åœ¨T-QuanLiangçš„ quanliang offsetæ¯”å¦‚=0ï¼ŒT-TARGETè¿›æ¥æ–°çš„æ¶ˆæ¯ï¼ˆæˆ–è€…ä¹‹å‰åšå…¨é‡å¿«ç…§çš„æŒ‡ä»¤æœ¬èº«å°±æ˜¯æ¡æ¶ˆæ¯ï¼‰ï¼Œç»§ç»­äº‹åŠ¡æ€§çš„è®°å½•å¢é‡å¿«ç…§ T-ZengLiangï¼Œæ­¤æ—¶æœ€æ–°è®°å½•çš„å¢é‡æ¶ˆæ¯çš„å†…å®¹æ˜¯ quanliang offset=0&&end offset=1001

  æ¢å¤çš„æ—¶å€™ï¼Œå…ˆ æ‰¾åˆ°T-ZengLiangæœ€åä¸€ä¸ªæ¶ˆæ¯ ï¼Œè·å–åˆ°quanliang offset=0&&end offset=1001ï¼Œç„¶åé€šè¿‡quanliang offset=0å»seek(T-QuanLiang, 0) æ‹¿åˆ° start offset=1000å’Œå½“æ—¶çš„å†…å­˜æ•°æ®ï¼Œä»è€Œæ¢å¤å†…å­˜æ•°æ®ï¼Œç„¶åä»1000å¼€å§‹(1000,1001],åªéœ€è¦é‡æ–°è®¡ç®—ä¸‹1001è¿™æ¡æ•°æ®æ›´æ–°ä¸‹å†…å­˜å³å¯ï¼Œä»1002å¼€å§‹å¾€åéƒ½æ˜¯æ–°çš„æ¶ˆæ¯


### 4.4 ä¸Šæ¸¸(produce to topic 2)->ä¸‹æ¸¸(consume topic 2) - isolation.level

**we can indicate with \*isolation.level\* that we should wait to read transactional messages until the associated transaction has been committed**:

```java
consumerProps.put("isolation.level", "read_committed");
```

åœ¨æ¶ˆè´¹ç«¯æœ‰ä¸€ä¸ªå‚æ•°isolation.levelï¼Œè®¾ç½®ä¸ºâ€œread_committedâ€ï¼Œè¡¨ç¤ºæ¶ˆè´¹ç«¯åº”ç”¨ä¸å¯ä»¥çœ‹åˆ°å°šæœªæäº¤çš„äº‹åŠ¡å†…çš„æ¶ˆæ¯ã€‚å¦‚æœç”Ÿäº§è€…å¼€å¯äº‹åŠ¡å¹¶å‘æŸä¸ªåˆ†åŒºå€¼å‘é€3æ¡æ¶ˆæ¯ msg1ã€msg2 å’Œ msg3ï¼Œåœ¨æ‰§è¡Œ commitTransaction() æˆ– abortTransaction()  æ–¹æ³•å‰ï¼Œè®¾ç½®ä¸ºâ€œread_committedâ€çš„æ¶ˆè´¹ç«¯åº”ç”¨æ˜¯æ¶ˆè´¹ä¸åˆ°è¿™äº›æ¶ˆæ¯çš„ï¼Œä¸è¿‡åœ¨ KafkaConsumer  å†…éƒ¨ä¼šç¼“å­˜è¿™äº›æ¶ˆæ¯ï¼Œç›´åˆ°ç”Ÿäº§è€…æ‰§è¡Œ commitTransaction() æ–¹æ³•ä¹‹åå®ƒæ‰èƒ½å°†è¿™äº›æ¶ˆæ¯æ¨é€ç»™æ¶ˆè´¹ç«¯åº”ç”¨ã€‚åä¹‹ï¼Œå¦‚æœç”Ÿäº§è€…æ‰§è¡Œäº†  abortTransaction() æ–¹æ³•ï¼Œé‚£ä¹ˆ KafkaConsumer ä¼šå°†è¿™äº›ç¼“å­˜çš„æ¶ˆæ¯ä¸¢å¼ƒè€Œä¸æ¨é€ç»™æ¶ˆè´¹ç«¯åº”ç”¨ã€‚

[kafka isolation level implications](https://stackoverflow.com/questions/56047968/kafka-isolation-level-implications)

## Troubleshooting

### ç½‘ç»œæ•…éšœ / kafkaé›†ç¾¤æœ‰èŠ‚ç‚¹æŒ‚æ‰ï¼ˆä¸æ˜¯æ­£å¸¸åœèŠ‚ç‚¹ï¼Œè€Œæ˜¯brokerèŠ‚ç‚¹æ‰€åœ¨æœåŠ¡å™¨ç½‘ç»œæ–­å¼€æˆ–æš´åŠ›åœæœºï¼‰

#### é€ æˆkafka clientç«¯ç¨‹åºè¯»å– metadata è¶…è¿‡é»˜è®¤ 30s æŠ›é”™

public java.util.Map<TopicPartition,java.lang.Long> endOffsets(java.util.Collection<TopicPartition> partitions)

TimeoutException - if the offset metadata could not be fetched before the amount of time allocated by **request.timeout.ms expires**

å¼‚å¸¸æ—¥å¿—åˆ†æï¼š
```
åœºæ™¯1ï¼š
ç°åœ¨devä¸Š3ä¸ªèŠ‚ç‚¹é…ç½® borker 0 1 2ï¼š
offsets.topic.replication.factor=3
min.insync.isr=2
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2
default.replication.factor=3
æ­£å¸¸æŒ‚æ‰1ä¸ªæ˜¯æ²¡é—®é¢˜çš„ï¼Œä½†æ˜¯å±…ç„¶æŒ‚æ‰2ä¸ªå±…ç„¶éƒ½èƒ½å¯åŠ¨clientç«¯ï¼ˆèƒ½æˆåŠŸjoin groupï¼ŒkafkaæˆåŠŸrebalanceï¼‰ï¼š
è¿™ä¸ªæ¯”è¾ƒè¯¡å¼‚ï¼Œå°±æ˜¯æµ‹è¯•çš„æ—¶å€™å…¶å®åªæœ‰ä¸¤ä¸ªæ´»ç€çš„èŠ‚ç‚¹ï¼Œbroker 0ä¸çŸ¥é“è¢«è°ç”¨rootæ›´æ”¹äº†å‡ ä¸ªkafka-logsæ–‡ä»¶æƒé™ï¼Œé€ æˆborker 0åœäº†ï¼Œ
ï¼Œç„¶åæµ‹è¯•æ–­å¼€broker 2çš„ç½‘ç»œï¼Œåªæœ‰broker 1ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹ï¼Œclientç«¯å±…ç„¶èƒ½å¤Ÿå¯åŠ¨å¹¶ä¸”è®¢é˜…topicï¼Œåªä¸è¿‡æ¢å¤çš„æ—¶å€™ï¼ˆonPartitionAssignå†…éƒ¨è¿›ä¸€æ­¥è®¿é—®kafkaè¯»å–å¿«ç…§æ•°æ®ï¼‰æŠ›é”™ï¼Œæ¯”å¦‚å¯èƒ½æ˜¯æ¢å¤çš„æ—¶å€™åˆ›å»ºä¸´æ—¶consumerï¼ŒkafkaæœåŠ¡ç«¯å‘zookeeperæ³¨å†Œçš„æ—¶å€™å‡ºç°è¶…æ—¶ï¼ˆä¸‹é¢æœ‰zookeeperé—®é¢˜çš„logï¼‰ï¼Œ
2022-03-12 18:33:29.793 [31mERROR[m [35m15732GG[m [TEST-MANAGER] [36mo.a.k.c.c.i.ConsumerCoordinator[m : [Consumer clientId=consumer-2, groupId=TEST-TRADEFRONT-SZL] User provided listener com.lyhistory.core.boot.SimpleWorkBalancer failed on partition assignment

com.lyhistory.core.exception.RecoveryException: Failed Recovery Worker
	at com.lyhistory.core.boot.SimpleWorkBalancer.onPartitionsAssigned(SimpleWorkBalancer.java:54)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:292)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:410)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:344)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:342)
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1226)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1191)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1176)
	at com.lyhistory.core.boot.SimpleWorkerManager.doServe(SimpleWorkerManager.java:96)
	at com.lyhistory.core.boot.AdministrableService.serve(AdministrableService.java:98)
	at com.lyhistory.core.boot.AdministrableService.start(AdministrableService.java:36)
	at com.lyhistory.core.boot.Starter$$Lambda$826/315805187.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.lyhistory.core.exception.RecoveryException: Failed Recovery Worker
	at com.lyhistory.core.boot.SimpleWorkBalancer.getWorkState(SimpleWorkBalancer.java:64)
	at com.lyhistory.core.boot.SimpleWorkBalancer$$Lambda$872/1231621690.apply(Unknown Source)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at com.lyhistory.core.boot.SimpleWorkBalancer.onPartitionsAssigned(SimpleWorkBalancer.java:52)
	... 12 more
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to get offsets by times in 30000ms
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.lyhistory.core.boot.SimpleWorkBalancer.getWorkState(SimpleWorkBalancer.java:62)
	... 22 more
Caused by: org.apache.kafka.common.errors.TimeoutException: Failed to get offsets by times in 30000ms

ä½†æ˜¯æ­£å¸¸ä¸åº”è¯¥èƒ½å¤Ÿè®¢é˜…topicï¼Œåº”è¯¥æŠ›å‡ºé”™è¯¯ç±»ä¼¼ï¼š
1 partitions have leader brokers without a matching listener ï¼ˆå› ä¸ºfactor=2ï¼ŒæŒ‚äº†2å°äº†è‚¯å®šæœ‰èŠ‚ç‚¹çš„leaderä¸åœ¨äº†ï¼‰
æˆ–è€…
insufficient isr (min.isr=2>live broker=1)
ä»è€Œå¯¼è‡´clientæ— æ³•join groupï¼š
Group coordinator XXXX:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery

çŒœæµ‹1ï¼š broker 0å¹¶éçœŸçš„æŒ‚ï¼Œåªæ˜¯å› ä¸ºkafka logæ–‡ä»¶æƒé™ä¸ºrootï¼Œé€ æˆkafkaæœåŠ¡å¤„äºå¼‚å¸¸çŠ¶æ€ï¼ˆè¿˜å¯ä»¥è·Ÿå…¶ä»–æœºå™¨æ²Ÿé€šï¼‰ï¼Œæ‰€ä»¥æ­¤æ—¶ä»ç„¶æ»¡è¶³min.isr=2çš„è¦æ±‚
å¦å®šï¼šæ ¹æ®broker 0ä¸Šé¢çš„æ—¥å¿—ï¼Œå¯ä»¥çœ‹åˆ°kafkaæ ¹æœ¬æ²¡æœ‰å¯¹åº”æ—¶é—´æ®µçš„ä»»ä½•æ—¥å¿—
çŒœæµ‹2: è™½ç„¶broker 2ç½‘ç»œæ–­å¼€ï¼Œæ­¤æ—¶ broker 2 å¯¹äº broker 1 æ¥è¯´å±äºå‡æ­»çŠ¶æ€ï¼Œå°šæœªæ›´æ–°metadataï¼Œæ‰€ä»¥æ²¡æœ‰æ£€æµ‹å‡ºinsufficient isr
éªŒè¯ï¼šå‘ç°æ–­ç½‘åå‡ºç°å¦‚ä¸‹é”™è¯¯ Opening socket connection to server sgkc2-devclr-v07/x.x.x.47:2181. Will not attempt to authenticate using SASL (unknown error)
æ¯”è¾ƒå¯é çš„çŒœæµ‹ï¼šçŒœæµ‹2åŸºæœ¬å¯¹çš„ï¼Œä¸è¿‡ä¸æ˜¯å‡æ­»ï¼Œè€Œæ˜¯æœ¬èº«å°±å·²ç»æˆäº†å­¤ç«‹èŠ‚ç‚¹ï¼Œåˆæ— æ³•ä¸broker 0å’Œ2çš„zookeeperé€šä¿¡æ›´æ–°ä¿¡æ¯ï¼Œä»è€Œé€ æˆkafkaæœåŠ¡ç«¯å¼‚å¸¸ï¼Œäº§ç”Ÿäº†ç»•è¿‡min.isré™åˆ¶çš„å‡è±¡
[2022-03-14 08:59:39,144] WARN Client session timed out, have not heard from server in 4002ms for sessionid 0x17862f2bedc0004 (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:39,145] INFO Client session timed out, have not heard from server in 4002ms for sessionid 0x17862f2bedc0004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:39,826] INFO Opening socket connection to server sgkc2-devclr-v07/x.x.x.47:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:41,829] WARN Client session timed out, have not heard from server in 2583ms for sessionid 0x17862f2bedc0004 (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:41,829] INFO Client session timed out, have not heard from server in 2583ms for sessionid 0x17862f2bedc0004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:42,820] INFO Opening socket connection to server sgkc2-devclr-v05/x.x.x.45:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2022-03-14 08:59:42,821] INFO Socket error occurred: sgkc2-devclr-v05/x.x.x.45:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2022-03-14 09:00:00,597] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=2095832195, epoch=8913816) to node 2: java.io.IOException: Connection to 2 was disconnected before the response was read. (org.apache.kafka.clients.FetchSessionHandler)
[2022-03-14 09:00:00,598] WARN [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=1, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=2095832195, epoch=8913816)) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 2 was disconnected before the response was read
        at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
        at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:100)
        at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:193)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:280)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:132)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:131)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
ã€‚ã€‚ã€‚
[2022-03-14 09:00:32,612] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=2095832195, epoch=INITIAL) to node 2: java.net.SocketTimeoutException: Failed to connect within 30000 ms. (org.apache.kafka.clients.FetchSessionHandler)
[2022-03-14 09:00:32,612] WARN [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=1, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={T-TRADE-CHK-0=(fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[0]), T-EOD-SNP-1=(fetchOffset=86, logStartOffset=86, maxBytes=1048576, currentLeaderEpoch=Optional[0]), T-DBMS-CHK-0=(fetchOffset=2002, logStartOffset=1992, maxBytes=1048576, currentLeaderEpoch=Optional[0]), T-CAPTURE-2=(fetchOffset=2002, logStartOffset=1992, maxBytes=1048576, currentLeaderEpoch=Optional[0])}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=2095832195, epoch=INITIAL)) (kafka.server.ReplicaFetcherThread)
java.net.SocketTimeoutException: Failed to connect within 30000 ms
        at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:96)
        at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:193)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:280)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:132)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:131)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)

åœºæ™¯2ï¼šç±»ä¼¼ä¸Šé¢åœºæ™¯
ç°åœ¨devä¸Š3ä¸ªèŠ‚ç‚¹é…ç½® borker 0 1 2ï¼š
offsets.topic.replication.factor=3 
min.insync.replicas=1 
transaction.state.log.replication.factor=3 
transaction.state.log.min.isr=1 
default.replication.factor=3 

åŒæ—¶æ–­å¼€äº†broker 0 å’Œ 2çš„ç½‘ç»œï¼Œç„¶åå¯åŠ¨kafka clientï¼Œæ­¤æ—¶kafka clientç«¯æ‰€åœ¨çš„ç½‘ç»œåˆ†åŒºå†…åªèƒ½è¿æ¥åˆ° borker 1ï¼ŒåŒæ ·å‡ºç°äº†åœºæ™¯1ä¸­çš„æƒ…å½¢ï¼Œ
å¯ä»¥æ­£å¸¸subscribe topicå¹¶ä¸”kafka serverç«¯borker 1æ­£å¸¸è¿›è¡Œäº†rebalanceï¼Œä½†æ˜¯æ¥ä¸‹æ¥clientç«¯åœ¨onPartitionAssignedå†…è¿›ä¸€æ­¥è®¿é—®kafkaè·å–ä¸€äº›å¿«ç…§æ•°æ®æ—¶äº§ç”Ÿè¶…æ—¶é”™è¯¯

ä¸è¿‡è·Ÿåœºæ™¯1ä¸å¤ªä¸€æ ·çš„æ˜¯ï¼Œåæ¥broker 2æ¢å¤äº†ï¼Œå¹¶ä¸”broker 1æˆåŠŸè¿æ¥äº†broker 2ï¼ˆä»¥åŠå…¶åŒä¸€æœºå™¨ä¸Šçš„zookeeperï¼‰ï¼Œåªä¸è¿‡è¿‡ç¨‹ä¸­fetchçš„æ—¶å€™å‡ºé”™
è™½ç„¶ 18:09:48,061 è¿æ¥ä¸Šäº†ï¼Œä½†æ˜¯åé¢fetch ioé”™è¯¯
[2022-03-12 18:10:07,087] INFO Opening socket connection to server vm-v01/x.x.x.x:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2022-03-12 18:10:07,089] INFO Socket connection established to vm-v01/x.x.x.x:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2022-03-12 18:10:07,093] INFO Session establishment complete on server vm-v01/x.x.x.x:2181, sessionid = 0x27b6bf74fac000e, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2022-03-12 18:10:07,093] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)

æ³¨æ„åˆ°å…³é”®è¯ï¼šShrinking
[2022-03-12 18:10:07,139] INFO [Partition __consumer_offsets-43 broker=3] Shrinking ISR from 3,1,4 to 3. Leader: (highWatermark: 115, endOffset: 120). Out of sync replicas: (brokerId: 1, endOffset: 115) (brokerId: 4, endOffset: 116). (kafka.cluster.Partition)
ç´§æ¥ç€åˆexpandï¼Ÿ
[2022-03-12 18:10:07,381] INFO [Partition __consumer_offsets-43 broker=3] Expanding ISR from 3 to 3,4 (kafka.cluster.Partition)

ä½†æ˜¯fetché”™è¯¯
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread)
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] 
	Error sending fetch request (sessionId=346995240, epoch=1677663) to node 4: 
	java.io.IOException: Client was shutdown before response was read. (org.apache.kafka.clients.FetchSessionHandler)
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
[2022-03-12 18:10:07,471] INFO [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread)

å¯è§ç½‘ç»œè¿˜æ˜¯ä¸ç¨³å®šæˆ–è€…broker 1 å’Œ 2ä¹‹é—´çš„é€šä¿¡åŒæ­¥å‡ºç°é”™è¯¯

åœºæ™¯3ï¼š
æ—¢ç„¶å‰é¢ä¸¤ä¸ªåœºæ™¯ä¸­éƒ½æ˜¯æœ‰ä¸¤ä¸ªèŠ‚ç‚¹ä¸æ­£å¸¸ï¼Œå†æµ‹è¯•ä¸€æ¬¡ï¼š
broker 0 1 2ï¼Œæ–­ç½‘broker 2ï¼ˆä»¥åŠå…¶æœåŠ¡å™¨ä¸Šé¢çš„zookeeperèŠ‚ç‚¹ï¼‰
åŒæ ·å¯åŠ¨kafka-clientï¼Œä»ç„¶è·Ÿå‰é¢ç±»ä¼¼ï¼Œclientç«¯èƒ½å¤Ÿå¯åŠ¨å¹¶ä¸”è®¢é˜…topicï¼Œåªä¸è¿‡æ¢å¤çš„æ—¶å€™ï¼ˆonPartitionAssignå†…éƒ¨è¿›ä¸€æ­¥è®¿é—®kafkaè¯»å–å¿«ç…§æ•°æ®ï¼‰æŠ›é”™ï¼›
ä½†æ˜¯å†è¯•äº†å‡ æ¬¡å‘ç°è¿™ä¸ªé”™è¯¯æœ‰ç‚¹æ„æ€ï¼Œåˆšå¼€å§‹çš„æ—¶å€™ï¼Œæ¯æ¬¡éƒ½ä¼šéšæœºæˆåŠŸæŸå‡ ä¸ªpartitionï¼Œç›´åˆ°2å°æ—¶åæ‰ç¨³å®šçš„æˆåŠŸï¼ˆåé¢å†æµ‹è¯•å‡ è½®å®é™…æ˜¯éšæœºæˆåŠŸå’Œå¤±è´¥ï¼‰
ç¨‹åºçš„main consumer subscribe topicåŸºæœ¬éƒ½æ²¡æœ‰é—®é¢˜ ï¼ˆä¸­é—´åªæœ‰ä¸€æ¬¡å‡ºç°äº†åœºæ™¯5ä¸­çš„æ— æ³•å¯åŠ¨çš„é—®é¢˜ï¼‰ï¼Œ
ç„¶åonPartitionAssignå†…éƒ¨è¿›ä¸€æ­¥è®¿é—®kafkaè¯»å–å¿«ç…§æ•°æ®çš„æ—¶å€™ä¸ç¨³å®š
è¯»å–å¿«ç…§è‡ªç„¶ç”¨çš„ä¸æ˜¯subscribeè€Œæ˜¯
        TopicPartition topicPartition = new TopicPartition(config.getSnapshotTopic(), partition);
        Consumer<String, Snapshot> consumer = createConsumer(partition);
        long lastOffset = consumer.endOffsets(Collections.singleton(topicPartition)).get(topicPartition) - 2;
        if (lastOffset < 0) {
            lastOffset = 0;
        }
        consumer.assign(Collections.singleton(topicPartition));
        consumer.seek(topicPartition, lastOffset);
        ConsumerRecords<String, Snapshot> records = consumer.poll(Duration.ofMillis(10_000L));
        Snapshot snapshot = null;
        for (ConsumerRecord<String, Snapshot> record : records) {
            snapshot = record.value();
        }
        consumer.close();
```
é€šè¿‡åŠ æ—¥å¿—ï¼Œå‘ç°é”™è¯¯éƒ½æ˜¯å‘ç”Ÿåœ¨endOffsetsè¿™é‡Œï¼Œé»˜è®¤çš„timeoutåº”è¯¥æ˜¯30sï¼Œ 
è¿™ä¸ªé—®é¢˜å°±æ˜¯è¯»å–metadataå‡ºç°é—®é¢˜ï¼Œ
æœ€ç»ˆåå¤æµ‹è¯•ç¡®å®šæ ¹æºï¼š
å¦‚æœæŸä¸ªbrokerèŠ‚ç‚¹æ˜¯ç›´æ¥æ€æ­»ï¼Œåªè¦è¯¥èŠ‚ç‚¹ç½‘ç»œé€šçš„ï¼Œclientç«¯è¯»å–metadataå°±ä¸ä¼šè¶…æ—¶ï¼Œ
ä½†æ˜¯å¦‚æœæŸä¸ªbrokerèŠ‚ç‚¹æ‰€åœ¨æœåŠ¡å™¨ç½‘ç»œæ–­å¼€ï¼Œclientç«¯çš„kafka clusteré…ç½®ä¸­ä»æœ‰è¯¥brokerçš„ä¿¡æ¯ï¼Œé‚£ä¹ˆclientç«¯è¯»å–metadataçš„æ—¶å€™å¯èƒ½ä¼šå°è¯•è¿æ¥æœ‰é—®é¢˜çš„èŠ‚ç‚¹ç”±äºç½‘ç»œé‡è¯•é€ æˆè¶…æ—¶
##### æ‰€ä»¥è§£å†³æ–¹æ¡ˆï¼š
1ï¼‰ä¿®æ”¹ consumer.properties.bootstrap.serversï¼Œç§»é™¤åèŠ‚ç‚¹
2ï¼‰ä¿®æ”¹apiè°ƒç”¨ï¼Œå¢åŠ timeoutæ—¶é—´åˆ°5åˆ†é’Ÿï¼ˆå®æµ‹è¶…æ—¶åœ¨2åˆ†é’Ÿå·¦å³ï¼‰ï¼šconsumer.endOffsets(Collections.singleton(topicPartition),Duration.ofMillis(300000)).get(topicPartition)
3ï¼‰ä¿®æ”¹é…ç½® consumer.properties.request.timeout.ms=300000

```
åœºæ™¯4ï¼š
1. broker 0 1 2 éƒ½å¯åŠ¨
2.åœ broker 2ï¼Œå¯åŠ¨kafka clientç«¯æœåŠ¡æ­£å¸¸ï¼Œexpected
3.åœbroker 0ï¼Œæ— æ³•å¯åŠ¨kafka clientç«¯æœåŠ¡ï¼Œexpected
4.æ¢å¤broker2ï¼Œæ— æ³•å¯åŠ¨kafka clientç«¯æœåŠ¡ï¼ŒæŠ¥é”™ï¼Œunexpected
  clientç«¯ä»ç„¶æ˜¯æŠ¥é”™è·Ÿ3ä¸€æ ·ï¼Œ[Consumer clientId=consumer-2, groupId=TEST-SZL] 1 partitions have leader brokers without a matching listener, including [T-TRADE-1]ï¼Œ
  kafkaæœåŠ¡ç«¯æŠ¥é”™ï¼š
  [2022-03-16 10:58:02,825] TRACE [Controller id=1] Leader imbalance ratio for broker 2 is 0.1282051282051282 (kafka.controller.KafkaController)
[2022-03-16 10:58:02,825] INFO [Controller id=1] Starting preferred replica leader election for partitions T-TEST-SNP-2,T-TEST-0,T-TRADE-SNP-2,T-TRADE-1,T-CAPTURE-1 (kafka.controller.KafkaController)
[2022-03-16 10:58:02,836] ERROR [Controller id=1] Error completing preferred replica leader election for partition T-TEST-0 (kafka.controller.KafkaController)
kafka.common.StateChangeFailedException: Failed to elect leader for partition T-TEST-0 under strategy PreferredReplicaPartitionLeaderElectionStrategy
        at kafka.controller.PartitionStateMachine.$anonfun$doElectLeaderForPartitions$9(PartitionStateMachine.scala:390)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at kafka.controller.PartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:388)
        at kafka.controller.PartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:315)
        at kafka.controller.PartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:225)
        at kafka.controller.PartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:141)
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$onPreferredReplicaElection(KafkaController.scala:649)
        at kafka.controller.KafkaController.$anonfun$checkAndTriggerAutoLeaderRebalance$6(KafkaController.scala:1008)
        at scala.collection.immutable.Map$Map3.foreach(Map.scala:195)
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$checkAndTriggerAutoLeaderRebalance(KafkaController.scala:989)
        at kafka.controller.KafkaController$AutoPreferredReplicaLeaderElection$.process(KafkaController.scala:1020)
        at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:94)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
        at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:94)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2022-03-16 10:58:02,837] WARN [Controller id=1] Partition T-TEST-0 failed to complete preferred replica leader election to 2. Leader is still 0 (kafka.controller.KafkaController)

åŸå› åˆ†æï¼šåº”è¯¥æ˜¯ç”±äºæ“ä½œé¢‘ç‡è¿‡å¿«ï¼Œå¯åœkafka borkerçš„æ—¶å€™æ²¡æœ‰ç»™è¶³æ—¶é—´åšfailoverï¼Œå¯è§kafkaæœåŠ¡æœ¬èº«å¯ä»¥çœ‹åˆ°ä»–çš„å¥å£®æ€§æœ‰é—®é¢˜äº†ï¼Œå±…ç„¶æ— æ³•é€‰ä¸¾æ–°leader

é‡æ–°è®¾è®¡è¿ç»­æµ‹è¯•åœºæ™¯ï¼š
-----------------------------------
--- borker 0 1 2 alive
-----------------------------------
start kafka client
test kafka client
-----------------------------------
--- kill 2(both zookeeper&kafka), borker 0 1 alive
-----------------------------------
test kafka client
restart kafka client
test kafka client

-----------------------------------
--- resume 2, borker 0 1 2 alive
-----------------------------------
test kafka client
restart kafka client
test kafka client

-----------------------------------
--- shutdown network for borker 2, borker 0 1 alive
-----------------------------------
test kafka client	
restart kafka client
test kafka client


åœºæ™¯5ï¼šè·Ÿå‰é¢ç±»ä¼¼ï¼Œåªä¸è¿‡è¿™æ¬¡æ˜¯kafkaå®¢æˆ·ç«¯å¯åœå¤ªå¿«
æ–­å¼€ borker 2ç½‘ç»œ, borker 0 1 alive
ç¬¬ä¸€æ¬¡é‡å¯kafka clientç«¯ï¼ˆstop then startï¼‰å¤±è´¥ï¼
    2022-03-16 17:14:21.728  INFO 370GG [main] o.a.k.c.u.AppInfoParser$AppInfo : Kafka version: 2.2.0
		2022-03-16 17:14:21.728  INFO 370GG [main] o.a.k.c.u.AppInfoParser$AppInfo : Kafka commitId: 05fcfde8f69b0349
		2022-03-16 17:15:21.737  INFO 370GG [main] ConditionEvaluationReportLoggingListener : 

		Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
		2022-03-16 17:15:21.767 ERROR 370GG [main] o.s.b.SpringApplication : Application run failed

		org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

		2022-03-16 17:15:21.808  INFO 370GG [main] o.s.s.c.ExecutorConfigurationSupport : Shutting down ExecutorService 'applicationTaskExecutor'
		2022-03-16 17:15:21.809  INFO 370GG [main] o.a.k.c.p.KafkaProducer : [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
		2022-03-16 17:15:21.816  INFO 370GG [main] c.a.d.p.DruidDataSource : {dataSource-1} closing ...
		2022-03-16 17:15:21.831  INFO 370GG [main] c.a.d.p.DruidDataSource : {dataSource-1} closed
    possible reason:
		åœæ‰ kafka clientå¤§æ¦‚æ˜¯åœ¨ï¼š
		2022-03-16 17:13:51.918 ^[[32m INFO^[[m ^[[35m30256GG^[[m [QFJ Timer] ^[[36mc.q.c.f.f.s.AbstractApplication^[[m : fix server toAdmin: [8=FIX.4.4|9=60|35=0|34=683|49=EXEC|52=20220316-09:13:51.918|56=EXCHANGE_FS|10=167|]
		ç„¶åå¾ˆå¿«å¯åŠ¨äº† kafka clientï¼š
		2022-03-16 17:14:17.944 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.StartupInfoLogger^[[m : Starting TradeFrontMain v1.1.0-SNAPSHOT using Java 1.8.0_40 on XXXX with PID 370 (/lyhistory/kafka client.jar started by xxx in /lyhistory)
		2022-03-16 17:14:17.955 ^[[32mDEBUG^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.StartupInfoLogger^[[m : Running with Spring Boot v2.4.5, Spring v5.3.6
		2022-03-16 17:14:17.956 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.SpringApplication^[[m : The following profiles are active: dev
		2022-03-16 17:14:19.821 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.s.b.w.e.t.TomcatWebServer^[[m : Tomcat initialized with port(s): 10102 (http)
		2022-03-16 17:14:19.835 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.a.j.l.DirectJDKLog^[[m : Initializing ProtocolHandler ["http-nio-10102"]
		2022-03-16 17:14:19.836 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.a.j.l.DirectJDKLog^[[m : Starting service [Tomcat]
		2022-03-16 17:14:19.836 ^[[32m INFO^[[m ^[[35m370GG^[[m [main] ^[[36mo.a.j.l.DirectJDKLog^[[m : Starting Servlet engine: [Apache Tomcat/9.0.45]
		å¯¹åº”çš„group coordinatoråœ¨broker 1ä¸Šï¼š
		[2022-03-16 17:14:07,063] INFO [GroupCoordinator 1]: Member consumer-2-f852e86d-7db8-4943-b378-097fe08415f8 in group TEST-SZL has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
		[2022-03-16 17:14:07,064] INFO [GroupCoordinator 1]: Preparing to rebalance group TEST-SZL in state PreparingRebalance with old generation 1 (__consumer_offsets-43) (reason: removing member consumer-2-f852e86d-7db8-4943-b378-097fe08415f8 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
		[2022-03-16 17:14:07,065] INFO [GroupCoordinator 1]: Group TEST-SZL with generation 2 is now empty (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
		[2022-03-16 17:15:54,415] INFO [GroupMetadataManager brokerId=1] Group TEST-SZL transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
		å¯ä»¥çœ‹åˆ°åœ¨kafka clientå¯åŠ¨çš„æ—¶å€™ï¼Œåˆšå¥½æ˜¯group coordinatoråœ¨å¤„ç†removeä¹‹å‰çš„kafka client consumerçš„æ—¶å€™ï¼Œæ‰€ä»¥é€ æˆè¯»å–offsetæ—¶é—´è¶…æ—¶ï¼

ç»éªŒæ€»ç»“ï¼š
1. é‡åˆ°ç½‘ç»œæ•…éšœï¼Œå³ä½¿è‡ªåŠ¨æ¢å¤äº†ï¼Œä¹Ÿæœ€å¥½è¦å¯åœä¸€ä¸‹zookeeperå’Œkafka
2. å¯åœkafkaæœåŠ¡æ“ä½œæ—¶ä¸è¦åŠ¨ä½œå¤ªå¿«
3. å¯åœkafkaå®¢æˆ·ç«¯æ“ä½œæ—¶ä¹Ÿä¸è¦åŠ¨ä½œå¤ªå¿«

è¿™äº›å¼‚å¸¸æƒ…å†µå¯èƒ½ä¹Ÿæ˜¯kafkaæœ€ç»ˆæŠ›å¼ƒzookeeperçš„åŸå› ï¼
```

#### é€ æˆkafka clientç«¯ç¨‹åºpollè‡ªå·±ç»´æŠ¤çš„offset topicå‡ºç°é—®é¢˜
ç»“åˆå‰é¢ [4.3.2 ä¸ä¾èµ–interal offsetï¼Œè‡ªå·±ç»´æŠ¤offset exactly-once](#432-ä¸ä¾èµ–interal-offsetè‡ªå·±ç»´æŠ¤offset-exactly-once)
public ConsumerRecords<K,V> poll(java.time.Duration timeout)

This method returns immediately if there are records available. Otherwise, it will await the passed timeout. If the timeout expires, an empty record set will be returned. 

```
consumer.seek(topicPartition, snpoffset);
        ConsumerRecords<String, SimpleSnapshot> records = consumer.poll(Duration.ofMillis(10_000L));
        if(records!=null) {
            System.out.println("records not null, count=" + String.valueOf(records.count()));
        }
```
å’Œå‰é¢æƒ…å†µç±»ä¼¼ï¼Œç»è¿‡å®é™…æµ‹è¯•ï¼Œå‘ç°è¿™ä¸ª10sçš„è®¾ç½®ï¼ŒæŒ‡å®šçš„offsetæœ‰æ—¶å€™èƒ½è¿”å›æ­£ç¡®çš„messageï¼Œæœ‰æ—¶å€™è¿”å›empty recordï¼ï¼Œ
ç„¶åæ”¹æˆ 30000 å³5åˆ†é’Ÿåˆ™æ²¡æœ‰é—®é¢˜

[kafka consumer.seek ä¹‹åç«‹å³ poll å¯èƒ½æ‹‰ä¸åˆ°æ¶ˆæ¯ ](https://www.cnblogs.com/duanguyuan/p/15906363.html)

#### é€ æˆkafka clientç«¯ç¨‹åºè¯»å– metadata è¶…è¿‡é»˜è®¤ 1åˆ†é’Ÿ æŠ›é”™

public java.util.List<PartitionInfo> partitionsFor(java.lang.String topic)
Get metadata about the partitions for a given topic. This method will issue a remote call to the server if it does not already have any metadata about the given topic.

TimeoutException - if the offset metadata could not be fetched before the amount of time allocated by **default.api.timeout.ms** expires.

### This member will leave the group because consumer poll timeout has expired

[36m.c.i.AbstractCoordinator$HeartbeatThread^[[m : 
[Consumer clientId=consumer-1, groupId=TEST-SZL] 
This member will leave the group because consumer poll timeout has expired. This means the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time processing messages. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.
[[36mo.a.k.c.c.i.AbstractCoordinator^[[m : [
Consumer clientId=consumer-1, groupId=TEST-SZL] Member consumer-1-7f40d109-cd66-4554-82a9-376f1922c1b5 sending LeaveGroup request to coordinator x.x.x.x:9092 (id: 2147483647 rack: null)

åˆ†æï¼š
é¦–å…ˆæŠ¥é”™çš„æ„æ€æ˜¯ç¨‹åºå¤„ç†ä¸€æ‰¹kafkaæ¶ˆæ¯ï¼ˆmax.poll.records é»˜è®¤500æ¡ï¼‰çš„æ—¶é—´è¶…è¿‡äº†max.poll.interval.msï¼Œæ‰€ä»¥ç›´è§‚çš„çœ‹æ¥å°±æ˜¯é™ä½max.poll.recordsæˆ–è€…æé«˜max.poll.interval.msï¼Œä¸è¿‡æˆ‘ä»¬è¿™é‡Œçš„åœºæ™¯å¹¶æ²¡æœ‰è¿™ä¹ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬å‡ºé”™çš„æ—¶å€™è¿˜æ²¡æœ‰å¼€å§‹è·å–å¹¶å¤„ç†kafkaæ¶ˆæ¯ï¼Œè€Œæ˜¯ç¬¬ä¸€æ¬¡pollè§¦å‘çš„rebalanceçš„è¿‡ç¨‹ä¸­å‡ºç°çš„é”™è¯¯ï¼›


è¯¥ç¨‹åºçš„ä¸»è¦é€»è¾‘ï¼š
```
private void start(ApplicationContext context) {
        if (managerThread == null) {
            workerManager = new SimpleWorkerManager(workContext);
            managerThread = new Thread(workerManager::start);
            managerThread.setDaemon(true);
            managerThread.setName(config.getTaskType() + "-MANAGER");
            managerThread.start();
        }
    }
SimpleWorkerManagerçš„æ„é€ æ–¹æ³•ä¸­è°ƒç”¨äº†kafkaçš„æ¶ˆæ¯è®¢é˜…
this.kafkaConsumer.subscribe(Collections.singleton(context.getConfig().getTaskTopic()), new SimpleWorkBalancer(context.getRestorer(), this::removeWorker, this::addWorker));

ç„¶åstrat()æ–¹æ³•æ˜¯å»pollæ¶ˆæ¯
ConsumerRecords<String, Info> records = kafkaConsumer.poll(pollDuration);
```
After subscribing to a set of topics, the consumer will automatically join the group when poll(Duration) is invoked.

æ‰€ä»¥é—®é¢˜å‡ºåœ¨ç¬¬ä¸€æ¬¡pollå¼•èµ·çš„rebalanceçš„å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œè¶…è¿‡äº†max.poll.interval.msï¼Œç„¶åæˆ‘ä»¬æŸ¥ä¸€ä¸‹è¿™ä¸ªé…ç½®ï¼š

> The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. For consumers using a non-null group.instance.id which reach this timeout, partitions will not be immediately reassigned. Instead, the consumer will stop sending heartbeats and partitions will be reassigned after expiration of session.timeout.ms. This mirrors the behavior of a static consumer which has shutdown.
> Type:	int
> Default:	300000 (5 minutes)
> Valid Values:	[1,...]
> Importance:	medium

æ­¤æ—¶æˆ‘æƒ³åˆ°äº†[å‰é¢çš„å¦å¤–ä¸€ä¸ªbugåšçš„ä¿®å¤](#ç½‘ç»œæ•…éšœ--kafkaé›†ç¾¤æœ‰èŠ‚ç‚¹æŒ‚æ‰ä¸æ˜¯æ­£å¸¸åœèŠ‚ç‚¹è€Œæ˜¯brokerèŠ‚ç‚¹æ‰€åœ¨æœåŠ¡å™¨ç½‘ç»œæ–­å¼€æˆ–æš´åŠ›åœæœº) request.timeout.ms æˆ‘è®¾ç½®æˆäº†5åˆ†é’Ÿï¼Œç„¶ååº”è¯¥æ˜¯åœ¨reblanceçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç¨‹åºçš„é€»è¾‘ä¼šå»ä»kafkaä¸­æ¢å¤æˆ‘ä»¬è‡ªå·±ç®¡ç†çš„å¿«ç…§ï¼Œä½†æ˜¯ç”±äºæ‰€æœ‰topicå·²ç»åˆ é™¤ï¼Œæ‰€ä»¥è¯»å–çš„æ—¶å€™å¯èƒ½ä¼šç­‰å¾…5åˆ†é’Ÿï¼Œç„¶å max.poll.interval.ms é»˜è®¤ä¹Ÿæ˜¯5åˆ†é’Ÿï¼Œè‡ªç„¶å°±timeoutäº†ï¼Œæ‰€ä»¥ä¿®å¤åŠæ³•æ˜¯å»¶é•¿ max.poll.interval.msåˆ°æ›´é•¿çš„æ—¶é—´

æ³¨æ„ï¼š
> Also as part of KIP-266, the default value of request.timeout.ms has been changed to 30 seconds. The previous value was a little higher than 5 minutes to account for maximum time that a rebalance would take. Now we treat the JoinGroup request in the rebalance as a special case and use a value derived from max.poll.interval.ms for the request timeout. All other request types use the timeout defined by request.timeout.ms
> Notable changes in 0.10.2.1
> The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer retries default value was changed from 0 to 10. The internal Kafka Streams consumer max.poll.interval.ms default value was changed from 300000 to Integer.MAX_VALUE.
> The new Java Consumer now supports heartbeating from a background thread. There is a new configuration max.poll.interval.ms which controls the maximum time between poll invocations before the consumer will proactively leave the group (5 minutes by default). The value of the configuration request.timeout.ms (default to 30 seconds) must always be smaller than max.poll.interval.ms(default to 5 minutes), since that is the maximum time that a JoinGroup request can block on the server while the consumer is rebalance. Finally, the default value of session.timeout.ms has been adjusted down to 10 seconds, and the default value of max.poll.records has been changed to 500.

---
[KIP-568: Explicit rebalance triggering on the Consumer](https://cwiki.apache.org/confluence/display/KAFKA/KIP-568%3A+Explicit+rebalance+triggering+on+the+Consumer)

[TIMEOUTS IN KAFKA CLIENTS AND KAFKA STREAMS](http://javierholguera.com/2018/01/01/timeouts-in-kafka-clients-and-kafka-streams/)

[nothing to guarantee=>at-most-once | at-least-once => exactly-once](https://kafka.apache.org/documentation/#design)

[Message Delivery Semantics](https://kafka.apache.org/documentation/#semantics)