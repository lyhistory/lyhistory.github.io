(window.webpackJsonp=window.webpackJsonp||[]).push([[142],{345:function(t,e,a){"use strict";a.r(e);var s=a(0),n=Object(s.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[a("a",{attrs:{href:"/docs/software"}},[t._v("回目录")]),t._v("  《Monitor》")]),t._v(" "),a("h2",{attrs:{id:"_1-basics"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-basics"}},[t._v("#")]),t._v(" 1.Basics")]),t._v(" "),a("h3",{attrs:{id:"_1-1-visualization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-visualization"}},[t._v("#")]),t._v(" 1.1 Visualization")]),t._v(" "),a("p",[t._v("Data Visualize\nhttps://www.elastic.co/")]),t._v(" "),a("p",[t._v("grafana\nhttps://grafana.com/docs/features/panels/graph/\nX-axis type: time, series, histogram\nhttps://grafana.com/docs/features/datasources/\nhttps://grafana.com/docs/features/datasources/prometheus/")]),t._v(" "),a("p",[t._v("Grafana vs kibana\nhttps://logz.io/blog/grafana-vs-kibana/\nhttps://grafana.com/docs/features/datasources/elasticsearch/")]),t._v(" "),a("p",[t._v("Grafana with nginx\nSet up domain\nRunning Grafana behind a reverse proxy http://docs.grafana.org/installation/behind_proxy/")]),t._v(" "),a("p",[t._v("tail -f /var/log/grafana/grafana.log")]),t._v(" "),a("p",[t._v("?#issues Template variables are not supported in alert queries\nCreate new chart/graph without using any template variables\n/etc/grafana/grafana.ini")]),t._v(" "),a("p",[t._v("?#grafana mail: expected single address, got\nResolved by comment out from_name")]),t._v(" "),a("p",[t._v("?#could not send email 1: 452 4.3.1 Out of memory")]),t._v(" "),a("blockquote",[a("p",[t._v("If you have a lot of messages queued up it could go over the max number of messages per connection. To see if this is the case you can try submitting only a few messages to that domain at a time and then keep increasing the number until you find the maximum number accepted by the server.\nhttps://social.msdn.microsoft.com/Forums/sqlserver/en-US/a1821fec-5109-46e0-8b18-36b96646a5c7/failure-sending-mail?forum=sqlreportingservices")])]),t._v(" "),a("p",[t._v("grafana permission\nAdd permission, and remember remove all default viewer and editor permission")]),t._v(" "),a("h3",{attrs:{id:"_1-2-data-source-compare"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-data-source-compare"}},[t._v("#")]),t._v(" 1.2 Data source compare")]),t._v(" "),a("p",[t._v("Elastic / prometheus/ influxdb/ opentsdb /…...\nhttps://prometheus.io/docs/introduction/comparison/\nevent logging and metrics recording")]),t._v(" "),a("p",[a("strong",[t._v("Prometheus vs influxdb")]),t._v("\nInfluxDB supports timestamps with up to nanosecond resolution\nPrometheus, by contrast, supports the float64 data type with limited support for strings, and millisecond resolution timestamps\nWhere InfluxDB is better:\n●\tIf you're doing event logging.\n●\tCommercial option offers clustering for InfluxDB, which is also better for long term data storage.\n●\tEventually consistent view of data between replicas.\nWhere Prometheus is better:\n●\tIf you're primarily doing metrics.\n●\tMore powerful query language, alerting, and notification functionality.\n●\tHigher availability and uptime for graphing and alerting.")]),t._v(" "),a("p",[t._v("InfluxDB is maintained by a single commercial company following the open-core model, offering premium features like closed-source clustering, hosting and support. Prometheus is a fully open source and independent project, maintained by a number of companies and individuals, some of whom also offer commercial services and support.")]),t._v(" "),a("p",[a("strong",[t._v("Prometheus vs. OpenTSDB")]),t._v("\nOpenTSDB is a distributed time series database based on Hadoop and HBase.")]),t._v(" "),a("h2",{attrs:{id:"_2-details"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-details"}},[t._v("#")]),t._v(" 2. Details")]),t._v(" "),a("h3",{attrs:{id:"_2-1-prometheus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-prometheus"}},[t._v("#")]),t._v(" 2.1 Prometheus")]),t._v(" "),a("p",[t._v("https://prometheus.io/docs/introduction/overview/\n"),a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor01.png",alt:""}})]),t._v(" "),a("p",[t._v("Pull or push?\nScrape config: scrape interval\nReload config(not work?) https://www.robustperception.io/reloading-prometheus-configuration")]),t._v(" "),a("p",[t._v("Prometheus    http://localhost:9090\nMonitor your applications with Prometheus https://blog.alexellis.io/prometheus-monitoring/")]),t._v(" "),a("h4",{attrs:{id:"_2-1-1-metric-type"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-1-metric-type"}},[t._v("#")]),t._v(" 2.1.1 Metric type")]),t._v(" "),a("p",[t._v("https://prometheus.io/docs/concepts/metric_types/\nSamples: https://gist.github.com/lyhistory/998448f439594d3eec073d7849d713a8\ncounter, gauge, histogram, summary\nCounter increase all the time\nGauge can be negative, decrease or increase")]),t._v(" "),a("p",[a("strong",[t._v("Summary vs histogram")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('Summary直接统计每个百分位的“数值”【客户端计算】，比如0.5即50%的请求相应时间是多少 （全范围统计，over all time）：\n\t*****_duration_seconds{quantile="0.5"}\n\t*****_duration_seconds_sum\n\t*****_duration_seconds_count\n\t*****_duration_seconds_sum/*****_duration_seconds_count\t每个请求的平均耗时\n而histogram直接反应了不同区间内样本的个数【服务端计算】（后面的区间是覆盖前面的区间）（全范围统计，over all time）\n\t*****_range_bucket{le="100"} 小于100秒的请求个数有几个\n\t*****_range_sum\n\t*****_range_count\n\t*****_range_sum/*****_range_count\t每个请求的耗时\n\thistogram计算百分位函数histogram_quantile\n\thistogram_quantile(0.5, *****_duration_seconds_bucket) 占50%的请求的平均耗时（需要注意的是通过histogram_quantile计算的分位数，并非为精确值，而是通过*****_duration_seconds_bucket和*****_duration_seconds_sum近似计算的结果）\n\n进一步, Over 10m统计：\nSummary\n\trate(*****_duration_seconds_sum[10m]) / rate(*****_duration_seconds_count[10])\nHistogram\n\trate(*****_range_sum[10m]) / rate(*****_range_count[10m])\n\thistogram_quantile(0.5, rate(*****_duration_seconds_bucket[10m])) 这个结果可能因为还有其他label比如responsecode=200/404产生多条记录：过去10分钟，返回200的50%的请求的平均响应时间以及返回404的50%的请求的平均响应时间，如果想获取过去10分钟，返回所有请求之中的50%的请求的平均响应时间，则需要用\n\thistogram_quantile(0.5, sum(rate(*****_duration_seconds_bucket[10m])) by (le)) \n\n')])])]),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor02.png",alt:""}}),t._v("\nhttps://groups.google.com/forum/#!topic/prometheus-developers/VYaiXJCsHxQ\nhttps://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/promql/prometheus-metrics-types\nhttps://www.yangcs.net/prometheus/3-prometheus/functions.html")]),t._v(" "),a("p",[t._v("Sample - Summary")]),t._v(" "),a("p",[t._v("The Python client doesn't store or expose quantile information at this time\nrequest_processing_seconds_sum\nrequest_processing_seconds_count\nrate(request_processing_seconds_sum[25m])/rate(request_processing_seconds_count[25m])")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY\nfrom prometheus_client import start_http_server, Summary\nimport random\nimport time\n# Create a metric to track time spent and requests made.\nREQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request')\n# Decorate function with metric.\n@REQUEST_TIME.time()\ndef process_request(t):\n   \"\"\"A dummy function that takes some time.\"\"\"\n   time.sleep(t)\nif __name__ == '__main__':\n   # Start up the server to expose the metrics.\n   parser = argparse.ArgumentParser()\n   parser.add_argument('-p', '--port', help=\"The port the metrics is serving from.\", required=True)\n   args = parser.parse_args()\n   start_http_server(int(args.port))\n   #Test Summary\n   while True: process_request(random.randint(60,600))\n\n")])])]),a("p",[t._v("Sample - histogram")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("request_latency_seconds_bucket\nrequest_latency_seconds_bucket[10m]\nhistogram_quantile(0.99, request_latency_seconds_bucket)\nhistogram_quantile(0.99, rate(request_latency_seconds_bucket[10m]))\nhistogram_quantile(0.99, sum(rate(request_latency_seconds_bucket[10m])) by (le))\n\n##DEFAULT_BUCKETS = (.005, .01, .025, .05, .075, .1, .25, .5, .75, 1.0, 2.5, 5.0, 7.5, 10.0, INF)\nINF = float(\"inf\")\n# Create a metric to track time spent and requests made.\nREQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request')\n\n# Decorate function with metric.\n@REQUEST_TIME.time()\ndef process_request(t):\n\t\"\"\"A dummy function that takes some time.\"\"\"\n\tprint(str(t))\n\ttime.sleep(t)\nif __name__ == '__main__':\n\t# Start up the server to expose the metrics.\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-p', '--port', help=\"The port the metrics is serving from.\", required=True)\n\targs = parser.parse_args()\n\tstart_http_server(int(args.port))\n\t#Test Summary\n\tMY_BUCKETS = (30.0,40.0,50.0,60.0,70.0,80.0,90.0,100.0, INF)\n\th = Histogram(name='request_latency_seconds', documentation='Description of histogram',buckets=MY_BUCKETS)\n\twhile True:\n    \tt = random.randint(20,110)\n    \th.observe(t)\n    \tprocess_request(t)\n\n")])])]),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor03.png",alt:""}})]),t._v(" "),a("p",[a("strong",[t._v("Visualize on grafana")]),t._v("\nhttps://yunlzheng.gitbook.io/prometheus-book/part-ii-prometheus-jin-jie/grafana/grafana-panels/use_graph_panel")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor04.png",alt:""}})]),t._v(" "),a("p",[t._v("histogram graph:\nBe careful, grafana x axis ‘histogram’ mode is meaningless for prometheus, so we need to use “Series” mode instead\ncount==****_count[pre-le,le]\tbased on separated buckets\nTotal == ****_count[,le]\tbased on le buckets\nDemonstrated above with x axis in “Time” Mode")]),t._v(" "),a("p",[t._v("Format as “Heatmap” so that it will separate le buckets into [] individual buckets\nle[,10] le[,20] ….  ====> le[,10] le[10,20] …...")]),t._v(" "),a("p",[t._v("heatmap graph:\nhttps://grafana.com/docs/features/panels/heatmap/\nPrometheus Histograms & Grafana Heatmaps https://vimeo.com/289620891\nBecause each **buckets accumulated over time(buckets here has already been separated into individual buckets), so use increase function to show the ‘rate’ along the time")]),t._v(" "),a("h4",{attrs:{id:"_2-1-2-query-data-type-and-promql"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-2-query-data-type-and-promql"}},[t._v("#")]),t._v(" 2.1.2 Query data type AND PromQL")]),t._v(" "),a("p",[t._v("Literals:\nString\nFloat/scala\nTime series selector:\nInstant vector selector\nRange vector selector\nBasic time series\nhttps://prometheus.io/docs/prometheus/latest/querying/examples/")]),t._v(" "),a("p",[t._v("Example: scrape every 15s, 4 request every 1m\nhttp_requests_total\nhttp_requests_total[1m]\nrate(http_requests_total[1m])\nsum(rate(http_requests_total[1m])) by (instance)\nsum(rate(http_requests_total[1m])) by job)\ntopk(5, sum(rate(http_requests_total[5m])) by (instance))")]),t._v(" "),a("p",[t._v("Histogram and summary\nhttps://prometheus.io/docs/practices/histograms/\nhttps://timber.io//blog/promql-for-humans/\nhttps://povilasv.me/prometheus-tracking-request-duration/")]),t._v(" "),a("h4",{attrs:{id:"_2-1-3-metric-and-alert-sample"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-3-metric-and-alert-sample"}},[t._v("#")]),t._v(" 2.1.3 Metric and Alert sample")]),t._v(" "),a("p",[t._v("Prometheus 监控 Nginx 流量 https://www.cnblogs.com/vovlie/p/Nginx_monitoring.html\nhttp://vearne.cc/archives/11085\nhttps://www.cnblogs.com/SleepDragon/p/10642955.html")]),t._v(" "),a("p",[t._v("Exporter\nEXPORTERS AND INTEGRATIONS https://prometheus.io/docs/instrumenting/exporters/\nMONITORING LINUX HOST METRICS WITH THE NODE EXPORTER https://prometheus.io/docs/guides/node-exporter/")]),t._v(" "),a("p",[t._v("Example\nhttp://michaeljones.tech/writing-exporters-for-prometheus/\nCoinmarketcap:\nhttps://blog.billyc.io/2017/12/02/a-prometheus-exporter-for-cryptocurrency-values-using-the-coinmarketcap-api/\nhttps://www.robustperception.io/writing-a-jenkins-exporter-in-python")]),t._v(" "),a("h4",{attrs:{id:"_2-1-4-troubleshooting"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-4-troubleshooting"}},[t._v("#")]),t._v(" 2.1.4 troubleshooting")]),t._v(" "),a("p",[t._v("Prometheus deafult admin dashboard:\nStatus -> Targets : check status")]),t._v(" "),a("p",[t._v("c= GaugeMetricFamily('btc_gauge', 'btc statics',labels=['menu'], value=1000)\nFile \"/usr/lib/python2.7/site-packages/prometheus_client/core.py\", line 212, in "),a("strong",[t._v("init")]),t._v("\nraise ValueError('Can only specify at most one of value and labels.')\nValueError: Can only specify at most one of value and labels.")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor05.png",alt:""}})]),t._v(" "),a("p",[t._v("No token found\nThis usually means that the output is not valid Prometheus text format. Look for hyphens in metric or label names, or either of those starting with numbers - those are the most common errors.")]),t._v(" "),a("h4",{attrs:{id:"_2-1-5-more"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-5-more"}},[t._v("#")]),t._v(" 2.1.5 More")]),t._v(" "),a("p",[t._v("Scalability")]),t._v(" "),a("p",[t._v("Thanos - a Scalable Prometheus with Unlimited Storage https://www.infoq.com/news/2018/06/thanos-scalable-prometheus")]),t._v(" "),a("h3",{attrs:{id:"_2-2-influxdb"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-influxdb"}},[t._v("#")]),t._v(" 2.2 influxdb")]),t._v(" "),a("h4",{attrs:{id:"_2-2-1"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-1"}},[t._v("#")]),t._v(" 2.2.1")]),t._v(" "),a("p",[t._v("V1.7\nhttps://docs.influxdata.com/influxdb/v1.7/introduction/getting-started/\nhttps://github.com/influxdata/influxdb-python\nhttps://influxdb-python.readthedocs.io/en/latest/api-documentation.html")]),t._v(" "),a("p",[t._v("Change port /etc/influxdb/influxdb.conf\nRun: influxdb or service start influxdb\nCli: influx -precision rfc3339 -host 127.0.0.1 -port 8086")]),t._v(" "),a("p",[t._v("Visualize-table panel\nhttps://grafana.com/docs/features/panels/table_panel/")]),t._v(" "),a("p",[t._v("?#data with same timestamp and tags gets overwritten\nBy design https://github.com/influxdata/influxdb/issues/4150")]),t._v(" "),a("p",[t._v("v2\nhttps://v2.docs.influxdata.com/v2.0/get-started/\nhttps://community.influxdata.com/c/getting-started\nhttps://community.influxdata.com/c/influxdb2\nhttps://www.influxdata.com/blog/getting-started-with-influxdb-2-0-scraping-metrics-running-telegraf-querying-data-and-writing-data/\nflux rpel https://docs.influxdata.com/flux/v0.24/")]),t._v(" "),a("p",[t._v("Key concepts: buckets\nhttps://docs.influxdata.com/flux/v0.24/introduction/getting-started")]),t._v(" "),a("p",[t._v("New release https://www.influxdata.com/blog/introducing-the-next-generation-influxdb-2-0-platform/")]),t._v(" "),a("p",[t._v("(Optional) Copy the influx and influxd binary to your $PATH\nsudo cp influxdb_2.0.0-alpha.8_darwin_amd64/{influx,influxd} /usr/local/bin/")]),t._v(" "),a("p",[t._v("influxd --http-bind-address=127.0.0.1:9999")]),t._v(" "),a("h4",{attrs:{id:"_2-2-2-example"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-2-example"}},[t._v("#")]),t._v(" 2.2.2 Example")]),t._v(" "),a("p",[t._v("Influxdb query last row of all series in a measurement\nhttps://community.influxdata.com/t/influxdb-query-last-row-of-all-series-in-a-measurement/4915/3https://community.influxdata.com/t/influxdb-query-last-row-of-all-series-in-a-measurement/4915\nhttps://stackoverflow.com/questions/29193898/influxdb-getting-only-last-value-in-query/56141349#56141349")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('from prometheus_client.core import Metric,GaugeMetricFamily, CounterMetricFamily, REGISTRY\nfrom prometheus_client import Gauge\nfrom prometheus_client import start_http_server\nimport time\nimport argparse\nimport requests\nfrom datetime import datetime,timedelta\nfrom influxdb import InfluxDBClient\n\nclass CustomCollector(object):\n\tdef __init__(self):\n     \tself.client = InfluxDBClient(host=\'localhost\', port=8087, database="TEST")\n\tdef collect(self):\n    \tr = requests.get("http://localhost/api/GetBankAccount")\n    \t#print(r)\n    \tdata = r.json()\n    \t#print(data)\n    \tprint(len(data["List"]))\n    \tmetric = Metric("banks","bank list","gauge")\n    \tindex = 0\n\n    \tcurrent_time = real_time = datetime.utcnow()\n    \tpoints = []\n    \tfor item in data["List"]:\n            \tindex+=1\n            \tgroup=\'NA\'\n            \tif item["group"] is not None:\n                    \tgroup = item["group"]\n            \tmetric.add_sample("bank_"+str(index), value=item["BALANCE"],labels={"type":"bank",\'bankname\': item["BANKNAME"],"code":item["CODE"],"group":group})\n            \tjson_body = {\n                    \t"measurement": "banklist",\n                    \t"time": real_time.strftime(\'%Y-%m-%dT%H:%M:%SZ\'),\n                    \t"tags":{\n                            \t"code":item["CODE"]\n                    \t},\n                    \t"fields": {\n                            \t"timestamp": real_time.strftime(\'%Y-%m-%dT%H:%M:%SZ\'),\n                            \t"balance": item["BALANCE"],\n                            \t"bankname": item["BANKNAME"],\n                            \t"bankcode":item["CODE"],\n                            \t"bankgroup":group\n                    \t}\n            \t}\n            \tprint("Write points: {0}".format(json_body))\n            \tpoints.append(json_body)\n            \tcurrent_time = current_time+timedelta(seconds=1)\n    \tif len(points)>0:\n            \tself.client.write_points(points)\n    \tyield metric\nif __name__ == \'__main__\':\n\t# Start up the server to expose the metrics.\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'-p\', \'--port\', help="The port the metrics is serving from.", required=True)\n\n\targs = parser.parse_args()\n\tstart_http_server(int(args.port))\n\tREGISTRY.register(CustomCollector())\nwhile True: time.sleep(60)\n\n')])])]),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor06.png",alt:""}})]),t._v(" "),a("p",[t._v("https://thingsmatic.com/2017/03/02/influxdb-and-grafana-for-sensor-time-series/")]),t._v(" "),a("h2",{attrs:{id:"_3-monitor-examples"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-monitor-examples"}},[t._v("#")]),t._v(" 3. Monitor Examples")]),t._v(" "),a("h3",{attrs:{id:"_3-1-postgresql-monitor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-postgresql-monitor"}},[t._v("#")]),t._v(" 3.1 Postgresql monitor")]),t._v(" "),a("p",[t._v("Why pgwatch2?\nhttps://www.cybertec-postgresql.com/en/announcing-pgwatch2-a-simple-but-versatile-postgresql-monitoring-tool/")]),t._v(" "),a("p",[t._v("https://github.com/cybertec-postgresql/pgwatch2")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor10.png",alt:""}})]),t._v(" "),a("h4",{attrs:{id:"_3-1-1-introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-1-introduction"}},[t._v("#")]),t._v(" 3.1.1 Introduction")]),t._v(" "),a("p",[t._v("1）Project background\nFor more background on the project motivations and design goals see the original series of blogposts announcing the project:\n●\tProject announcement\n●\tImplementation details\n●\tFeature pack 1\n●\tFeature pack 2\n●\tFeature pack 3")]),t._v(" "),a("p",[t._v("2）Source Code explain\n"),a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor11.png",alt:""}}),t._v("\npgwatch2/Dockerfile")]),t._v(" "),a("p",[t._v("pgwatch2/docker-launcher.sh\nhttps://github.com/cybertec-postgresql/pgwatch2/blob/9eb5ac699df9873d2138f94dc35e1ba509dd82a6/docker-launcher.sh")]),t._v(" "),a("p",[t._v("Supervisord\nhttp://supervisord.org/")]),t._v(" "),a("p",[t._v("pgwatch2/supervisord.conf")]),t._v(" "),a("p",[t._v("http://blog.51cto.com/youerning/1714627\nhttps://blog.csdn.net/vbaspdelphi/article/details/53324673\nAuto start on system start\n/etc/init.d/supervisor\nhttps://serverfault.com/questions/96499/how-to-automatically-start-supervisord-on-linux-ubuntu")]),t._v(" "),a("h4",{attrs:{id:"_3-1-2-setup"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-2-setup"}},[t._v("#")]),t._v(" 3.1.2 setup")]),t._v(" "),a("h5",{attrs:{id:"_3-1-2-1-env"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-2-1-env"}},[t._v("#")]),t._v(" 3.1.2.1 env")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor12.png",alt:""}})]),t._v(" "),a("h5",{attrs:{id:"_3-1-2-2-install-upgrade-grafana"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-2-2-install-upgrade-grafana"}},[t._v("#")]),t._v(" 3.1.2.2 install/upgrade grafana")]),t._v(" "),a("p",[t._v("wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.2.2-1.x86_64.rpm sudo yum localinstall grafana-5.2.2-1.x86_64.rpm\nhttp://docs.grafana.org/installation/upgrading/")]),t._v(" "),a("p",[t._v("sudo yum install https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.2.2-1.x86_64.rpm\n/etc/grafna/grafana.ini")]),t._v(" "),a("h5",{attrs:{id:"_3-1-2-3-run-integrated-docker-image"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-2-3-run-integrated-docker-image"}},[t._v("#")]),t._v(" 3.1.2.3 run integrated docker image")]),t._v(" "),a("ol",[a("li",[t._v("run docker\npull\nhttps://hub.docker.com/r/cybertec/pgwatch2/tags/\nsudo docker run -d -p 8080:8080 -p 8086:8086 -p 9001:9001 -p 3001:3000 --name pw2 cybertec/pgwatch2\nsudo docker exec -ti pw2 /bin/bash\nNote: don’t follow instructions given by pgwatch2, failed if using the cmd below, need to check the reason:\n"),a("em",[t._v("sudo docker run -d -p 8080:8080 -p 9001:9001 -p 3001:3000 -e PW2_GRAFANA_BASEURL='http://10.20.70.205:3000' --name pw2")])])]),t._v(" "),a("p",[t._v("In order to avoid conflicts with existing grafana, we use host port 3001 mapping to docker grafana 3000,\n9001 is for supervisor UI, 8080 is for the web ui, 8086 is for influx db")]),t._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[t._v("Postgresql server config\nAt pgsql server:\npgsql -p 6432\n/c oureadb\ncreate role pgwatch2 with login password 'secret';\n/i pgwatch2/sql/metric_fetching_helpers/stat_activity_wrapper.sql\nCREATE EXTENSION pg_stat_statements;\nCREATE EXTENSION plpythonu;\n/i pgwatch2/sql/metric_fetching_helpers/stat_statements_wrapper.sql\n/i pgwatch2/sql/metric_fetching_helpers/cpu_load_plpythonu.sql")])]),t._v(" "),a("p",[t._v("Add user to pgbouncer")]),t._v(" "),a("ol",{attrs:{start:"3"}},[a("li",[t._v("config supervisor http server")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('root@28daac38a798:/# supervisor\nsupervisorctl  supervisord\nroot@28daac38a798:/# supervisorctl stop\nError: stop requires a process name\nstop <name>         \tStop a process\nstop <gname>:*      \tStop all processes in a group\nstop <name> <name>  \tStop multiple processes or groups\nstop all            \tStop all processes\n\nroot@28daac38a798:/# ps -lef|grep "super"\n4 S root     \t1 \t0  0  80   0 - 12459 poll_s 08:55 ?    \t00:00:00 /usr/bin/python /usr/bin/supervisord --configuration=/etc/supervisor/supervisord.conf --nodaemon\nroot@28daac38a798:/# netstat -anp|grep :9001\ntcp    \t0  \t0 0.0.0.0:9001        \t0.0.0.0:*           \tLISTEN  \t1/python\nroot@28daac38a798:/#\n')])])]),a("p",[t._v("Modify /etc/supervisor/supervisord.conf\tadd:\n[inet_http_server]\nport = 9001\nusername = user\npassword = 123\nBut how to restart with configuration changes or reload?\nhttp://www.onurguzel.com/supervisord-restarting-and-reloading/")]),t._v(" "),a("p",[t._v("?# pgwatch2 fatal error\ncheck /var/log/supervisor/")]),t._v(" "),a("ol",{attrs:{start:"4"}},[a("li",[t._v("Add postgresql db config at WebUI\nhttp://hostip:8080/dbs\nName: dbname\nHost: port: db:\nUsername: pgwatch2, password: secret")])]),t._v(" "),a("p",[t._v('Note: by default there is a test database, removing from here is useless, because I find the test database still in influxdb, so from grafana you still can see test in the list, the way to get rid of it is change the Variables config in 4)  from default\nSHOW TAG VALUES WITH KEY = "dbname"\nTo\nSHOW TAG VALUES WITH KEY = "dbname"  where "dbname" !~ /(test)+/')]),t._v(" "),a("ol",{attrs:{start:"5"}},[a("li",[t._v("grafana config\nhttp://hostip:3000/datasources\nAdd influxdb datasource:\nInflux\nInfluxDB\nhttp://localhost:8086\nDatabase: pgwatch2, username: root, password: root\nMake it default source, otherwise you have to make changes to all the dashboards to specify the datasource from default to influx")])]),t._v(" "),a("p",[t._v("User and password is required here, because by default, influxdb disable authentication, pls refer to below how to enable and config with auth enabled")]),t._v(" "),a("p",[t._v("Add Monitor script:\nhttps://github.com/cybertec-postgresql/pgwatch2/tree/master/grafana_dashboards/v5")]),t._v(" "),a("p",[t._v('Change dashboard setting:\nVariables -> edit\nDatasource:  select influxdb\nQuery: change\nSHOW TAG VALUES WITH KEY = "dbname"\nTo\nSHOW TAG VALUES WITH KEY = "dbname"  where "dbname" !~ /(test)+/')]),t._v(" "),a("h5",{attrs:{id:"_3-1-2-4-results"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-2-4-results"}},[t._v("#")]),t._v(" 3.1.2.4 results")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('Available measurements (InfluxDB "tables" with metric info) overview\nNB! Metrics that are actually gathered need to be configured for every DB separately - for that open the Web UI config page or modify the "pgwatch2.monitored_host" table directly in the "pgwatch2" database\nbackends - active, total, waiting sessions\npgbouncer_stats - pgbouncer (1.8+) statistics\nbgwriter - pg_stat_bgwriter snapshots\nblocking_locks - detailed info on sessions that are waiting\ncpu_load - CPU load info acquired via a plpython sproc (/pgwatch2/sql/metric_fetching_helpers/)\ndb_stats - pg_stat_database snapshots + DB size info\nindex_stats - pg_stat_user_indexes snapshots\nkpi - most important high level metrics\nlocks - different locktype (page, tuple, ...) counts. NB! for usable data one should set the polling interval very low\nlocks_mode - different lock-mode (exclusive, share) counts. NB! for usable data one should set the polling interval very low\nreplication - pg_stat_replication info (including replica lag)\nsproc_stats - pg_stat_user_functions snapshots\ntable_io_stats - pg_statio_user_tables snapshots\nstat_statements - pg_stat_statements snapshots (requires the extension)\nstat_statements_calls - total query count according to pg_stat_statements\ntable_bloat_approx_summary - bloat summary for the whole DB (needs pgstattuple extension)\ntable_stats - pg_stat_user_tables snapshots\nwal - pg_current_(xlog_location|wal_lsn) values\n\nFor getting started with Grafana in general start here\nFor learning InfluxDB query language InfluxQL start here\nWhen stuck then additional support and consultations are available from Cybertec here\n')])])]),a("h4",{attrs:{id:"_3-1-3-lesson-learned"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-3-lesson-learned"}},[t._v("#")]),t._v(" 3.1.3 Lesson Learned")]),t._v(" "),a("h5",{attrs:{id:"_3-1-3-1-auth"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-3-1-auth"}},[t._v("#")]),t._v(" 3.1.3.1 auth")]),t._v(" "),a("p",[t._v("So many auth in so many places!!!\nGrafana - add datasource - Auth")]),t._v(" "),a("p",[t._v("This is different for varaires datasource, need to specifiy accoridingly")]),t._v(" "),a("p",[t._v("For example, for prometheus, it has a default web portal, basic auth is to protect it from access by non authenticated user,\nAnd also protect from http api call\nSecuring Prometheus with Basic Auth for Grafana https://www.youtube.com/watch?time_continue=431&v=oPlk0GHYmrE")]),t._v(" "),a("p",[t._v("In the influxdb example here, we didn’t user basic auth, because:")]),t._v(" "),a("ol",[a("li",[t._v("In latest influxdb, it already removed the web portal")]),t._v(" "),a("li",[t._v("Influxdb has its own authentication, by config:\nEnable auth will affect current pgwatch2, so we need add deafult user root for pgwatch2")])]),t._v(" "),a("p",[t._v("Create user admin with password ‘123456’ with all priveleges;\n/etc/influxdb/influxdb.conf\nAfter restart,  influx -precision rfc3339 -username admin -password 123456")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor14.png",alt:""}})]),t._v(" "),a("h5",{attrs:{id:"_3-1-3-2-docker-local-volumes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-3-2-docker-local-volumes"}},[t._v("#")]),t._v(" 3.1.3.2 docker local volumes")]),t._v(" "),a("p",[t._v("https://github.com/cybertec-postgresql/pgwatch2/blob/master/Dockerfile\nhttps://dzone.com/articles/demystifying-the-data-volume-storage-in-docker\nhttps://docs.docker.com/engine/reference/commandline/volume_inspect/")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor15.png",alt:""}})]),t._v(" "),a("h5",{attrs:{id:"_3-1-3-3-influxdb"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-3-3-influxdb"}},[t._v("#")]),t._v(" 3.1.3.3 influxdb")]),t._v(" "),a("p",[t._v("https://docs.influxdata.com/influxdb/v1.6/query_language/schema_exploration/\n#retention policy\nhttps://docs.influxdata.com/influxdb/v1.2/query_language/database_management/#create-retention-policies-with-create-retention-policy，https://community.influxdata.com/t/what-is-the-retention-policy-and-how-exactly-it-work/1080/3\nQuerying data in a non-DEFAULT retention policy https://www.influxdata.com/blog/tldr-influxdb-tech-tips-april-27-2017/\nRetention policies not dropping old data https://community.influxdata.com/t/retention-policies-not-dropping-old-data/4538")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor16.png",alt:""}})]),t._v(" "),a("h5",{attrs:{id:"_3-1-3-4-shift-to-existing-grafana"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-3-4-shift-to-existing-grafana"}},[t._v("#")]),t._v(" 3.1.3.4 Shift to existing grafana")]),t._v(" "),a("ol",[a("li",[t._v("PW2_GRAFANA_BASEURL doesn’t work\nNeed to check scripts")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("sudo docker run -d -p 8080:8080 -p 8086:8086 -p 9001:9001 -p 3001:3000 -e PW2_GRAFANA_BASEURL='http://10.20.70.205:3000' --name pw2 cybertec/pgwatch2\n")])])]),a("ol",{attrs:{start:"2"}},[a("li",[t._v('instructions not clear\ncheck section "To use an existing Grafana installation" in https://github.com/cybertec-postgresql/pgwatch2')])]),t._v(" "),a("h5",{attrs:{id:"_3-1-3-5-backup-upgrade"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-3-5-backup-upgrade"}},[t._v("#")]),t._v(" 3.1.3.5 backup & upgrade")]),t._v(" "),a("p",[t._v("Need to explore and refer to “Updating to a newer Docker version” (https://github.com/cybertec-postgresql/pgwatch2)")]),t._v(" "),a("h4",{attrs:{id:"_3-1-4-trouble-shoot"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-4-trouble-shoot"}},[t._v("#")]),t._v(" 3.1.4 Trouble shoot")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('cd /tmp/\nwget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.2.2-1.x86_64.rpm\nsudo yum localinstall grafana-5.2.2-1.x86_64.rpm\n\ngit clone https://github.com/cybertec-postgresql/pgwatch2.git\nsudo git clone https://github.com/cybertec-postgresql/pgwatch2.git\n\nsudo docker pull cybertec/pgwatch2\nsudo docker images\n\nsudo netstat -anp|grep :8080\n##sudo docker run -d -p 8080:8080 -p 9001:9001 -p 3001:3000 --name pw2 cybertec/pgwatch2\n##sudo docker run -d -p 8080:8080 -p 9001:9001 -p 3001:3000 -e PW2_GRAFANA_BASEURL=\'http://10.20.70.205:3000\' --name pw2 cybertec/pgwatch2\nsudo docker run -d -p 8080:8080 -p 8086:8086 -p 9001:9001 -p 3001:3000 --name pw2 cybertec/pgwatch2\nsudo docker ps -a\n\n##    sudo vi /etc/grafana/grafana.ini\n##   \t ;type = postgres\n##   \t ;host = 10.20.70.168:6432\n##   \t ;name = oureadb\n##   \t ;user = pgwatch2\n##   \t # If the password contains # or ; you have to wrap it with trippel quotes. Ex """#password;"""\n##   \t ;password = secret\n\nsudo docker exec -ti pw2 /bin/bash\n    apt-get install net-tools\n    service supervisor status\n    ps -lef|grep "super"\n   \t /usr/bin/supervisord --configuration=/etc/supervisor/supervisord.conf --nodaemon\n    vi /etc/supervisor/supervisord.conf\n   \t [inet_http_server]\n   \t port = 9001\n   \t username = peter\n   \t password = 123456\n    service supervisor restart\n\t    influx -precision rfc3339\n\t\tinflux -precision rfc3339 -username admin -password 123456\n   \t        show databases\n\t\t\tshow grants for root;\n\t\t\tgrant all on pgwatch2 to root;\n   \t        use pgwatch2\n\t\t\tshow measurements\n\t\t\tselect * from backends limit 1\n       SHOW TAG VALUES WITH KEY = "dbname" where "dbname" !~ /(test)+/\n\tvi /pgwatch2/webpy/pgwatch2_influx.py\n\t\tmodify influx_connect_params\nsudo docker start pw2    \n    netstat -anp|grep :9001\n    netstat -anp|egrep :8080\n    ps -lef|egrep "35|PID"\n    ls -l /proc/35/exe\n    cat /proc/35/cmdline | xargs -0 echo\n    ps -p 35 -o cmd\n    vi /etc/grafana/grafana.ini\n   \t [server]\n   \t protocol = http\n   \t cert_file = /pgwatch2/persistent-config/self-signed-ssl.pem\n   \t cert_key = /pgwatch2/persistent-config/self-signed-ssl.key\n\n   \t [database]\n   \t type = postgres\n   \t host = 127.0.0.1:5432\n   \t name = pgwatch2_grafana\n   \t user = pgwatch2\n   \t password = pgwatch2admin\n\n   \t [security]\n   \t admin_user = admin\n   \t admin_password = pgwatch2admin\n\n   \t [auth.anonymous]\n   \t enabled = true\n\n   \t [metrics]\n   \t enabled = false\n\nsudo docker top 0c7180296ad6    \n\nsudo systemctl status grafana-server\nsudo systemctl start grafana-server\njournalctl -xe\n\ntelnet 10.20.70.168 6432\njournalctl -xe\n\nhttp://10.20.70.205:8080/dbs\noureadb\n10.20.70.168 6432 oureadb pgwatch2 secret\n\n')])])]),a("p",[t._v("?# postgresql connection timeout\nTest on monitor server(running docker), try to connect use pgql\nyum localinstall https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-7-x86_64/pgdg-centos11-11-2.noarch.rpm\nyum list postgres*\nyum install postgresql11.x86_64\npsql -h "),a("IP",[t._v(" -p "),a("Port",[t._v(" -U pgwatch2 -W -d "),a("Database",[t._v("\nWorks fine on the host server, it turns out that the docker get network connectivity issue,\nTest by wget google.com\nhttps://dataonthego.wordpress.com/2015/09/05/install-postgresql-client-only-2/\nhttps://wiki.postgresql.org/wiki/YUM_Installation#Install_PGDG_RPM_file\nhttps://yum.postgresql.org/repopackages.php\nhttps://stackoverflow.com/questions/20430371/my-docker-container-has-no-internet")])],1)],1)],1),t._v(" "),a("p",[t._v("?# prometheus not work => caused by server diskspace used up => caused by pgwatch2 influxdb use up space=>caused by pgwatch metrics preset config(full will scrape too much data)\nServer date ahead of real datetime\nRoot disk space used up\nInfluxdb used up space\nvim /etc/influxdb/influxdb.conf\tmodify check-interval\ndu -h /var/lib/influxdb/data\n"),a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor17.png",alt:""}})]),t._v(" "),a("p",[t._v('?# Grafana did not display the records in the expected order in table\nvery simple solution :\nclick on max/average whatever column you want to sort, click it once or twice until the result is sorted in your expected order, then save, so next time when you open it, it will display in expected sort order,\nactually what happens is the grafana json file will change to for example:\n"legend": {\n"alignAsTable": true,\n"avg": true,\n"current": true,\n"max": true,\n"min": false,\n"rightSide": true,\n"show": true,\n"sideWidth": 300,\n"sort": "max",\n"sortDesc": true,\n"total": true,\n"values": true\n},\nppl asking and discussing it here\nhttps://community.grafana.com/t/grafana-did-not-display-the-records-in-the-expected-order-in-table/4564\nand\nhttps://groups.io/g/grafana/topic/prometheus_data_not_sorted/4314229?p=,,,20,0,0,0::recentpostdate%2Fsticky,,,20,0,0,4314229')]),t._v(" "),a("h3",{attrs:{id:"_3-2-node-exporter-server-metrics"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-node-exporter-server-metrics"}},[t._v("#")]),t._v(" 3.2 Node Exporter Server Metrics")]),t._v(" "),a("h4",{attrs:{id:"_3-2-1-linux"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-1-linux"}},[t._v("#")]),t._v(" 3.2.1 Linux")]),t._v(" "),a("p",[t._v("Install node_exporter released for prometheus and config in prometheus\nhttps://prometheus.io/docs/guides/node-exporter/#installing-and-running-the-node-exporter\nGrafana dashboard")]),t._v(" "),a("p",[t._v("Check http://"),a("em",[t._v(".")]),t._v("."),a("em",[t._v(".")]),t._v(":9100/metrics")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("OR\nbuilding running from source:\ngo get github.com/prometheus/node_exporter\ncd ${GOPATH-$HOME/go}/src/github.com/prometheus/node_exporter\nmake\n./node_exporter\n\n")])])]),a("p",[t._v("Add alias, never change original variables,")]),t._v(" "),a("p",[t._v("?# node_exporter(latest Version 0.16.0) not showing on dashbaord\nBecause of naming changed https://www.robustperception.io/new-features-in-node-exporter-0-16-0\nUse the latest grafana dashboard https://grafana.com/dashboards/1860\nAnd check from prometheus web portal to confirm the query statement")]),t._v(" "),a("p",[t._v("?# # multiple series error\nFixed by itself after a while")]),t._v(" "),a("p",[t._v("Not verified link:\nhttp://d-prototype.com/archives/9672")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor18.png",alt:""}})]),t._v(" "),a("h4",{attrs:{id:"_3-2-2-windows"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-2-windows"}},[t._v("#")]),t._v(" 3.2.2 Windows")]),t._v(" "),a("p",[t._v("Windows support is removed by node exporter, the wmi_exporter is recommended as a replacement.\nhttps://github.com/martinlindhe/wmi_exporter\nhttps://grafana.com/dashboards/2129")]),t._v(" "),a("h3",{attrs:{id:"_3-3-server-alive-monitor-http-request-200-ok"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-server-alive-monitor-http-request-200-ok"}},[t._v("#")]),t._v(" 3.3 Server alive monitor - http request 200 OK")]),t._v(" "),a("p",[t._v("Endpoint prob - Blackbox-exporter\nhttps://github.com/prometheus/blackbox_exporter")]),t._v(" "),a("p",[t._v("https://cloudprober.org/how-to/external-probe/")]),t._v(" "),a("p",[t._v("https://grafana.com/dashboards/5345")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor19.png",alt:""}})]),t._v(" "),a("p",[t._v("Alert\nhttp://docs.grafana.org/alerting/notifications/#all-supported-notifier")]),t._v(" "),a("h3",{attrs:{id:"_3-4-more-prometheus-exporter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-more-prometheus-exporter"}},[t._v("#")]),t._v(" 3.4 More prometheus exporter")]),t._v(" "),a("p",[t._v("Btc exporter\nhttps://grafana.com/dashboards/6973\nhttps://github.com/hunterlong/btcexporter")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("docker run -it -d -p 9019:9019 -v /opt/btc/btcexporter/addresses.txt:/app/addresses.txt hunterlong/btcexporter\n\nps -lef|grep “prome”\nls /proc/<PID>/exe\nvi /path/prometheus.yml\nkill -HUP <PID>\n")])])]),a("p",[t._v("http://10.20.70.205:9019/metrics")]),t._v(" "),a("p",[t._v("how to Customize exporter?")]),t._v(" "),a("p",[t._v("promethus lib\nhttps://prometheus.io/docs/instrumenting/writing_exporters/\npython\nhttps://github.com/prometheus/client_python#custom-collectors\ngo\nhttps://redbyte.eu/en/blog/real-time-metrics-using-prometheus-and-grafana/")]),t._v(" "),a("h3",{attrs:{id:"_3-5-more-monitor-with-prometheus-influxdb"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-more-monitor-with-prometheus-influxdb"}},[t._v("#")]),t._v(" 3.5 More Monitor with prometheus|influxdb")]),t._v(" "),a("p",[t._v("Real Time performance monitor for .NET CORE\n.Net Core 2.0+ InfluxDB+Grafana+App Metrics 实现跨平台的实时性能监控 https://www.cnblogs.com/landonzeng/p/7904402.html")]),t._v(" "),a("p",[t._v("http request/traffic | web api monitor\nnodejs\nhttp://swaggerstats.io/docs.html")]),t._v(" "),a("p",[t._v("java perform monitor\nhttps://github.com/stagemonitor/stagemonitor/wiki/Installation")]),t._v(" "),a("h3",{attrs:{id:"_3-6-project-monitor-ci-with-jenkins-nothing-to-do-with-prometheus-influxdb"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-project-monitor-ci-with-jenkins-nothing-to-do-with-prometheus-influxdb"}},[t._v("#")]),t._v(" 3.6 Project Monitor CI with Jenkins(nothing to do with prometheus&influxdb)")]),t._v(" "),a("p",[t._v("CI-Jenkins\nhttps://jenkins.io/doc/tutorials/")]),t._v(" "),a("p",[a("img",{attrs:{src:"/docs/docs_image/software/project_manage/monitor/monitor07.png",alt:""}})]),t._v(" "),a("p",[t._v("Docker mode:")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('docker pull jenkinsci/blueocean\ndocker run \\\n  --rm \\\n  -u root \\\n  -p 8080:8080 \\\n  -v jenkins-data:/var/jenkins_home \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v "$HOME":/home \\\n  jenkinsci/blueocean\ndocker exec -ti 0dc7a2730c43 bash\n\n')])])]),a("p",[t._v("Find initial password:\ndocker log 0dc7a2730c43\nUI:\nhttp://192.168.56.101:8080/\nWork path:\n/var/jenkins_home/workspace")]),t._v(" "),a("p",[t._v("Use Maven::\n?#maven not found\nhttps://my.oschina.net/u/2450666/blog/844170\nhttps://stackoverflow.com/questions/45777031/maven-not-found-in-jenkins/56833922#56833922")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("export MAVEN_HOME=/opt/maven\nexport PATH=$PATH:$MAVEN_HOME/bin\nmvn --version\nmvn clean compile\nmvn clean install\nmvn clean package -P dev\n")])])]),a("p",[t._v('Use Sonarqube::\nhttps://docs.sonarqube.org/latest/setup/get-started-2-minutes/\nJava version requirements: https://docs.sonarqube.org/7.8/requirements/requirements/\njava.io.IOException: Cannot run program "sonar-scanner" error=2, No such file or directory\nhttps://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-jenkins/\nJenkins+sonar+sonar-scanner https://www.jianshu.com/p/27e6ed4f6dbc')]),t._v(" "),a("p",[t._v("Start sonar.sh on host machine:\ncd /home/test/workspace/sonarqube\n./sonarqube-7.8/bin/linux-x86-64/sonar.sh console")]),t._v(" "),a("p",[t._v("Install sonarscanner on docker/jenkins\nsudo docker cp /home/test/workspace/test/sourcecode/ 0dc7a2730c43:/home/workspace/")]),t._v(" "),a("p",[t._v("Pipline\nhttps://jenkins.io/doc/tutorials/build-a-java-app-with-maven/")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("ref")]),t._v(" "),a("p",[t._v("实战 Prometheus 搭建监控系统 https://www.codercto.com/a/35819.html")])])}),[],!1,null,null,null);e.default=n.exports}}]);