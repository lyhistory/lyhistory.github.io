(window.webpackJsonp=window.webpackJsonp||[]).push([[235],{675:function(e,a,t){"use strict";t.r(a);var n=t(65),r=Object(n.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("https://flink.apache.org/")]),e._v(" "),t("p",[t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_home_graphic.png",alt:""}})]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Is Flink a Hadoop Project?")]),e._v("\nFlink is a data processing system and an alternative to Hadoop’s MapReduce component. It comes with its own runtime rather than building on top of MapReduce. As such, it can work completely independently of the Hadoop ecosystem. However, Flink can also access Hadoop’s distributed file system (HDFS) to read and write data, and Hadoop’s next-generation resource manager (YARN) to provision cluster resources. Since most Flink users are using Hadoop HDFS to store their data, Flink already ships the required libraries to access HDFS.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Do I have to install Apache Hadoop to use Flink?")]),e._v("\nNo. Flink can run without a Hadoop installation. However, a very common setup is to use Flink to analyze data stored in the Hadoop Distributed File System (HDFS). To make these setups work out of the box, Flink bundles the Hadoop client libraries by default.")]),e._v(" "),t("p",[e._v("Additionally, we provide a special YARN Enabled download of Flink for users with an existing Hadoop YARN cluster. Apache Hadoop YARN is Hadoop’s cluster resource manager that allows use of different execution engines next to each other on a cluster.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("An Alternative to Hadoop MapReduce?")]),e._v("\nApache Flink is considered an alternative to Hadoop MapReduce. Flink offers cyclic data, a flow which is missing in MapReduce. Flink offers APIs, which are easier to implement compared to MapReduce APIs. It supports in-memory processing, which is much faster. Flink is also capable of working with other file systems along with HDFS. Flink can analyze real-time stream data along with graph processing and using machine learning algorithms. It also extends the MapReduce model with new operators like join, cross and union. Flink offers lower latency, exactly one processing guarantee, and higher throughput. Flink is also considered as an alternative to Spark and Storm. (To learn more about Spark, see How Apache Spark Helps Rapid Application Development.)")])])]),e._v(" "),t("p",[e._v("Flink可以完全独立于Hadoop，在不依赖Hadoop组件下运行。但是做为大数据的基础设施，Hadoop体系是任何大数据框架都绕不过去的。Flink可以集成众多Hadooop 组件，例如Yarn、Hbase、HDFS等等。例如，Flink可以和Yarn集成做资源调度，也可以读写HDFS，或者利用HDFS做检查点")]),e._v(" "),t("h2",{attrs:{id:"_1-intro"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-intro"}},[e._v("#")]),e._v(" 1. Intro")]),e._v(" "),t("h3",{attrs:{id:"_1-1-architecture"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-architecture"}},[e._v("#")]),e._v(" 1.1 Architecture")]),e._v(" "),t("p",[e._v("Flink is a distributed system and requires effective allocation and management of compute resources in order to execute streaming applications. It integrates with all common cluster resource managers such as Hadoop YARN and Kubernetes, but can also be set up to run as a standalone cluster or even as a library.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://nightlies.apache.org/flink/flink-docs-release-1.15/fig/processes.svg",alt:""}})]),e._v(" "),t("p",[e._v("The Client is not part of the runtime and program execution, but is used to prepare and send a dataflow to the JobManager. After that, the client can disconnect (detached mode), or stay connected to receive progress reports (attached mode). The client runs either as part of the Java/Scala program that triggers the execution, or in the command line process ./bin/flink run ....")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/concepts/flink-architecture/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Flink Architecture"),t("OutboundLink")],1)])]),e._v(" "),t("p",[e._v("Flink 集群是由 JobManager（JM）、TaskManager（TM）两大组件组成的，每个 JM/TM 都是运行在一个独立的 JVM 进程中。JM 相当于 Master，是集群的管理节点，TM 相当于 Worker，是集群的工作节点，每个 TM 最少持有 1 个 Slot，Slot 是 Flink 执行 Job 时的最小资源分配单位，在 Slot 中运行着具体的 Task 任务。")]),e._v(" "),t("ul",[t("li",[e._v("Flink Memory Model\n"),t("ul",[t("li",[t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/memory/mem_setup/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Set up Flink’s Process Memory"),t("OutboundLink")],1),e._v("\nTotal Process Memory(The total process memory of Flink JVM processes) =\nJVM Off heap Memory to run the flink application(means the job manager)[=JVM Metaspace+JVM Overhead] + Total Flink Memory(consumed by the flink application)[=JVM heap + Off heap Memory(direct/native memory)]")])]),e._v(" "),t("li",[t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/memory/mem_setup_tm/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Set up TaskManager Memory"),t("OutboundLink")],1),e._v("\nthe TaskManager memory components have a similar but more sophisticated structure compared to the memory model of the JobManager process.")]),e._v(" "),t("p",[e._v("Total Process Memory(configed e.g taskmanager.memory.process.size:16G) =\nJVM Specific Memory to run the flink application(means the taskmanager)\n+\nTotal Flink Memory(consumed by the flink application)\nJVM Specific Memory=\nJVM Metaspace(taskmanager.memory.jvm-metaspace.size:256M)\n+\nJVM Overhead(taskmanager.memory.jvm-overhead.fraction,taskmanager.memory.jvm-overhead.max)\nJVM Overhead= 0.1*totalProcessMemory=1.6G>1G=1G\nTotal Flink Memory=\nJVM Heap{=Framework Heap(taskmanager.memory.framework.heap.size)+Task Heap}\n+\nOff-Heap Memory\nTotal Flink Memory=Total Process Memory-JVM Metaspace - JVM Overhead=16G-256M-1G=14.75G\nOff-Heap Memory=\nManaged Memory(taskmanager.memory.managed.fraction,taskmanager.memory.managed.size)\n+\nDirect Memory\n"),t("em",[e._v("The following workloads use managed memory:")]),e._v("\na)  Streaming jobs can use it for RocksDB state backend.\nb)  Both streaming and batch jobs can use it for sorting, hash tables, caching of intermediate results.\nc)  Both streaming and batch jobs can use it for executing User Defined Functions in Python processes\nDirect Memory=\nFramework Off-heap(taskmanager.memory.framework.off-heap.size)\n+\nTask Off-heap(taskmanager.memory.task.off-heap.size)\n+\nNetwork(taskmanager.memory.network.fraction,taskmanager.memory.network.max)\nManaged Memory=0.4*totalFlinkMemory=0.4*14.75=5.9G\nNetwork Memory=0.1*totalFlinkMemory=0.1*14.75=1.475G>1G=1G\nDirect Memory=128M+0bytes+1G=1.125G\nOff-Heap Memory=5.9G+1.125G=7.025G\nJVM Heap(Framework Heap+Task Heap)=Total Flink Memory-Off-Heap Memory=14.75G-7.025G=7.725G=7.73G\nTask Heap=JVM Heap-Framework Heap=7.725G-128M=7.6G")]),e._v(" "),t("p",[e._v("Note: If you want to guarantee that a certain amount of JVM Heap is available for your user code, you can set the task heap memory explicitly (taskmanager.memory.task.heap.size). It will be added to the JVM Heap size and will be dedicated to Flink’s operators running the user code.\nspecify explicitly both task heap and managed memory. It gives more control over the available JVM Heap to Flink’s tasks and its managed memory.")])])])])]),e._v(" "),t("h4",{attrs:{id:"_1-1-1-jobmanager"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-1-jobmanager"}},[e._v("#")]),e._v(" 1.1.1 JobManager")]),e._v(" "),t("p",[e._v("The JobManager has a number of responsibilities related to coordinating the distributed execution of Flink Applications: it decides when to schedule the next task (or set of tasks), reacts to finished tasks or execution failures, coordinates checkpoints, and coordinates recovery on failures, among others. This process consists of three different components:")]),e._v(" "),t("h5",{attrs:{id:"resourcemanager"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#resourcemanager"}},[e._v("#")]),e._v(" ResourceManager")]),e._v(" "),t("p",[e._v("is responsible for resource de-/allocation and provisioning in a Flink cluster — it manages task slots, which are the unit of resource scheduling in a Flink cluster (see TaskManagers). Flink implements multiple ResourceManagers for different environments and resource providers such as YARN, Kubernetes and standalone deployments. In a standalone setup, the ResourceManager can only distribute the slots of available TaskManagers and cannot start new TaskManagers on its own.")]),e._v(" "),t("h5",{attrs:{id:"dispatcher"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dispatcher"}},[e._v("#")]),e._v(" Dispatcher")]),e._v(" "),t("p",[e._v("provides a REST interface to submit Flink applications for execution and starts a new JobMaster for each submitted job. It also runs the Flink WebUI to provide information about job executions.")]),e._v(" "),t("h5",{attrs:{id:"jobmaster"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#jobmaster"}},[e._v("#")]),e._v(" JobMaster")]),e._v(" "),t("p",[e._v("is responsible for managing the execution of a single JobGraph. Multiple jobs can run simultaneously in a Flink cluster, each having its own JobMaster.")]),e._v(" "),t("h4",{attrs:{id:"_1-1-2-taskmanagers"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-2-taskmanagers"}},[e._v("#")]),e._v(" 1.1.2 TaskManagers")]),e._v(" "),t("h5",{attrs:{id:"flink-job"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flink-job"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Flink Job")])]),e._v(" "),t("p",[e._v("A Flink Job is the runtime representation of a logical graph (also often called dataflow graph) that is created and submitted by calling execute() in a Flink Application.\n一个Job代表一个可以独立提交给Flink执行的作业，我们向JobManager提交任务的时候就是以Job为单位的，只不过一份代码里可以包含多个Job（每个Job对应一个类的main函数）\nExample:\n"),t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_wordcount.png",alt:"https://medium.com/@tirthshah100/word-count-in-apache-hadoop-mapreduce-c6ee8e737fb9#/"}})]),e._v(" "),t("h5",{attrs:{id:"jobgraph-logical-graph"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#jobgraph-logical-graph"}},[e._v("#")]),e._v(" "),t("strong",[e._v("JobGraph / Logical Graph")])]),e._v(" "),t("p",[e._v("A logical graph is a directed graph where the nodes are Operators and the edges define input/output-relationships of the operators and correspond to data streams or data sets. A logical graph is created by submitting jobs from a Flink Application.\nLogical graphs are also often referred to as dataflow graphs.")]),e._v(" "),t("h5",{attrs:{id:"executiongraph-physical-graph"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#executiongraph-physical-graph"}},[e._v("#")]),e._v(" "),t("strong",[e._v("ExecutionGraph/Physical Graph")])]),e._v(" "),t("p",[e._v("A physical graph is the result of translating a Logical Graph for execution in a distributed runtime. The nodes are Tasks and the edges indicate input/output-relationships or partitions of data streams or data sets.")]),e._v(" "),t("h5",{attrs:{id:"tm-task-manager"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#tm-task-manager"}},[e._v("#")]),e._v(" "),t("strong",[e._v("TM: Task Manager")])]),e._v(" "),t("ul",[t("li",[e._v("is a JVM process, (also called workers) execute the tasks of a dataflow, and buffer and exchange the data streams. There must always be at least one TaskManager. Each worker (TaskManager) is a JVM process, and may execute one or more subtasks in separate threads. To control how many tasks a TaskManager accepts, it has so called task slots (at least one).")])]),e._v(" "),t("h5",{attrs:{id:"ts-task-slot"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ts-task-slot"}},[e._v("#")]),e._v(" "),t("strong",[e._v("TS: Task Slot")])]),e._v(" "),t("ul",[t("li",[e._v("each TS represents a fixed subset of resources of the TaskManager (No CPU isolation happens between the slots, just the managed memory is divided.)\nThe smallest unit of resource scheduling in a TaskManager is a task slot. The number of task slots in a TaskManager indicates the number of concurrent processing tasks. Note that multiple operators may execute in a task slot")])]),e._v(" "),t("blockquote",[t("p",[e._v("One Slot is not one thread. One slot can have multiple threads. A Task can have multiple parallel instances which are called Sub-tasks. Each sub-task is ran in a separate thread. Multiple sub-tasks from different tasks can come together and share a slot. This group of sub-tasks is called a slot-sharing group. Please note that two sub-tasks of the same task (parallel instances of the same task) can not share a slot together.\nhttps://stackoverflow.com/questions/61791811/how-to-understand-slot-and-task-in-apache-flink")])]),e._v(" "),t("p",[e._v("Each task slot represents a fixed subset of resources of the TaskManager. A TaskManager with three slots, for example, will dedicate 1/3 of its managed memory to each slot. Slotting the resources means that a subtask will not compete with subtasks from other jobs for managed memory, but instead has a certain amount of reserved managed memory. Note that no CPU isolation happens here; currently slots only separate the managed memory of tasks.")]),e._v(" "),t("p",[e._v("By adjusting the number of task slots, users can define how subtasks are isolated from each other. Having one slot per TaskManager means that each task group runs in a separate JVM (which can be started in a separate container, for example). Having multiple slots means more subtasks share the same JVM. Tasks in the same JVM share TCP connections (via multiplexing) and heartbeat messages. They may also share data sets and data structures, thus reducing the per-task overhead.")]),e._v(" "),t("h5",{attrs:{id:"task"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#task"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Task")])]),e._v(" "),t("ul",[t("li",[e._v("Node of a Physical Graph.")])]),e._v(" "),t("p",[e._v("A task is the basic unit of work, which is executed by Flink’s runtime. Tasks encapsulate exactly one parallel instance of an Operator or Operator Chain.\nFor distributed execution, Flink chains operator subtasks together into tasks.")]),e._v(" "),t("p",[e._v("Task是逻辑概念，一个Operator就代表一个Task（多个Operator被chain之后产生的新Operator算一个Operator）, 真正运行的时候，Task会按照并行度分成多个Subtask，Subtask是执行/调度的基本单元,每个Subtask需要一个线程（Thread）来执行。")]),e._v(" "),t("p",[e._v("A Sub-Task is a Task responsible for processing a partition of the data stream. The term “Sub-Task” emphasizes that there are multiple parallel Tasks for the same Operator or Operator Chain. Each subtask is executed by one thread.\n"),t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_operator_chaining.png",alt:""}})]),e._v(" "),t("p",[e._v("A task is an abstraction representing a chain of operators that could be executed in a single thread. Something like a keyBy (which causes a network shuffle to partition the stream by some key) or a change in the parallelism of the pipeline will break the chaining and force operators into separate tasks. In the diagram above, the application has three tasks.")]),e._v(" "),t("p",[e._v("举例：source.map().filter().sink() 如果parallel=1，会看到只有一个subtask 跑在一个slot里面，所有操作都是链接在里面，如果想打破chain，可以使用startNewChain，disableChaining 或者采用分区策略，让上下游的并行度不同")]),e._v(" "),t("p",[e._v("A subtask is one parallel slice of a task. This is the schedulable, runable unit of execution. In the diagram above, the application is to be run with a parallelism of two for the source/map and keyBy/Window/apply tasks, and a parallelism of one for the sink -- resulting in a total of 5 subtasks.")]),e._v(" "),t("p",[e._v("A job is a running instance of an application. Clients submit jobs to the jobmanager, which slices them into subtasks and schedules those subtasks for execution by the taskmanagers.")]),e._v(" "),t("p",[e._v("Update:")]),e._v(" "),t("p",[e._v("The community decided to re-align the definitions of task and sub-task to match how these terms are used in the code -- which means that task and sub-task now mean the same thing: exactly one parallel instance of an operator or operator chain. -- https://stackoverflow.com/questions/53610342/difference-between-job-task-and-subtask-in-flink")]),e._v(" "),t("p",[e._v("Note:\nTaskSlot = Thread only (!) if slot sharing is disabled. It is an optimization that is on by default and in most cases, you would want to keep it that way. It is more precise to say that an Operator Chain = a Thread.\nChaining operators together into tasks is a useful optimization: it reduces the overhead of thread-to-thread handover and buffering, and increases overall throughput while decreasing latency.")]),e._v(" "),t("p",[e._v("By default, Flink allows subtasks to share slots even if they are subtasks of different tasks, so long as they are from the same job. The result is that one slot may hold an entire pipeline of the job. Allowing this slot sharing has two main benefits:")]),e._v(" "),t("h5",{attrs:{id:"parallelism"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#parallelism"}},[e._v("#")]),e._v(" "),t("strong",[e._v("parallelism")])]),e._v(" "),t("ul",[t("li",[t("p",[e._v("A Flink cluster needs exactly as many task slots as the highest parallelism used in the job. No need to calculate how many tasks (with varying parallelism) a program contains in total. slot的个数不能多于cpu-cores")])]),e._v(" "),t("li",[t("p",[e._v("It is easier to get better resource utilization. Without slot sharing, the non-intensive source/map() subtasks would block as many resources as the resource intensive window subtasks. With slot sharing, increasing the base parallelism in our example from two to six yields full utilization of the slotted resources, while making sure that the heavy subtasks are fairly distributed among the TaskManagers.")])])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/parallel/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Operator Level / Execution Environment Level / Client Level / System Level"),t("OutboundLink")],1)]),e._v(" "),t("ul",[t("li",[e._v("Operator Level"),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nDataStream<String> text = [...]\nDataStream<Tuple2<String, Integer>> wordCounts = text\n    .flatMap(new LineSplitter())\n    .keyBy(0)\n    .timeWindow(Time.seconds(5))\n    .sum(1).setParallelism(5);\n\nwordCounts.print();\n\nenv.execute("Word Count Example");\noperators、data sources、data sinks都可以调用setParallelism()方法来设置parallelism\n')])])])]),e._v(" "),t("li",[e._v("Execution Environment Level"),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setParallelism(3);\n\nDataStream<String> text = [...]\nDataStream<Tuple2<String, Integer>> wordCounts = [...]\nwordCounts.print();\n\nenv.execute("Word Count Example");\n在ExecutionEnvironment里头可以通过setParallelism来给operators、data sources、data sinks设置默认的parallelism；如果operators、data sources、data sinks自己有设置parallelism则会覆盖ExecutionEnvironment设置的parallelism\n')])])])]),e._v(" "),t("li",[e._v("Client Level"),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('./bin/flink run -p 10 ../examples/*WordCount-java*.jar\n或者\n\ntry {\n    PackagedProgram program = new PackagedProgram(file, args);\n    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport("localhost:6123");\n    Configuration config = new Configuration();\n\n    Client client = new Client(jobManagerAddress, config, program.getUserCodeClassLoader());\n\n    // set the parallelism to 10 here\n    client.run(program, 10, true);\n\n} catch (ProgramInvocationException e) {\n    e.printStackTrace();\n}\n使用CLI client，可以在命令行调用是用-p来指定，或者Java/Scala调用时在Client.run的参数中指定parallelism\n')])])])]),e._v(" "),t("li",[e._v("System Level"),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("# The parallelism used for programs that did not specify and other parallelism.\n\nparallelism.default: 1\n可以在flink-conf.yaml中通过parallelism.default配置项给所有execution environments指定系统级的默认parallelism\n")])])])])]),e._v(" "),t("p",[e._v("example:")]),e._v(" "),t("p",[e._v("If run with parallelism of two in a cluster with 2 task managers, each offering 3 slots, the scheduler will use 5 task slots, like this:")]),e._v(" "),t("p",[t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_taskslot_example1.png",alt:""}})]),e._v(" "),t("p",[e._v("However, if the base parallelism is increased to six, then the scheduler will do this (note that the sink remains at a parallelism of one in this example):")]),e._v(" "),t("p",[t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_taskslot_example2.png",alt:""}})]),e._v(" "),t("p",[e._v("实测 1 task manager with 4 slots, run wordcount with p=2/3/4/5:\n"),t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_parallelism_wordcount.png",alt:""}}),e._v("\n这个测试很有意思，p=1的时候最快，p=2反而慢了（因为增加了任务分割和聚合的过程吧），p从2到3，用时降低在预期之内，但是p=4反而更久(后来又测了几次，这个耗时不稳定)，另外p>4居然也能成功，不过耗时变长，找到解释：")]),e._v(" "),t("blockquote",[t("p",[e._v("在Flink中，Slot和并行度是相互影响的。如果一个任务的并行度大于Slot的数量，那么这个任务就无法完全并行执行。在这种情况下，Flink会根据一定的算法将任务的子任务分配到不同的Slot中执行，从而实现部分并行执行。另外，如果一个任务的并行度小于Slot的数量，那么有些Slot可能会闲置，从而浪费资源。")])]),e._v(" "),t("p",[e._v("就是说实际上p=5是把并行度是5的子任务中只有4个是真正并行的，另外一个是放在等某个slots空闲的时候再跑")]),e._v(" "),t("p",[e._v("日志：\np=1")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2023-12-06 10:44:35,152 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 1040904f312d051825b9205caf4c87de for job cab7d3dd7306786f754237f2771c0a62 from resource manager with leader id 00000000000000000000000000000000.\n2023-12-06 10:44:35,152 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 1040904f312d051825b9205caf4c87de.\n..............................\n\n2023-12-06 10:44:35,206 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1040904f312d051825b9205caf4c87de.\n2023-12-06 10:44:35,207 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (1/1).\n2023-12-06 10:44:35,211 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/1) (4e89f2aef62b7d61bbafc9fff95cd9aa) switched from CREATED to DEPLOYING.\n")])])]),t("p",[e._v("p=2")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2023-12-06 10:44:43,328 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 5bd806ad550923769e95d4d86a4744e5 for job fd43c3b3c49c0afe0be4c6706b904cc3 from resource manager with leader id 00000000000000000000000000000000.\n2023-12-06 10:44:43,328 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 5bd806ad550923769e95d4d86a4744e5.\n...................................\n2023-12-06 10:44:43,365 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5bd806ad550923769e95d4d86a4744e5.\n2023-12-06 10:44:43,366 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (1/2).\n2023-12-06 10:44:43,370 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot bae54b7f4b9437bf81acafd292978241.\n2023-12-06 10:44:43,370 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot bae54b7f4b9437bf81acafd292978241.\n2023-12-06 10:44:43,371 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (2/2).\n2023-12-06 10:44:43,371 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/2) (1acde70402c0fe9c43e309b6f2f4b4cd) switched from CREATED to DEPLOYING.\n2023-12-06 10:44:43,373 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/2) (c347c9c47c4b81fb954311a874b7d79d) switched from CREATED to DEPLOYING.\n")])])]),t("p",[e._v("p=3")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2023-12-06 10:44:56,191 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 413728b6858e88e8be8ddeb986d865c1 for job 1851bfb11c5dc283209bbda14a5b7a91 from resource manager with leader id 00000000000000000000000000000000.\n2023-12-06 10:44:56,191 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 413728b6858e88e8be8ddeb986d865c1.\n.......................................\n2023-12-06 10:44:56,252 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 413728b6858e88e8be8ddeb986d865c1.\n2023-12-06 10:44:56,252 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (1/3).\n2023-12-06 10:44:56,253 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 46bc54661e712a95825369cac5cc6af6.\n2023-12-06 10:44:56,253 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 46bc54661e712a95825369cac5cc6af6.\n2023-12-06 10:44:56,254 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot c7a90293611545315fb34eefd29ce7d3.\n2023-12-06 10:44:56,254 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/3) (ced34a25137bf391bafb0044262f04ae) switched from CREATED to DEPLOYING.\n2023-12-06 10:44:56,254 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (1/3) (ced34a25137bf391bafb0044262f04ae) [DEPLOYING].\n2023-12-06 10:44:56,254 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 46bc54661e712a95825369cac5cc6af6.\n2023-12-06 10:44:56,254 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (1/3) (ced34a25137bf391bafb0044262f04ae) [DEPLOYING].\n2023-12-06 10:44:56,254 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (2/3).\n2023-12-06 10:44:56,254 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/3) (ced34a25137bf391bafb0044262f04ae) switched from DEPLOYING to RUNNING.\n2023-12-06 10:44:56,255 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot c7a90293611545315fb34eefd29ce7d3.\n2023-12-06 10:44:56,255 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (3/3).\n2023-12-06 10:44:56,256 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/3) (ae17aca0dbc22c6cc0b3ef75ec03e097) switched from CREATED to DEPLOYING.\n2023-12-06 10:44:56,256 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (2/3) (ae17aca0dbc22c6cc0b3ef75ec03e097) [DEPLOYING].\n2023-12-06 10:44:56,256 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (2/3) (ae17aca0dbc22c6cc0b3ef75ec03e097) [DEPLOYING].\n2023-12-06 10:44:56,256 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/3) (ae17aca0dbc22c6cc0b3ef75ec03e097) switched from DEPLOYING to RUNNING.\n2023-12-06 10:44:56,256 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (3/3) (a25fcb1abd5fa545eb5f055335661f2a) switched from CREATED to DEPLOYING.\n2023-12-06 10:44:56,256 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (3/3) (a25fcb1abd5fa545eb5f055335661f2a) [DEPLOYING].\n2023-12-06 10:44:56,257 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (3/3) (a25fcb1abd5fa545eb5f055335661f2a) [DEPLOYING].\n2023-12-06 10:44:56,257 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (3/3) (a25fcb1abd5fa545eb5f055335661f2a) switched from DEPLOYING to RUNNING.\n2023-12-06 10:44:56,297 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/3) (ae17aca0dbc22c6cc0b3ef75ec03e097) switched from RUNNING to FINISHED.\n2023-12-06 10:44:56,297 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Reduce (SUM(1), at main(WordCount.java:87) (2/3) (ae17aca0dbc22c6cc0b3ef75ec03e097).\n2023-12-06 10:44:56,298 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Reduce (SUM(1), at main(WordCount.java:87) (2/3) ae17aca0dbc22c6cc0b3ef75ec03e097.\n2023-12-06 10:44:56,299 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/3) (ced34a25137bf391bafb0044262f04ae) switched from RUNNING to FINISHED.\n2023-12-06 10:44:56,299 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Reduce (SUM(1), at main(WordCount.java:87) (1/3) (ced34a25137bf391bafb0044262f04ae).\n2023-12-06 10:44:56,299 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Reduce (SUM(1), at main(WordCount.java:87) (1/3) ced34a25137bf391bafb0044262f04ae.\n")])])]),t("p",[e._v("p=4")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("\n2023-12-06 10:45:07,403 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 02585101726cd4ee416a88da77fc4618 for job bc618f19d10f2973cbdb4e74b1def0f0 from resource manager with leader id 00000000000000000000000000000000.\n2023-12-06 10:45:07,404 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 02585101726cd4ee416a88da77fc4618.\n................................\n10:45:07,449 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 02585101726cd4ee416a88da77fc4618.\n2023-12-06 10:45:07,449 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (1/4).\n2023-12-06 10:45:07,452 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot eb4fbc442d7c400158fe9a9ebfa2691c.\n2023-12-06 10:45:07,452 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot eb4fbc442d7c400158fe9a9ebfa2691c.\n2023-12-06 10:45:07,452 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot fa67f55e86b4b171eb32e58df1ab73ac.\n2023-12-06 10:45:07,452 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot eb4fbc442d7c400158fe9a9ebfa2691c.\n2023-12-06 10:45:07,452 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot fa67f55e86b4b171eb32e58df1ab73ac.\n2023-12-06 10:45:07,452 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 4fb95f5d1a690c5954e6a0f4d4e96828.\n2023-12-06 10:45:07,453 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot eb4fbc442d7c400158fe9a9ebfa2691c.\n2023-12-06 10:45:07,453 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (2/4).\n2023-12-06 10:45:07,453 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/4) (a36fef48a5c8328355d9630b67947b14) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:07,465 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot fa67f55e86b4b171eb32e58df1ab73ac.\n2023-12-06 10:45:07,466 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (3/4).\n2023-12-06 10:45:07,466 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/4) (3d4d2bfeada831e87d50771c71809e63) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:07,468 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 4fb95f5d1a690c5954e6a0f4d4e96828.\n2023-12-06 10:45:07,468 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (4/4).\n2023-12-06 10:45:07,468 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (3/4) (8c8d42092ff829ac4aef87f9371116c4) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:07,470 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (1/4) (a36fef48a5c8328355d9630b67947b14) [DEPLOYING].\n2023-12-06 10:45:07,470 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (1/4) (a36fef48a5c8328355d9630b67947b14) [DEPLOYING].\n2023-12-06 10:45:07,470 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/4) (a36fef48a5c8328355d9630b67947b14) switched from DEPLOYING to RUNNING.\n2023-12-06 10:45:07,470 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (3/4) (8c8d42092ff829ac4aef87f9371116c4) [DEPLOYING].\n2023-12-06 10:45:07,470 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (3/4) (8c8d42092ff829ac4aef87f9371116c4) [DEPLOYING].\n2023-12-06 10:45:07,470 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (3/4) (8c8d42092ff829ac4aef87f9371116c4) switched from DEPLOYING to RUNNING.\n2023-12-06 10:45:07,470 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (4/4) (def950d6feb829be9cd06a581c6f3089) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:07,522 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (2/4) (3d4d2bfeada831e87d50771c71809e63) [DEPLOYING].\n2023-12-06 10:45:07,522 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (2/4) (3d4d2bfeada831e87d50771c71809e63) [DEPLOYING].\n2023-12-06 10:45:07,522 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/4) (3d4d2bfeada831e87d50771c71809e63) switched from DEPLOYING to RUNNING.\n2023-12-06 10:45:07,534 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (4/4) (def950d6feb829be9cd06a581c6f3089) [DEPLOYING].\n2023-12-06 10:45:07,535 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (4/4) (def950d6feb829be9cd06a581c6f3089) [DEPLOYING].\n2023-12-06 10:45:07,535 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (4/4) (def950d6feb829be9cd06a581c6f3089) switched from DEPLOYING to RUNNING.\n")])])]),t("p",[e._v("p=5")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2023-12-06 10:45:51,425 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 36baecc30276e70935cea5246ce9738d for job 1fd73f83a716da9f0b3aa769687287e2 from resource manager with leader id 00000000000000000000000000000000.\n2023-12-06 10:45:51,425 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 36baecc30276e70935cea5246ce9738d.\n..............................................\n2023-12-06 10:45:51,463 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 1fd73f83a716da9f0b3aa769687287e2.\n2023-12-06 10:45:51,463 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 36baecc30276e70935cea5246ce9738d.\n2023-12-06 10:45:51,464 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (1/5).\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f2b43e2529e1649dbdd931214c9f94ea.\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f2b43e2529e1649dbdd931214c9f94ea.\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 8158510413de83b2cf98f1b610fbc85d.\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f2b43e2529e1649dbdd931214c9f94ea.\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 8158510413de83b2cf98f1b610fbc85d.\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a305dcf6190e4f1ce377a8dd4bff7461.\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f2b43e2529e1649dbdd931214c9f94ea.\n2023-12-06 10:45:51,475 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (2/5).\n2023-12-06 10:45:51,489 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/5) (b42996ba5c2dd049f48bdf145a958284) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:51,490 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 8158510413de83b2cf98f1b610fbc85d.\n2023-12-06 10:45:51,491 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (3/5).\n2023-12-06 10:45:51,491 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/5) (d8864a3c570049ce4eca5a353564e6df) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:51,492 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a305dcf6190e4f1ce377a8dd4bff7461.\n2023-12-06 10:45:51,492 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (4/5).\n2023-12-06 10:45:51,492 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (3/5) (c5916f065223d42e912caaedfacf84c8) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:51,494 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (4/5) (130f7afca9aef9f118f3a8bf2ef0faf0) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:51,494 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (1/5) (b42996ba5c2dd049f48bdf145a958284) [DEPLOYING].\n2023-12-06 10:45:51,494 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (1/5) (b42996ba5c2dd049f48bdf145a958284) [DEPLOYING].\n2023-12-06 10:45:51,494 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/5) (b42996ba5c2dd049f48bdf145a958284) switched from DEPLOYING to RUNNING.\n2023-12-06 10:45:51,495 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (4/5) (130f7afca9aef9f118f3a8bf2ef0faf0) [DEPLOYING].\n2023-12-06 10:45:51,495 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (4/5) (130f7afca9aef9f118f3a8bf2ef0faf0) [DEPLOYING].\n2023-12-06 10:45:51,495 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (4/5) (130f7afca9aef9f118f3a8bf2ef0faf0) switched from DEPLOYING to RUNNING.\n2023-12-06 10:45:51,508 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (3/5) (c5916f065223d42e912caaedfacf84c8) [DEPLOYING].\n2023-12-06 10:45:51,509 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (3/5) (c5916f065223d42e912caaedfacf84c8) [DEPLOYING].\n2023-12-06 10:45:51,509 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (3/5) (c5916f065223d42e912caaedfacf84c8) switched from DEPLOYING to RUNNING.\n2023-12-06 10:45:51,522 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (2/5) (d8864a3c570049ce4eca5a353564e6df) [DEPLOYING].\n2023-12-06 10:45:51,522 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (2/5) (d8864a3c570049ce4eca5a353564e6df) [DEPLOYING].\n2023-12-06 10:45:51,522 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (2/5) (d8864a3c570049ce4eca5a353564e6df) switched from DEPLOYING to RUNNING.\n2023-12-06 10:45:51,524 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (1/5) (b42996ba5c2dd049f48bdf145a958284) switched from RUNNING to FINISHED.\n2023-12-06 10:45:51,524 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Reduce (SUM(1), at main(WordCount.java:87) (1/5) (b42996ba5c2dd049f48bdf145a958284).\n2023-12-06 10:45:51,524 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Reduce (SUM(1), at main(WordCount.java:87) (1/5) b42996ba5c2dd049f48bdf145a958284.\n2023-12-06 10:45:51,549 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (4/5) (130f7afca9aef9f118f3a8bf2ef0faf0) switched from RUNNING to FINISHED.\n2023-12-06 10:45:51,549 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Reduce (SUM(1), at main(WordCount.java:87) (4/5) (130f7afca9aef9f118f3a8bf2ef0faf0).\n2023-12-06 10:45:51,549 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Reduce (SUM(1), at main(WordCount.java:87) (4/5) 130f7afca9aef9f118f3a8bf2ef0faf0.\n2023-12-06 10:45:51,550 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (3/5) (c5916f065223d42e912caaedfacf84c8) switched from RUNNING to FINISHED.\n2023-12-06 10:45:51,550 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Reduce (SUM(1), at main(WordCount.java:87) (3/5) (c5916f065223d42e912caaedfacf84c8).\n2023-12-06 10:45:51,550 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Reduce (SUM(1), at main(WordCount.java:87) (3/5) c5916f065223d42e912caaedfacf84c8.\n2023-12-06 10:45:51,551 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f2b43e2529e1649dbdd931214c9f94ea.\n2023-12-06 10:45:51,551 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task DataSink (collect()) (1/5).\n2023-12-06 10:45:51,557 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 36baecc30276e70935cea5246ce9738d.\n\n2023-12-06 10:45:51,557 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Reduce (SUM(1), at main(WordCount.java:87) (5/5).\n.....\n\n2023-12-06 10:45:51,562 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (5/5) (98314de2f88839e8129230fdec72e1c0) switched from CREATED to DEPLOYING.\n2023-12-06 10:45:51,562 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Reduce (SUM(1), at main(WordCount.java:87) (5/5) (98314de2f88839e8129230fdec72e1c0) [DEPLOYING].\n2023-12-06 10:45:51,563 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Reduce (SUM(1), at main(WordCount.java:87) (5/5) (98314de2f88839e8129230fdec72e1c0) [DEPLOYING].\n2023-12-06 10:45:51,563 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Reduce (SUM(1), at main(WordCount.java:87) (5/5) (98314de2f88839e8129230fdec72e1c0) switched from DEPLOYING to RUNNING.\n.....................\n\n\n")])])]),t("p",[e._v("不过需要注意，再高就会出问题 "),t("a",{attrs:{href:"https://stackoverflow.com/questions/57732800/flink-fail-fast-if-job-parallelism-is-larger-than-the-total-number-of-slots",target:"_blank",rel:"noopener noreferrer"}},[e._v("Flink: fail fast if job parallelism is larger than the total number of slots"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("中文解读："),t("a",{attrs:{href:"https://www.jianshu.com/p/62fa262736b9",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apache Flink——任务（Tasks）和任务槽（Task Slots）"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://stackoverflow.com/questions/57732800/flink-fail-fast-if-job-parallelism-is-larger-than-the-total-number-of-slots",target:"_blank",rel:"noopener noreferrer"}},[e._v("Flink: fail fast if job parallelism is larger than the total number of slots"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://lists.apache.org/thread/yt9kd8lhgjvc6fcw4wozwl6nmbfhzsxz",target:"_blank",rel:"noopener noreferrer"}},[e._v("Re: Is there any way to set the parallelism of operators like group by, join?"),t("OutboundLink")],1)]),e._v(" "),t("h5",{attrs:{id:"operator-chaining"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#operator-chaining"}},[e._v("#")]),e._v(" "),t("strong",[e._v("operator chaining")])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("An Operator Chain consists of two or more consecutive Operators without any repartitioning in between. Operators within the same Operator Chain forward records to each other directly without going through serialization or Flink’s network stack.\nThe sample dataflow in the figure below is executed with five subtasks, and hence with five parallel threads:\n![](/docs/docs_image/software/bigdata/flink/flink_operator_chaining.png)\n\n![](/docs/docs_image/software/bigdata/flink/flink_operator_chaining2.png)\n\nhttp://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/\nhttps://stackoverflow.com/questions/62664972/what-happens-if-total-parallel-instances-of-operators-are-higher-than-the-parall\n")])])]),t("h5",{attrs:{id:"总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[e._v("#")]),e._v(" 总结：")]),e._v(" "),t("p",[e._v("一个程序Process可以运行在多个TM上，一个TM有多个TS（TS的总和代表支持的最高并行度），一个TS中可以运行多个sub task（task实例），每个subtask都对应一个Thread")]),e._v(" "),t("p",[e._v("对 TM 而言：它占用着一定数量的 CPU 和 Memory 资源，具体可通过 taskmanager.numberOfTaskSlots, taskmanager.heap.size 来配置，实际上 taskmanager.numberOfTaskSlots 只是指定 TM 的 Slot 数量，并不能隔离指定数量的 CPU 给 TM 使用。在不考虑 Slot Sharing的情况下，一个 Slot 内运行着一个 SubTask（Task 实现 Runable，SubTask 是一个执行 Task 的具体实例），所以官方建议 taskmanager.numberOfTaskSlots 配置的 Slot 数量和 CPU 相等或成比例。")]),e._v(" "),t("p",[e._v("当然，我们可以借助 Yarn 等调度系统，用 Flink On Yarn 的模式来为 Yarn Container 分配指定数量的 CPU 资源，以达到较严格的 CPU 隔离（Yarn 采用 Cgroup 做基于时间片的资源调度，每个 Container 内运行着一个 JM/TM 实例）。而 taskmanager.heap.size 用来配置 TM 的 Memory，如果一个 TM 有 N 个 Slot，则每个 Slot 分配到的 Memory 大小为整个 TM Memory 的 1/N，同一个 TM 内的 Slots 只有 Memory 隔离，CPU 是共享的。")]),e._v(" "),t("p",[e._v("对 Job 而言：一个 Job 所需的 Slot 数量大于等于 Operator 配置的最大 Parallelism 数，在保持所有 Operator 的 slotSharingGroup 一致的前提下 Job 所需的 Slot 数量与 Job 中 Operator 配置的最大 Parallelism 相等。")]),e._v(" "),t("h4",{attrs:{id:"_1-1-3-streamgraph-jobgraph-executiongraph"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-3-streamgraph-jobgraph-executiongraph"}},[e._v("#")]),e._v(" 1.1.3 StreamGraph/JobGraph/ExecutionGraph")]),e._v(" "),t("p",[t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_graphs1.png",alt:""}})]),e._v(" "),t("ul",[t("li",[t("p",[e._v("StreamGraph：根据用户通过 Stream API 编写的代码生成的最初的图。")]),e._v(" "),t("ul",[t("li",[e._v("StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。")]),e._v(" "),t("li",[e._v("StreamEdge：表示连接两个StreamNode的边。")])])]),e._v(" "),t("li",[t("p",[e._v("JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给JobManager 的数据结构。")]),e._v(" "),t("ul",[t("li",[e._v("JobVertex：经过优化后符合条件的多个StreamNode可能会chain在一起生成一个JobVertex，即一个JobVertex包含一个或多个operator，JobVertex的输入是JobEdge，输出是IntermediateDataSet。")]),e._v(" "),t("li",[e._v("IntermediateDataSet：表示JobVertex的输出，即经过operator处理产生的数据集。producer是JobVertex，consumer是JobEdge。")]),e._v(" "),t("li",[e._v("JobEdge：代表了job graph中的一条数据传输通道。source 是 IntermediateDataSet，target 是 JobVertex。即数据通过JobEdge由IntermediateDataSet传递给目标JobVertex。")])])]),e._v(" "),t("li",[t("p",[e._v("ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。")]),e._v(" "),t("ul",[t("li",[e._v("ExecutionJobVertex：和JobGraph中的JobVertex一一对应。每一个ExecutionJobVertex都有和并发度一样多的 ExecutionVertex。")]),e._v(" "),t("li",[e._v("ExecutionVertex：表示ExecutionJobVertex的其中一个并发子任务，输入是ExecutionEdge，输出是IntermediateResultPartition。")]),e._v(" "),t("li",[e._v("IntermediateResult：和JobGraph中的IntermediateDataSet一一对应。一个IntermediateResult包含多个IntermediateResultPartition，其个数等于该operator的并发度")]),e._v(" "),t("li",[e._v("IntermediateResultPartition：表示ExecutionVertex的一个输出分区，producer是ExecutionVertex，consumer是若干个ExecutionEdge。")]),e._v(" "),t("li",[e._v("ExecutionEdge：表示ExecutionVertex的输入，source是IntermediateResultPartition，target是ExecutionVertex。source和target都只能是一个。")]),e._v(" "),t("li",[e._v("Execution：是执行一个 ExecutionVertex 的一次尝试。当发生故障或者数据需要重算的情况下 ExecutionVertex 可能会有多个 ExecutionAttemptID。一个 Execution 通过 ExecutionAttemptID 来唯一标识。JM和TM之间关于 task 的部署和 task status 的更新都是通过 ExecutionAttemptID 来确定消息接受者。")])])]),e._v(" "),t("li",[t("p",[e._v("物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。")]),e._v(" "),t("ul",[t("li",[e._v("Task：Execution被调度后在分配的 TaskManager 中启动对应的 Task。Task 包裹了具有用户执行逻辑的 operator。")]),e._v(" "),t("li",[e._v("ResultPartition：代表由一个Task的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应。")]),e._v(" "),t("li",[e._v("ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费 Task 数和 DistributionPattern 来决定。")]),e._v(" "),t("li",[e._v("InputGate：代表Task的输入封装，和JobGraph中JobEdge一一对应。每个InputGate消费了一个或多个的ResultPartition。")]),e._v(" "),t("li",[e._v("InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出。")])])])]),e._v(" "),t("p",[t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_graphs2.png",alt:""}})]),e._v(" "),t("p",[e._v("图中每个圆代表一个Operator（算子），每个虚线圆角框代表一个Task，每个虚线直角框代表一个Subtask，其中的p表示算子的并行度。\n最上面是StreamGraph，在没有经过任何优化时，可以看到包含4个Operator/Task：Task A1、Task A2、Task B、Task C。")]),e._v(" "),t("p",[e._v("StreamGraph经过Chain优化（后面讲）之后，Task A1和Task A2两个Task合并成了一个新的Task A（可以认为合并产生了一个新的Operator），得到了中间的JobGraph。")]),e._v(" "),t("p",[e._v("然后以并行度为2（需要2个Slot）执行的时候，Task A产生了2个Subtask，分别占用了Thread #1和Thread #2两个线程；Task B产生了2个Subtask，分别占用了Thread #3和Thread #4两个线程；Task C产生了1个Subtask，占用了Thread #5。")]),e._v(" "),t("h3",{attrs:{id:"_1-2-key-concepts"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-key-concepts"}},[e._v("#")]),e._v(" 1.2 Key Concepts")]),e._v(" "),t("h4",{attrs:{id:"streams"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#streams"}},[e._v("#")]),e._v(" Streams")]),e._v(" "),t("p",[e._v("Obviously, streams are a fundamental aspect of stream processing. However, streams can have different characteristics that affect how a stream can and should be processed. Flink is a versatile processing framework that can handle any kind of stream.")]),e._v(" "),t("p",[t("strong",[e._v("Bounded and unbounded streams:")]),e._v("\nStreams can be unbounded or bounded, i.e., fixed-sized data sets. Flink has sophisticated features to process unbounded streams, but also dedicated operators to efficiently process bounded streams.")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Unbounded streams\nhave a start but no defined end. They do not terminate and provide data as it is generated. Unbounded streams must be continuously processed, i.e., events must be promptly handled after they have been ingested. It is not possible to wait for all input data to arrive because the input is unbounded and will not be complete at any point in time. Processing unbounded data often requires that events are ingested in a specific order, such as the order in which events occurred, to be able to reason about result completeness.")])]),e._v(" "),t("li",[t("p",[e._v("Bounded streams\nhave a defined start and end. Bounded streams can be processed by ingesting all data before performing any computations. Ordered ingestion is not required to process bounded streams because a bounded data set can always be sorted. Processing of bounded streams is also known as batch processing.")])])]),e._v(" "),t("p",[t("strong",[e._v("Real-time and recorded streams:")]),e._v("\nAll data are generated as streams. There are two ways to process the data. Processing it in real-time as it is generated or persisting the stream to a storage system, e.g., a file system or object store, and processed it later. Flink applications can process recorded or real-time streams.")]),e._v(" "),t("h4",{attrs:{id:"state"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#state"}},[e._v("#")]),e._v(" State")]),e._v(" "),t("p",[e._v("Every non-trivial streaming application is stateful, i.e., only applications that apply transformations on individual events do not require state. Any application that runs basic business logic needs to remember events or intermediate results to access them at a later point in time, for example when the next event is received or after a specific time duration.\nApplication state is a first-class citizen in Flink. You can see that by looking at all the features that Flink provides in the context of state handling.")]),e._v(" "),t("ul",[t("li",[e._v("Multiple State Primitives:\nFlink provides state primitives for different data structures, such as atomic values, lists, or maps. Developers can choose the state primitive that is most efficient based on the access pattern of the function.")]),e._v(" "),t("li",[e._v("Pluggable State Backends:\nApplication state is managed in and checkpointed by a pluggable state backend. Flink features different state backends that store state in memory or in RocksDB, an efficient embedded on-disk data store. Custom state backends can be plugged in as well.")]),e._v(" "),t("li",[e._v("Exactly-once state consistency:\nFlink’s checkpointing and recovery algorithms guarantee the consistency of application state in case of a failure. Hence, failures are transparently handled and do not affect the correctness of an application.")]),e._v(" "),t("li",[e._v("Very Large State:\nFlink is able to maintain application state of several terabytes in size due to its asynchronous and incremental checkpoint algorithm.")]),e._v(" "),t("li",[e._v("Scalable Applications:\nFlink supports scaling of stateful applications by redistributing the state to more or fewer workers.")])]),e._v(" "),t("h4",{attrs:{id:"time"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#time"}},[e._v("#")]),e._v(" Time")]),e._v(" "),t("ul",[t("li",[e._v("Event-time Mode:\nApplications that process streams with event-time semantics compute results based on timestamps of the events. Thereby, event-time processing allows for accurate and consistent results regardless whether recorded or real-time events are processed.")]),e._v(" "),t("li",[e._v("Watermark Support:\nFlink employs watermarks to reason about time in event-time applications. Watermarks are also a flexible mechanism to trade-off the latency and completeness of results.")]),e._v(" "),t("li",[e._v("Late Data Handling:\nWhen processing streams in event-time mode with watermarks, it can happen that a computation has been completed before all associated events have arrived. Such events are called late events. Flink features multiple options to handle late events, such as rerouting them via side outputs and updating previously completed results.")]),e._v(" "),t("li",[e._v("Processing-time Mode:\nIn addition to its event-time mode, Flink also supports processing-time semantics which performs computations as triggered by the wall-clock time of the processing machine. The processing-time mode can be suitable for certain applications with strict low-latency requirements that can tolerate approximate results.")])]),e._v(" "),t("h4",{attrs:{id:"other-terms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#other-terms"}},[e._v("#")]),e._v(" Other Terms")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Cluster")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Flink Cluster")]),e._v("\nA distributed system consisting of (typically) one JobManager and one or more Flink TaskManager processes.")]),e._v(" "),t("li",[t("strong",[e._v("Flink Application Cluster")]),e._v("\nA Flink Application Cluster is a dedicated Flink Cluster that only executes Flink Jobs from one Flink Application. The lifetime of the Flink Cluster is bound to the lifetime of the Flink Application.")]),e._v(" "),t("li",[t("strong",[e._v("Flink Job Cluster")]),e._v("\nA Flink Job Cluster is a dedicated Flink Cluster that only executes a single Flink Job. The lifetime of the Flink Cluster is bound to the lifetime of the Flink Job. This deployment mode has been deprecated since Flink 1.15.")]),e._v(" "),t("li",[t("strong",[e._v("Flink Session Cluster")]),e._v("\nA long-running Flink Cluster which accepts multiple Flink Jobs for execution. The lifetime of this Flink Cluster is not bound to the lifetime of any Flink Job. Formerly, a Flink Session Cluster was also known as a Flink Cluster in session mode. Compare to Flink Application Cluster.")])])]),e._v(" "),t("li",[t("p",[e._v("Manager")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Flink TaskManager")]),e._v("\nTaskManagers are the worker processes of a Flink Cluster. Tasks are scheduled to TaskManagers for execution. They communicate with each other to exchange data between subsequent Tasks.")]),e._v(" "),t("li",[t("strong",[e._v("Flink JobManager")]),e._v("\nThe JobManager is the orchestrator of a Flink Cluster. It contains three distinct components:\n"),t("ul",[t("li",[t("strong",[e._v("Flink Resource Manager")]),e._v(",")]),e._v(" "),t("li",[t("strong",[e._v("Flink Dispatcher")])]),e._v(" "),t("li",[e._v("and one  "),t("strong",[e._v("Flink JobMaster")]),e._v(" per running Flink Job.\nJobMasters are one of the components running in the JobManager. A JobMaster is responsible for supervising the execution of the Tasks of a single job.")])])])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Flink Application")]),e._v("\nA Flink application is a Java Application that submits one or multiple Flink Jobs from the main() method (or by some other means). Submitting jobs is usually done by calling execute() on an execution environment.\nThe jobs of an application can either be submitted to a long running Flink Session Cluster, to a dedicated Flink Application Cluster, or to a Flink Job Cluster.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Record")]),e._v("\nRecords are the constituent elements of a data set or data stream. Operators and Functions receive records as input and emit records as output.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Event")]),e._v("\nAn event is a statement about a change of the state of the domain modelled by the application. Events can be input and/or output of a stream or batch processing application. Events are special types of records.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Instance")]),e._v("\nThe term instance is used to describe a specific instance of a specific type (usually Operator or Function) during runtime. As Apache Flink is mostly written in Java, this corresponds to the definition of Instance or Object in Java. In the context of Apache Flink, the term parallel instance is also frequently used to emphasize that multiple instances of the same Operator or Function type are running in parallel.")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Operator")]),e._v("\nNode of a Logical Graph. An Operator performs a certain operation, which is usually executed by a Function. Sources and Sinks are special Operators for data ingestion and data egress.\nhttps://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/datastream/operators/overview/")]),e._v(" "),t("li",[t("strong",[e._v("Function")]),e._v("\nFunctions are implemented by the user and encapsulate the application logic of a Flink program. Most Functions are wrapped by a corresponding Operator.\nhttps://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/datastream/user_defined_functions/")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("JobResultStore")]),e._v("\nThe JobResultStore is a Flink component that persists the results of globally terminated (i.e. finished, cancelled or failed) jobs to a filesystem, allowing the results to outlive a finished job. These results are then used by Flink to determine whether jobs should be subject to recovery in highly-available clusters.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Managed State")]),e._v("\nManaged State describes application state which has been registered with the framework. For Managed State, Apache Flink will take care about persistence and rescaling among other things.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Checkpoint Storage")]),e._v("\nThe location where the State Backend will store its snapshot during a checkpoint (Java Heap of JobManager or Filesystem).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Partition")]),e._v("\nA partition is an independent subset of the overall data stream or data set. A data stream or data set is divided into partitions by assigning each record to one or more partitions. Partitions of data streams or data sets are consumed by Tasks during runtime. A transformation which changes the way a data stream or data set is partitioned is often called repartitioning.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("(Runtime) Execution Mode")]),e._v("\nDataStream API programs can be executed in one of two execution modes: BATCH or STREAMING. See Execution Mode for more details.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("State Backend")]),e._v("\nFor stream processing programs, the State Backend of a Flink Job determines how its state is stored on each TaskManager (Java Heap of TaskManager or (embedded) RocksDB).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Table Program")]),e._v("\nA generic term for pipelines declared with Flink’s relational APIs (Table API or SQL).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Transformation")]),e._v("\nA Transformation is applied on one or more data streams or data sets and results in one or more output data streams or data sets. A transformation might change a data stream or data set on a per-record basis, but might also only change its partitioning or perform an aggregation. While Operators and Functions are the “physical” parts of Flink’s API, Transformations are only an API concept. Specifically, most transformations are implemented by certain Operators.")])])]),e._v(" "),t("h2",{attrs:{id:"_2-deployment"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-deployment"}},[e._v("#")]),e._v(" 2. Deployment")]),e._v(" "),t("h3",{attrs:{id:"_2-0-deployment-mode"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-0-deployment-mode"}},[e._v("#")]),e._v(" 2.0 Deployment Mode")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/overview/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deployment Modes"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("img",{attrs:{src:"/docs/docs_image/software/bigdata/flink/flink_deployment_overview.png",alt:""}})]),e._v(" "),t("h4",{attrs:{id:"flink-client"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flink-client"}},[e._v("#")]),e._v(" Flink Client")]),e._v(" "),t("p",[e._v("Compiles batch or streaming applications into a dataflow graph, which it then submits to the JobManager.")]),e._v(" "),t("p",[t("strong",[e._v("Implementation:")])]),e._v(" "),t("ul",[t("li",[t("RouterLink",{attrs:{to:"//nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/cli/"}},[e._v("Command Line Interface")])],1),e._v(" "),t("li",[t("RouterLink",{attrs:{to:"//nightlies.apache.org/flink/flink-docs-release-1.18/docs/ops/rest_api/"}},[e._v("REST Endpoint")])],1),e._v(" "),t("li",[t("RouterLink",{attrs:{to:"//nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/sqlclient/"}},[e._v("SQL Client")])],1),e._v(" "),t("li",[t("RouterLink",{attrs:{to:"//nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/repls/python_shell/"}},[e._v("Python REPL")])],1)]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("$ ./bin/flink list\n\n./bin/flink run -p 2 ./examples/*WordCount-java*.jar\n\npost http://localhost:8081/jars/${jarId}/run\n")])])]),t("h4",{attrs:{id:"jobmanager"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#jobmanager"}},[e._v("#")]),e._v(" JobManager")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("JobManager is the name of the central work coordination component of Flink. It has implementations for different resource providers, which differ on high-availability, resource allocation behavior and supported job submission modes.\n")])])]),t("p",[e._v("JobManager modes for job submissions:")]),e._v(" "),t("ul",[t("li",[e._v("Application Mode:\nruns the cluster exclusively for one application. The job's main method (or client) gets executed on the JobManager. Calling "),t("code",[e._v("execute")]),e._v("/"),t("code",[e._v("executeAsync")]),e._v(" multiple times in an application is supported.\n-Per-Job Mode:\nruns the cluster exclusively for one job. The job's main method (or client) runs only prior to the cluster creation.")]),e._v(" "),t("li",[e._v("Session Mode:\none JobManager instance manages multiple jobs sharing the same cluster of TaskManagers")])]),e._v(" "),t("p",[t("strong",[e._v("Implementation:")])]),e._v(" "),t("ul",[t("li",[e._v("Standalone (this is the barebone mode that requires just JVMs to be launched. Deployment with Docker, Docker Swarm / Compose, non-native Kubernetes and other models is possible through manual setup in this mode)")]),e._v(" "),t("li",[e._v("Kubernetes")]),e._v(" "),t("li",[e._v("YARN")])]),e._v(" "),t("h4",{attrs:{id:"taskmanager"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#taskmanager"}},[e._v("#")]),e._v(" TaskManager")]),e._v(" "),t("p",[e._v("TaskManagers are the services actually performing the work of a Flink job.")]),e._v(" "),t("h4",{attrs:{id:"optional-external-components-high-availability-service-provider"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#optional-external-components-high-availability-service-provider"}},[e._v("#")]),e._v(" Optional External Components - High Availability Service Provider")]),e._v(" "),t("p",[e._v("Flink's JobManager can be run in high availability mode which allows Flink to recover from JobManager faults. In order  to failover faster, multiple standby JobManagers can be started to act as backups.")]),e._v(" "),t("ul",[t("li",[e._v("Zookeeper")]),e._v(" "),t("li",[e._v("Kubernetes HA")])]),e._v(" "),t("h4",{attrs:{id:"optional-external-components-file-storage-and-persistency"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#optional-external-components-file-storage-and-persistency"}},[e._v("#")]),e._v(" Optional External Components - File Storage and Persistency")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("For checkpointing (recovery mechanism for streaming jobs) Flink relies on external [file storage systems](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/filesystems/overview/)\n")])])]),t("ul",[t("li",[e._v("Local File System\nFlink has built-in support for the file system of the local machine, including any NFS or SAN drives mounted into that local file system. It can be used by default without additional configuration. Local files are referenced with the file:// URI scheme.")]),e._v(" "),t("li",[e._v("hadoop-compatible")]),e._v(" "),t("li",[e._v("Pluggable File Systems (Amazon S3, Aliyun OSS and Azure Blob Storage.)")])]),e._v(" "),t("h4",{attrs:{id:"optional-external-components-resource-provider"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#optional-external-components-resource-provider"}},[e._v("#")]),e._v(" Optional External Components - Resource Provider")]),e._v(" "),t("p",[e._v("Flink can be deployed through different Resource Provider Frameworks, such as Kubernetes or YARN.\n"),t("a",{attrs:{href:"#jobmanager"}},[e._v("See JobManager implementations above.")])]),e._v(" "),t("h4",{attrs:{id:"optional-external-components-metrics-storage"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#optional-external-components-metrics-storage"}},[e._v("#")]),e._v(" Optional External Components - Metrics Storage")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/metric_reporters/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Flink components report internal metrics and Flink jobs can report additional, job specific metrics as well."),t("OutboundLink")],1)]),e._v(" "),t("h4",{attrs:{id:"optional-external-components-application-level-data-sources-and-sinks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#optional-external-components-application-level-data-sources-and-sinks"}},[e._v("#")]),e._v(" Optional External Components - Application-level data sources and sinks")]),e._v(" "),t("p",[e._v("While application-level data sources and sinks are not technically part of the deployment of Flink cluster components, they should be considered when planning a new Flink production deployment. Colocating frequently used data with Flink can have significant performance benefits")]),e._v(" "),t("ul",[t("li",[t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/datastream/overview/#data-sources",target:"_blank",rel:"noopener noreferrer"}},[e._v("Predefined Sources"),t("OutboundLink")],1),e._v(" and "),t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/datastream/overview/#data-sinks",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sinks"),t("OutboundLink")],1),e._v("\nA few basic data sources and sinks are built into Flink and are always available. The predefined data sources include reading from files, directories, and sockets, and ingesting data from collections and iterators. The predefined data sinks support writing to files, to stdout and stderr, and to sockets.")])]),e._v(" "),t("li",[t("p",[e._v("Bundled Connectors\nConnectors provide code for interfacing with various third-party systems. Currently these systems are supported:")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/kafka/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apache Kafka (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/cassandra/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apache Cassandra (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/dynamodb/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amazon DynamoDB (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/kinesis/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amazon Kinesis Data Streams (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/firehose/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amazon Kinesis Data Firehose (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/datagen/",target:"_blank",rel:"noopener noreferrer"}},[e._v("DataGen (source)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/elasticsearch/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Elasticsearch (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/opensearch/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Opensearch (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/filesystem/",target:"_blank",rel:"noopener noreferrer"}},[e._v("FileSystem (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/rabbitmq/",target:"_blank",rel:"noopener noreferrer"}},[e._v("RabbitMQ (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/pubsub/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Google PubSub (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/hybridsource/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hybrid Source (source)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/pulsar/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apache Pulsar (source)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/jdbc/",target:"_blank",rel:"noopener noreferrer"}},[e._v("JDBC (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/mongodb/",target:"_blank",rel:"noopener noreferrer"}},[e._v("MongoDB (source/sink)"),t("OutboundLink")],1)])])]),e._v(" "),t("li",[t("p",[e._v("Connectors in Apache Bahir\nAdditional streaming connectors for Flink are being released through Apache Bahir, including:")])])]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://bahir.apache.org/docs/flink/current/flink-streaming-activemq/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apache ActiveMQ (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://bahir.apache.org/docs/flink/current/flink-streaming-flume/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apache Flume (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://bahir.apache.org/docs/flink/current/flink-streaming-redis/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Redis (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://bahir.apache.org/docs/flink/current/flink-streaming-akka/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Akka (sink)"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://bahir.apache.org/docs/flink/current/flink-streaming-netty/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Netty (source)"),t("OutboundLink")],1)])]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/datastream/operators/asyncio/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Data Enrichment via Async I/O"),t("OutboundLink")],1),e._v("\nUsing a connector isn’t the only way to get data in and out of Flink. One common pattern is to query an external database or web service in a Map or FlatMap in order to enrich the primary datastream. Flink offers an API for Asynchronous I/O to make it easier to do this kind of enrichment efficiently and robustly.")])]),e._v(" "),t("h3",{attrs:{id:"_2-1-resource-provider-standalone-mode-local-standalone"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-resource-provider-standalone-mode-local-standalone"}},[e._v("#")]),e._v(" 2.1 Resource Provider Standalone Mode: Local Standalone")]),e._v(" "),t("p",[e._v("The standalone mode is the most barebone way of deploying Flink: The Flink services described in the deployment overview are just launched as processes on the operating system. Unlike deploying Flink with a resource provider such as Kubernetes or YARN, you have to take care of restarting failed processes, or allocation and de-allocation of resources during operation.")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/resource-providers/standalone/overview/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deployment/Standalone"),t("OutboundLink")],1)]),e._v(" "),t("h4",{attrs:{id:"insall"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#insall"}},[e._v("#")]),e._v(" Insall")]),e._v(" "),t("h5",{attrs:{id:"session-mode"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#session-mode"}},[e._v("#")]),e._v(" Session Mode")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("$ java -version\n$ tar -xzf flink-*.tgz\n$ cd flink-* && ls -l\n$ ./bin/start-cluster.sh  //started 2 processes: A JVM for the JobManager, and a JVM for the TaskManager.\n\nlocalhost:8081 to view the Flink dashboard \n\n$ ./bin/stop-cluster.sh\n\n$ ./bin/flink run examples/streaming/WordCount.jar\n$ tail log/flink-*-taskexecutor-*.out\n")])])]),t("p",[e._v("WordCount:\nhttps://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/wordcount/WordCount.java")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('public class WordCount\n{\n  public static void main(String[] args) throws Exception {\n    MultipleParameterTool params = MultipleParameterTool.fromArgs(args);\n    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n    env.getConfig().setGlobalJobParameters(params);\n\n    DataStreamSource dataStreamSource = null;\n    if (params.has("input")) {\n      DataStream dataStream;\n      for (String input : params.getMultiParameterRequired("input")) {\n        if (dataStreamSource == null) {\n          DataStreamSource dataStreamSource1; dataStreamSource1 = env.readTextFile(input); continue;\n        } \n        dataStream = dataStreamSource1.union(new DataStream[] { env.readTextFile(input) });\n      }      \n      Preconditions.checkNotNull(dataStream, "Input DataStream should not be null.");\n    } else {\n      System.out.println("Executing WordCount example with default input data set.");\n      System.out.println("Use --input to specify file input.");\n      \n      dataStreamSource = env.fromElements(WordCountData.WORDS);\n    } \n \n    SingleOutputStreamOperator singleOutputStreamOperator = dataStreamSource.flatMap(new Tokenizer()).keyBy(value -> (String)value.f0).sum(1);\n\n    if (params.has("output")) {\n      singleOutputStreamOperator.writeAsText(params.get("output"));\n    } else {\n      System.out.println("Printing result to stdout. Use --output to specify output path.");\n      singleOutputStreamOperator.print();\n    } \n    env.execute("Streaming WordCount");\n  }\n  \n  public static final class Tokenizer extends Object implements FlatMapFunction<String, Tuple2<String, Integer>> {\n    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) { // Byte code:\n      //   0: aload_1\n      //   1: invokevirtual toLowerCase : ()Ljava/lang/String;\n      //   4: ldc \'\\W+\'\n      //   6: invokevirtual split : (Ljava/lang/String;)[Ljava/lang/String;\n      //   9: astore_3\n      //   10: aload_3\n      //   11: astore #4\n      //   13: aload #4\n      //   15: arraylength\n      //   16: istore #5\n      //   18: iconst_0\n      //   19: istore #6\n      //   21: iload #6\n      //   23: iload #5\n      //   25: if_icmpge -> 68\n      //   28: aload #4\n      //   30: iload #6\n      //   32: aaload\n      //   33: astore #7\n      //   35: aload #7\n      //   37: invokevirtual length : ()I\n      //   40: ifle -> 62\n      //   43: aload_2\n      //   44: new org/apache/flink/api/java/tuple/Tuple2\n      //   47: dup\n      //   48: aload #7\n      //   50: iconst_1\n      //   51: invokestatic valueOf : (I)Ljava/lang/Integer;\n      //   54: invokespecial <init> : (Ljava/lang/Object;Ljava/lang/Object;)V\n      //   57: invokeinterface collect : (Ljava/lang/Object;)V\n      //   62: iinc #6, 1\n      //   65: goto -> 21\n      //   68: return\n      // Line number table:\n      //   Java source line number -> byte code offset\n      //   #115\t-> 0\n      //   #118\t-> 10\n      //   #119\t-> 35\n      //   #120\t-> 43\n      //   #118\t-> 62\n      //   #123\t-> 68\n      // Local variable table:\n      //   start\tlength\tslot\tname\tdescriptor\n      //   35\t27\t7\ttoken\tLjava/lang/String;\n      //   0\t69\t0\tthis\tLorg/apache/flink/streaming/examples/wordcount/WordCount$Tokenizer;\n      //   0\t69\t1\tvalue\tLjava/lang/String;\n      //   0\t69\t2\tout\tLorg/apache/flink/util/Collector;\n      //   10\t59\t3\ttokens\t[Ljava/lang/String;\n      // Local variable type table:\n      //   start\tlength\tslot\tname\tsignature\n      //   0\t69\t2\tout\tLorg/apache/flink/util/Collector<Lorg/apache/flink/api/java/tuple/Tuple2<Ljava/lang/String;Ljava/lang/Integer;>;>; }\n  }\n}\n\n')])])]),t("h5",{attrs:{id:"application-mode"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#application-mode"}},[e._v("#")]),e._v(" Application Mode")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("The application jar file needs to be available in the classpath. The easiest approach to achieve that is putting the jar into the lib/ folder:\n\n$ cp ./examples/streaming/TopSpeedWindowing.jar lib/\n\nTo start a Flink JobManager with an embedded application, we use the bin/standalone-job.sh script. We demonstrate this mode by locally starting the TopSpeedWindowing.jar example, running on a single TaskManager.\n\n$ ./bin/standalone-job.sh start --job-classname org.apache.flink.streaming.examples.windowing.TopSpeedWindowing\n\nThe web interface is now available at localhost:8081. However, the application won’t be able to start, because there are no TaskManagers running yet:\n\n$ ./bin/taskmanager.sh start //Note: You can start multiple TaskManagers, if your application needs more resources.\n\nStopping the services is also supported via the scripts. Call them multiple times if you want to stop multiple instances, or use stop-all:\n\n$ ./bin/taskmanager.sh stop\n$ ./bin/standalone-job.sh stop\n")])])]),t("h5",{attrs:{id:"multiple-instances"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#multiple-instances"}},[e._v("#")]),e._v(" Multiple Instances")]),e._v(" "),t("p",[e._v("bin/start-cluster.sh and bin/stop-cluster.sh rely on conf/masters and conf/workers to determine the number of cluster component instances.")]),e._v(" "),t("p",[t("strong",[e._v("Example 1: Start a cluster with 2 TaskManagers locally")])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("conf/masters contents:\n\nlocalhost\nconf/workers contents:\n\nlocalhost\nlocalhost\n")])])]),t("p",[e._v("If password-less SSH access to the listed machines is configured, and they share the same directory structure, the scripts also support starting and stopping instances remotely.")]),e._v(" "),t("p",[t("strong",[e._v("Example 2: Start a distributed cluster JobManagers")])]),e._v(" "),t("p",[e._v("This assumes a cluster with 4 machines (master1, worker1, worker2, worker3), which all can reach each other over the network.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("conf/masters contents:\n\nmaster1\nconf/workers contents:\n\nworker1\nworker2\nworker3\n\nNote that the configuration key jobmanager.rpc.address needs to be set to master1 for this to work.\n")])])]),t("p",[e._v("High-Availability\n"),t("strong",[e._v("Example 3: Standalone HA Cluster with 2 JobManagers")])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("1. In order to enable HA for a standalone cluster, you have to use the ZooKeeper HA services.\nConfigure high availability mode and ZooKeeper quorum in conf/flink-conf.yaml:\nhigh-availability.type: zookeeper\nhigh-availability.zookeeper.quorum: localhost:2181\nhigh-availability.zookeeper.path.root: /flink\nhigh-availability.cluster-id: /cluster_one # important: customize per cluster\nhigh-availability.storageDir: hdfs:///flink/recovery\n\n2. In order to start an HA-cluster configure the masters file in conf/masters, the masters file contains all hosts, on which JobManagers are started, and the ports to which the web user interface binds.\nConfigure masters in conf/masters:\nlocalhost:8081\nlocalhost:8082\n\n3. Configure ZooKeeper server in conf/zoo.cfg (currently it’s only possible to run a single ZooKeeper server per machine):\nserver.0=localhost:2888:3888\n\n4. Start ZooKeeper quorum:\n$ ./bin/start-zookeeper-quorum.sh\nStarting zookeeper daemon on host localhost.\n\n5. Start an HA-cluster:\n$ ./bin/start-cluster.sh\nStarting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.\nStarting standalonesession daemon on host localhost.\nStarting standalonesession daemon on host localhost.\nStarting taskexecutor daemon on host localhost.\n\n6. Stop ZooKeeper quorum and cluster:\n$ ./bin/stop-cluster.sh\nStopping taskexecutor daemon (pid: 7647) on localhost.\nStopping standalonesession daemon (pid: 7495) on host localhost.\nStopping standalonesession daemon (pid: 7349) on host localhost.\n$ ./bin/stop-zookeeper-quorum.sh\nStopping zookeeper daemon (pid: 7101) on host localhost.\n\nBy default, the JobManager will pick a random port for inter process communication. You can change this via the high-availability.jobmanager.port key. This key accepts single ports (e.g. 50010), ranges (50000-50025), or a combination of both (50010,50011,50020-50025,50050-50075).\n")])])]),t("h4",{attrs:{id:"log-analysis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#log-analysis"}},[e._v("#")]),e._v(" Log Analysis")]),e._v(" "),t("h5",{attrs:{id:"job-manager-log"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#job-manager-log"}},[e._v("#")]),e._v(" Job Manager Log")]),e._v(" "),t("p",[e._v("vim log/flink-root-standalonesession-0-vm01.log")]),e._v(" "),t("h6",{attrs:{id:"启动"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#启动"}},[e._v("#")]),e._v(" 启动")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2022-05-26 14:14:26,762 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at localhost:8081\n2022-05-26 14:14:26,763 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://localhost:8081 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000\n2022-05-26 14:14:26,765 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Web frontend listening at http://localhost:8081.\n2022-05-26 14:14:26,816 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new DispatcherLeaderProcess.\n2022-05-26 14:14:26,823 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.\n2022-05-26 14:14:26,825 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.\n2022-05-26 14:14:26,827 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.\n2022-05-26 14:14:26,827 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 00000000-0000-0000-0000-000000000000.\n2022-05-26 14:14:26,827 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.\n2022-05-26 14:14:27,481 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_0 .\n2022-05-26 14:14:27,516 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .\n2022-05-26 14:14:27,543 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.\n2022-05-26 14:14:27,927 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 10.136.100.48:35016-a4d337 (akka.tcp://flink@10.136.100.48:35016/user/rpc/taskmanager_0) at ResourceManager\n")])])]),t("h6",{attrs:{id:"接收job-create-running-schedule-deploy"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#接收job-create-running-schedule-deploy"}},[e._v("#")]),e._v(" 接收job，create->running/schedule->deploy")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2022-05-27 16:13:00,098 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:00,100 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:00,151 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .\n2022-05-27 16:13:00,165 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:00,211 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for Streaming WordCount (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:00,279 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job Streaming WordCount (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:00,279 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.\n2022-05-27 16:13:00,321 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 0 ms\n2022-05-27 16:13:00,393 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3a91353f\n2022-05-27 16:13:00,394 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend\n2022-05-27 16:13:00,396 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'\n2022-05-27 16:13:00,417 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.\n2022-05-27 16:13:00,444 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@6e20b54e for Streaming WordCount (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:00,460 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467) under job master id 00000000000000000000000000000000.\n2022-05-27 16:13:00,463 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]\n2022-05-27 16:13:00,463 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Streaming WordCount (f69c1ca4892ecbc08d4247ded254f467) switched from state CREATED to RUNNING.\n2022-05-27 16:13:00,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Collection Source -> Flat Map (1/1) (c83c41ff9f43c36e7a6aea483e073ec1) switched from CREATED to SCHEDULED.\n2022-05-27 16:13:00,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (a602bd7b23ece40a69422f7b36701083) switched from CREATED to SCHEDULED.\n2022-05-27 16:13:00,492 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@localhost:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)\n2022-05-27 16:13:00,499 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration\n2022-05-27 16:13:00,502 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job f69c1ca4892ecbc08d4247ded254f467.\n2022-05-27 16:13:00,509 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job f69c1ca4892ecbc08d4247ded254f467.\n2022-05-27 16:13:00,512 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.\n2022-05-27 16:13:00,514 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job f69c1ca4892ecbc08d4247ded254f467: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]\n2022-05-27 16:13:00,636 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Collection Source -> Flat Map (1/1) (c83c41ff9f43c36e7a6aea483e073ec1) switched from SCHEDULED to DEPLOYING.\n2022-05-27 16:13:00,637 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Collection Source -> Flat Map (1/1) (attempt #0) with attempt id c83c41ff9f43c36e7a6aea483e073ec1 to 10.136.100.48:35016-a4d337 @ vm-v08 (dataPort=59281) with allocation id 3b41f2b6c9f47bf531ac47e91afde9fb\n2022-05-27 16:13:00,646 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (a602bd7b23ece40a69422f7b36701083) switched from SCHEDULED to DEPLOYING.\n2022-05-27 16:13:00,646 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Keyed Aggregation -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id a602bd7b23ece40a69422f7b36701083 to 10.136.100.48:35016-a4d337 @ vm-v08 (dataPort=59281) with allocation id 3b41f2b6c9f47bf531ac47e91afde9fb\n2022-05-27 16:13:00,905 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (a602bd7b23ece40a69422f7b36701083) switched from DEPLOYING to INITIALIZING.\n2022-05-27 16:13:00,908 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Collection Source -> Flat Map (1/1) (c83c41ff9f43c36e7a6aea483e073ec1) switched from DEPLOYING to INITIALIZING.\n2022-05-27 16:13:01,166 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Collection Source -> Flat Map (1/1) (c83c41ff9f43c36e7a6aea483e073ec1) switched from INITIALIZING to RUNNING.\n2022-05-27 16:13:01,196 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (a602bd7b23ece40a69422f7b36701083) switched from INITIALIZING to RUNNING.\n2022-05-27 16:13:01,223 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Collection Source -> Flat Map (1/1) (c83c41ff9f43c36e7a6aea483e073ec1) switched from RUNNING to FINISHED.\n2022-05-27 16:13:01,246 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (a602bd7b23ece40a69422f7b36701083) switched from RUNNING to FINISHED.\n2022-05-27 16:13:01,249 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Streaming WordCount (f69c1ca4892ecbc08d4247ded254f467) switched from state RUNNING to FINISHED.\n2022-05-27 16:13:01,249 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job f69c1ca4892ecbc08d4247ded254f467.\n2022-05-27 16:13:01,250 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job f69c1ca4892ecbc08d4247ded254f467\n2022-05-27 16:13:01,279 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job f69c1ca4892ecbc08d4247ded254f467 reached terminal state FINISHED.\n2022-05-27 16:13:01,314 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:01,320 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down\n2022-05-27 16:13:01,322 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [3b41f2b6c9f47bf531ac47e91afde9fb].\n2022-05-27 16:13:01,328 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 4a2508526d0621625a55daa90f37e499: Stopping JobMaster for job 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:01,330 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job f69c1ca4892ecbc08d4247ded254f467 from the resource manager.\n")])])]),t("h5",{attrs:{id:"task-manager-log"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#task-manager-log"}},[e._v("#")]),e._v(" Task Manager Log")]),e._v(" "),t("p",[e._v("vim flink-root-taskexecutor-0-vm01.log")]),e._v(" "),t("h6",{attrs:{id:"启动-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#启动-2"}},[e._v("#")]),e._v(" 启动")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("INFO  [] - Final TaskExecutor Memory configuration:\nINFO  [] -   Total Process Memory:          1.688gb (1811939328 bytes)\nINFO  [] -     Total Flink Memory:          1.250gb (1342177280 bytes)\nINFO  [] -       Total JVM Heap Memory:     512.000mb (536870902 bytes)\nINFO  [] -         Framework:               128.000mb (134217728 bytes)\nINFO  [] -         Task:                    384.000mb (402653174 bytes)\nINFO  [] -       Total Off-heap Memory:     768.000mb (805306378 bytes)\nINFO  [] -         Managed:                 512.000mb (536870920 bytes)\nINFO  [] -         Total JVM Direct Memory: 256.000mb (268435458 bytes)\nINFO  [] -           Framework:             128.000mb (134217728 bytes)\nINFO  [] -           Task:                  0 bytes\nINFO  [] -           Network:               128.000mb (134217730 bytes)\nINFO  [] -     JVM Metaspace:               256.000mb (268435456 bytes)\nINFO  [] -     JVM Overhead:                192.000mb (201326592 bytes)\n\n2022-05-26 14:14:23,738 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------\n2022-05-26 14:14:23,739 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Starting TaskManager (Version: 1.14.4, Scala: 2.11, Rev:895c609, Date:2022-02-25T11:57:14+01:00)\n................\n2022-05-26 14:14:27,050 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.\n2022-05-26 14:14:27,134 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.\n2022-05-26 14:14:27,137 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 86 ms).\n2022-05-26 14:14:27,143 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.\n2022-05-26 14:14:27,186 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 47 ms). Listening on SocketAddress /0.0.0.0:59281.\n2022-05-26 14:14:27,187 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.\n2022-05-26 14:14:27,519 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .\n2022-05-26 14:14:27,541 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.\n2022-05-26 14:14:27,543 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-b87b98d1-b215-449a-b134-71cb2efd67e5\n2022-05-26 14:14:27,546 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@localhost:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).\n....\n2022-05-26 14:14:24,566 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - Trying to select the network interface and address to use by connecting to the leading JobManager.\n....\n2022-05-26 14:14:27,812 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration\n2022-05-26 14:14:27,950 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@localhost:6123/user/rpc/resourcemanager_* under registration id 82d6263f9d0c12c01c047caa988f2d1a.\n")])])]),t("h6",{attrs:{id:"接收task-具体执行"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#接收task-具体执行"}},[e._v("#")]),e._v(" 接收task,具体执行")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2022-05-27 16:13:00,529 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 3b41f2b6c9f47bf531ac47e91afde9fb for job f69c1ca4892ecbc08d4247ded254f467 from resource manager with leader id 00000000000000000000000000000000.\n2022-05-27 16:13:00,548 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 3b41f2b6c9f47bf531ac47e91afde9fb.\n2022-05-27 16:13:00,551 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job f69c1ca4892ecbc08d4247ded254f467 for job leader monitoring.\n2022-05-27 16:13:00,554 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.\n2022-05-27 16:13:00,589 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration\n2022-05-27 16:13:00,613 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job f69c1ca4892ecbc08d4247ded254f467.\n2022-05-27 16:13:00,614 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job f69c1ca4892ecbc08d4247ded254f467.\n2022-05-27 16:13:00,618 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job f69c1ca4892ecbc08d4247ded254f467.\n2022-05-27 16:13:00,682 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3b41f2b6c9f47bf531ac47e91afde9fb.\n2022-05-27 16:13:00,706 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.\n2022-05-27 16:13:00,737 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Collection Source -> Flat Map (1/1)#0 (c83c41ff9f43c36e7a6aea483e073ec1), deploy into slot with allocation id 3b41f2b6c9f47bf531ac47e91afde9fb.\n2022-05-27 16:13:00,739 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Collection Source -> Flat Map (1/1)#0 (c83c41ff9f43c36e7a6aea483e073ec1) switched from CREATED to DEPLOYING.\n2022-05-27 16:13:00,746 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3b41f2b6c9f47bf531ac47e91afde9fb.\n2022-05-27 16:13:00,748 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Collection Source -> Flat Map (1/1)#0 (c83c41ff9f43c36e7a6aea483e073ec1) [DEPLOYING].\n2022-05-27 16:13:00,754 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3b41f2b6c9f47bf531ac47e91afde9fb.\n2022-05-27 16:13:00,754 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading f69c1ca4892ecbc08d4247ded254f467/p-1e9bc735196982c3db4e502b3af82b3579da2836-dbdb712e9143718fee67e7aab9708f9a from localhost/127.0.0.1:34934\n2022-05-27 16:13:00,782 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 (a602bd7b23ece40a69422f7b36701083), deploy into slot with allocation id 3b41f2b6c9f47bf531ac47e91afde9fb.\n2022-05-27 16:13:00,784 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 (a602bd7b23ece40a69422f7b36701083) switched from CREATED to DEPLOYING.\n2022-05-27 16:13:00,786 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 (a602bd7b23ece40a69422f7b36701083) [DEPLOYING].\n2022-05-27 16:13:00,867 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@47ff2446\n2022-05-27 16:13:00,867 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend\n2022-05-27 16:13:00,870 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'\n2022-05-27 16:13:00,874 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4f8c0490\n2022-05-27 16:13:00,874 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend\n2022-05-27 16:13:00,874 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'\n2022-05-27 16:13:00,894 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 (a602bd7b23ece40a69422f7b36701083) switched from DEPLOYING to INITIALIZING.\n2022-05-27 16:13:00,895 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Collection Source -> Flat Map (1/1)#0 (c83c41ff9f43c36e7a6aea483e073ec1) switched from DEPLOYING to INITIALIZING.\n2022-05-27 16:13:01,162 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Collection Source -> Flat Map (1/1)#0 (c83c41ff9f43c36e7a6aea483e073ec1) switched from INITIALIZING to RUNNING.\n2022-05-27 16:13:01,165 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.\n2022-05-27 16:13:01,178 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.\n2022-05-27 16:13:01,194 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 (a602bd7b23ece40a69422f7b36701083) switched from INITIALIZING to RUNNING.\n2022-05-27 16:13:01,197 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Collection Source -> Flat Map (1/1)#0 (c83c41ff9f43c36e7a6aea483e073ec1) switched from RUNNING to FINISHED.\n2022-05-27 16:13:01,197 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Collection Source -> Flat Map (1/1)#0 (c83c41ff9f43c36e7a6aea483e073ec1).\n2022-05-27 16:13:01,199 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Collection Source -> Flat Map (1/1)#0 c83c41ff9f43c36e7a6aea483e073ec1.\n2022-05-27 16:13:01,240 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 (a602bd7b23ece40a69422f7b36701083) switched from RUNNING to FINISHED.\n2022-05-27 16:13:01,240 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 (a602bd7b23ece40a69422f7b36701083).\n2022-05-27 16:13:01,242 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Keyed Aggregation -> Sink: Print to Std. Out (1/1)#0 a602bd7b23ece40a69422f7b36701083.\n2022-05-27 16:13:01,338 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: 3b41f2b6c9f47bf531ac47e91afde9fb, jobId: f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:01,342 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job f69c1ca4892ecbc08d4247ded254f467 from job leader monitoring.\n2022-05-27 16:13:01,343 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job f69c1ca4892ecbc08d4247ded254f467.\n")])])]),t("h6",{attrs:{id:"具体执行输出"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#具体执行输出"}},[e._v("#")]),e._v(" 具体执行输出")]),e._v(" "),t("p",[e._v("vim log/flink-root-taskexecutor-0-vm01.out")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("(to,1)\n(be,1)\n(or,1)\n(not,1)\n(to,2)\n(be,2)\n(that,1)\n(is,1)\n(the,1)\n")])])]),t("h5",{attrs:{id:"client-log"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#client-log"}},[e._v("#")]),e._v(" Client Log")]),e._v(" "),t("p",[e._v("vim log/flink-root-client-vm01.log")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2022-05-27 16:12:57,601 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Program Arguments:\n2022-05-27 16:12:57,603 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     run\n2022-05-27 16:12:57,603 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     examples/streaming/WordCount.jar\n2022-05-27 16:12:58,065 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Running 'run' command.\n2022-05-27 16:12:58,098 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Building program from JAR file\n2022-05-27 16:12:58,118 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: false)\n2022-05-27 16:12:59,179 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'\n2022-05-27 16:12:59,310 INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Submitting job 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467).\n2022-05-27 16:13:00,207 INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Successfully submitted job 'Streaming WordCount' (f69c1ca4892ecbc08d4247ded254f467) to 'http://localhost:8081'.\n2022-05-27 16:13:03,555 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'\n")])])]),t("h3",{attrs:{id:"_2-2-resource-provider-stanalone-mode-hadoop-file-system"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-resource-provider-stanalone-mode-hadoop-file-system"}},[e._v("#")]),e._v(" 2.2 Resource Provider Stanalone Mode: Hadoop file system")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://help.aliyun.com/document_detail/141362.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("在文件存储 HDFS 版上使用Apache Flink"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://www.infoq.cn/article/olljnzqptohfyrgog8xq",target:"_blank",rel:"noopener noreferrer"}},[e._v("阿里巴巴 Flink 踩坑经验：如何大幅降低 HDFS 压力？"),t("OutboundLink")],1)]),e._v(" "),t("h4",{attrs:{id:"install-with-hadoop"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#install-with-hadoop"}},[e._v("#")]),e._v(" Install with Hadoop")]),e._v(" "),t("h5",{attrs:{id:"hadoop"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hadoop"}},[e._v("#")]),e._v(" Hadoop")]),e._v(" "),t("p",[e._v("https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html#Slaves_File")]),e._v(" "),t("p",[e._v("Typically one machine in the cluster is designated as the NameNode and another machine as the ResourceManager, exclusively. These are the masters. Other services (such as Web App Proxy Server and MapReduce Job History server) are usually run either on dedicated hardware or on shared infrastructure, depending upon the load.")]),e._v(" "),t("p",[e._v("The rest of the machines in the cluster act as both DataNode and NodeManager. These are the workers.")]),e._v(" "),t("p",[t("strong",[e._v("准备：")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz",target:"_blank",rel:"noopener noreferrer"}},[e._v("download hadoop"),t("OutboundLink")],1)]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("target:\n10.1.1.1: JournalNode, NameNode (active), DataNode, ZKFailoverController\n10.1.1.2: JournalNode, NameNode (standby), DataNode, ZKFailoverController\n10.1.1.3: JournalNode, NameNode (standby), DataNode, ZKFailoverController\n\nON MAIN VM:\nuseradd -m -d /home/hadoop hadoop\npasswd hadoop\n\nchmod 755 /home/hadoop\n\ntar -zxvf /tmp/hadoop-3.3.0.tar.gz -C /home/hadoop\n\nln -s hadoop-3.3.0 hadoop-current\nssh-keygen\n\nssh-copy-id hadoop@vm-v01\nssh-copy-id hadoop@vm-v02\nssh-copy-id hadoop@vm-v03\n\nyum install -y psmisc\n")])])]),t("p",[t("strong",[e._v("配置Hadoop")])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('\ncd hadoop-current/etc/hadoop/\ncp -p hadoop-env.sh hadoop-env.sh_factory\nvi hadoop-env.sh\n\texport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.332.b09-1.el7_9.x86_64/jre\nchmod u+x hadoop-env.sh\n\ncp core-site.xml core-site.xml_factory\nvim core-site.xml\n<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n\x3c!--\n  Licensed under the Apache License, Version 2.0 (the "License");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an "AS IS" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n--\x3e\n\n\x3c!-- Put site-specific property overrides in this file. --\x3e\n\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://test</value>\n  </property>\n</configuration>\n\ncp hdfs-site.xml hdfs-site.xml_factory\n\nHA集群需要使用命名空间区分一个HDFS集群。同一个集群中的不同NameNode，使用不同的NameNode ID区分。为了支持所有NameNode使用相同的配置文件，因此在配置参数中，需要把“nameservice ID”作为NameNode ID的前缀。\n\ndfs.nameservices 命名空间的逻辑名称。提供服务的NS（nameservices）逻辑名称，与core-site.xml里的对应。如果有多个HDFS集群，可以配置多个命名空间的名称，使用逗号分开即可。\n\ndfs.ha.namenodes.[nameservice ID] 命名空间中所有NameNode的唯一标示名称。可以配置多个，使用逗号分隔。该名称是可以让DataNode知道每个集群的所有NameNode。\n\ndfs.namenode.rpc-address.[nameservice ID].[name node ID] 每个namenode监听的RPC地址。\n\ndfs.namenode.http-address.[nameservice ID].[name node ID] 每个namenode监听的http地址。\n\nvim hdfs-site.xml\n<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n\x3c!--\n            Licensed under the Apache License, Version 2.0 (the "License");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n  \n    http://www.apache.org/licenses/LICENSE-2.0\n  \n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an "AS IS" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n--\x3e\n\n\x3c!-- Put site-specific property overrides in this file. --\x3e\n\n<configuration>\n  <property>\n    <name>dfs.permissions.enabled</name>\n   <value>false</value>\n  </property>\n  <property>\n    <name>dfs.namenode.datanode.registration.ip-hostname-check</name>\n   <value>false</value>\n  </property>\n  <property>\n    <name>dfs.nameservices</name>\n    <value>test</value>\n  </property>\n  <property>\n    <name>dfs.ha.namenodes.test</name>\n    <value>nn1,nn2,nn3</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.test.nn1</name>\n    <value>10.1.1.1:13101</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.test.nn2</name>\n    <value>10.1.1.2:13101</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.test.nn3</name>\n    <value>10.1.1.3:13101</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.test.nn1</name>\n    <value>10.1.1.1:13102</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.test.nn2</name>\n    <value>10.1.1.2:13102</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.test.nn3</name>\n    <value>10.1.1.3:13102</value>\n  </property>\n  <property>\n    <name>dfs.namenode.shared.edits.dir</name>\n    <value>qjournal://10.1.1.1:13106;10.1.1.2:13106;10.1.1.3:13106/test</value>\n  </property>\n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>file:///home/hadoop/hadoop-current/data/dfs/name</value>\n  </property>\n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>file:///home/hadoop/hadoop-current/data/dfs/data</value>\n  </property>\n  <property>\n    <name>dfs.datanode.address</name>\n    <value>0.0.0.0:13103</value>\n  </property>\n  <property>\n    <name>dfs.datanode.http.address</name>\n    <value>0.0.0.0:13104</value>\n  </property>\n  <property>\n    <name>dfs.datanode.ipc.address</name>\n    <value>0.0.0.0:13105</value>\n  </property>\n  <property>\n    <name>dfs.journalnode.edits.dir</name>\n    <value>/home/hadoop/hadoop-current/data/dfs/journal</value>\n  </property>\n  <property>\n    <name>dfs.journalnode.rpc-address</name>\n    <value>0.0.0.0:13106</value>\n  </property>\n  <property>\n    <name>dfs.journalnode.http-address</name>\n    <value>0.0.0.0:13107</value>\n  </property>\n  <property>\n    <name>dfs.ha.automatic-failover.enabled</name>\n    <value>true</value>\n  </property>\n  <property>\n    <name>dfs.client.failover.proxy.provider.test</name>\n    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n  </property>\n  <property>\n    <name>dfs.ha.fencing.methods</name>\n    <value>sshfence</value>\n  </property>\n  <property>\n    <name>dfs.ha.fencing.ssh.private-key-files</name>\n    <value>/home/hadoop/.ssh/id_rsa</value>\n  </property>\n  <property>\n    <name>ha.zookeeper.quorum</name>\n    <value>10.1.1.1:12006,10.1.1.2:12006,10.1.1.3:12006</value>\n  </property>\n</configuration>\n\ncp workers workers_factory\nvim workers\n10.1.1.1\n10.1.1.2\n10.1.1.3\n')])])]),t("p",[t("strong",[e._v("启动")])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('cd ~/hadoop-current\n#In all hosts, start the JournalNode.\n./bin/hdfs --daemon start journalnode / sbin/hadoop-daemon.sh start journalnode\ntail -f logs/hadoop-hadoop-journalnode-vm-v01.log \n\n# If this is the host for active NameNode (only execute in one host)\n# Format (initialize metadata) the NameNode.\n# Caution: pls wait for all journalnode fully started first otherwise will get connection refused error\n./bin/hdfs namenode -format\n# If above commend executed successfully, it will gracefully quit.\n\n# Start the NameNode (the first NameNode started will be the active NameNode).\n./bin/hdfs --daemon start namenode  / sbin/hadoop-daemon.sh start namenode\n\n# If this is the host for standby NameNode (only execute in hosts other than active namenode)\n# Copy metadata from formatted NameNode to this NameNode.\n./bin/hdfs namenode -bootstrapStandby\n# Start the NameNode (the NameNodes started later will be standby NameNodes).\n./bin/hdfs --daemon start namenode / sbin/hadoop-daemon.sh start namenode\n\n此时两个NameNode都是standby模式，需要强制转换一个\nbin/hdfs haadmin -transitionToActive --forcemanual nn1\n##查看一下状态\nbin/hdfs haadmin -getServiceState nn1\nbin/hdfs haadmin -getServiceState nn2\n\n#In all hosts, start the DataNode.\n./bin/hdfs --daemon start datanode\n\n#Initialize Zookeeper state (execute the following command in only one of the hosts).\n./bin/hdfs zkfc -formatZK\n#In all hosts, start the ZKFailoverController.\n./bin/hdfs --daemon start zkfc\n\ntail -f logs/hadoop-hadoop-journalnode-vm-v01.log \n\n// check\n./bin/hdfs getconf -namenodes\n./bin/hdfs getconf -secondaryNameNodes\n\n\n//clearn up and Restart\nps -lef|grep "hadoop"\nkill -9 31156\n\ncd hadoop-current/\n./sbin/stop-dfs.sh\nps -lef|grep "hadoop"\n\nrm -r ~/hadoop-current/data/ ~/hadoop-current/logs/\n\n#In Zookeeper, delete the directory storing HDFS HA information.\n./bin/zkCli.sh -server localhost:12006\n[zk: localhost:2181(CONNECTED) 0] rmr /hadoop-ha\n\nthen repeat previous boot up process\n\n')])])]),t("h5",{attrs:{id:"flink"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flink"}},[e._v("#")]),e._v(" Flink")]),e._v(" "),t("p",[e._v("HOME: /home/flink")]),e._v(" "),t("p",[e._v("download: https://downloads.apache.org/flink/flink-1.11.2/flink-1.11.2-bin-scala_2.11.tgz")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("Target Setup\n10.1.1.1: active JobManager, TaskManager\n10.1.1.2: standby JobManager, TaskManager\n10.1.1.3: standby JobManager, TaskManager\n\nuseradd -m -d /home/flink flink\npasswd flink\n\nsu - flink\n\ntar zxvf /tmp/flink-1.11.2-bin-scala_2.11.tgz \nln -s flink-1.11.2 flink-current\n\nchmod -R 755 /home/hadoop\n\nssh-keygen\nssh-copy-id flink@10.1.1.1\nssh-copy-id flink@10.1.1.2\nssh-copy-id flink@10.1.1.3\n\nvi .bash_profile \n    PATH=$PATH:$HOME/.local/bin:$HOME/bin\n\n    export PATH\n    export HADOOP_HOME=/home/hadoop/hadoop-current\n    export HADOOP_CLASSPATH=$(/home/hadoop/hadoop-current/bin/hadoop classpath)\n\n\ncd ~/flink-current/conf/\ncp flink-conf.yaml flink-conf.yaml.bak\nvi flink-conf.yaml\n\nenv.hadoop.conf.dir: /home/hadoop/hadoop-current/etc/hadoop\nenv.log.dir: /home/flink/flink-current/log\n\n\nio.tmp.dirs: /home/flink/flink-current/iotmp\n\nparallelism.default: 1\n\nrest.port: 13001\n\n# Enable uploading JAR files via web UI.\nweb.submit.enable: true\nweb.upload.dir: /home/flink/flink-current/upload\n\nhigh-availability: zookeeper\nhigh-availability.zookeeper.quorum: 10.1.1.1:12006,10.1.1.2:12006,10.1.1.3:12006\nhigh-availability.zookeeper.path.root: /flink\n#keeps the hostname without underscore\nhigh-availability.storageDir: hdfs://vm-v01:13101/flink/ha/ \nhigh-availability.cluster-id: /default_ns\nhigh-availability.jobmanager.port: 13002\n\njobmanager.memory.process.size: 16384m\n#jobmanager.heap.size: 4096m\njobmanager.execution.failover-strategy: region\n\n# JobManager blob server port used for data transfer.\nblob.server.port: 13003\n\ntaskmanager.memory.process.size: 16384m\n#taskmanager.memory.flink.size: 8192m\ntaskmanager.memory.jvm-metaspace.size: 4096m\ntaskmanager.numberOfTaskSlots: 8\ntaskmanager.data.port: 13004\ntaskmanager.rpc.port: 13005\n\nrestart-strategy: failure-rate\nrestart-strategy.failure-rate.max-failures-per-interval: 10\nrestart-strategy.failure-rate.failure-rate-interval: 300s\nrestart-strategy.failure-rate.delay: 15s\n\n# JobManager and TaskManager expose /metrics REST API for Prometheus to scrape.\nmetrics.reporters: prom\nmetrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter\nmetrics.reporter.prom.port: 13006-13007\n\nvim masters\n10.1.1.1:50001\n10.1.1.2:50001\n10.1.1.3:50001\n\nvim slaves \n10.1.1.1\n10.1.1.2\n10.1.1.3\n\ncd ~/flink-current\nmkdir upload\nmkdir iotmp\n./bin/start-cluster.sh (start only one host)\n./bin/stop-cluster.sh\n\n./flink-current/bin/flink -v\n\n#clean up & restart\n\n./bin/stop-cluster.sh\nrm -r ~/flink-current/log/* ~/flink-current/iotmp/* ~/flink-current/upload/*\n#In HDFS, delete the directory storing Flink HA information (according to flink-conf.yaml high-availability.storageDir).\n./bin/hdfs dfs -rm -r /flink/ha\n\n#In Zookeeper, delete the znode storing Flink HA information (according to flink-conf.yaml high-availability.zookeeper.path.root and high-availability.cluster-id).\n./bin/zkCli.sh -server localhost:12006\n[zk: localhost:2181(CONNECTED) 0] rmr /flink/default_ns\n")])])]),t("h4",{attrs:{id:"log-analysis-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#log-analysis-2"}},[e._v("#")]),e._v(" Log Analysis")]),e._v(" "),t("h5",{attrs:{id:"job-manager-log-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#job-manager-log-2"}},[e._v("#")]),e._v(" Job Manager Log")]),e._v(" "),t("p",[e._v("vim log/flink-root-standalonesession-0-vm01.log")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("\n")])])]),t("h5",{attrs:{id:"task-manager-log-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#task-manager-log-2"}},[e._v("#")]),e._v(" Task Manager Log")]),e._v(" "),t("p",[e._v("vim flink-root-taskexecutor-0-vm01.log")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("2022-05-26 19:05:08,330 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils            - Trying to select the network interface and address to use by connecting to the leading JobManager.\n2022-05-26 19:05:08,330 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils            - TaskManager will try to connect for PT10S before falling back to heuristics\n2022-05-26 19:05:08,474 INFO  org.apache.flink.runtime.net.ConnectionUtils                  - Retrieved new target address /X.X.X.3:13002.\n")])])]),t("h4",{attrs:{id:"failover-recoery"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#failover-recoery"}},[e._v("#")]),e._v(" Failover&Recoery")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/try-flink/flink-operations-playground/")]),e._v(" "),t("h3",{attrs:{id:"_2-3-resource-provider-cluster-mode-hadoop-yarn-cluster"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-resource-provider-cluster-mode-hadoop-yarn-cluster"}},[e._v("#")]),e._v(" 2.3 Resource Provider Cluster Mode: Hadoop YARN cluster")]),e._v(" "),t("p",[e._v("Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN’s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.")]),e._v(" "),t("p",[e._v("Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/resource-providers/yarn/")]),e._v(" "),t("h3",{attrs:{id:"_2-4-configuration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-configuration"}},[e._v("#")]),e._v(" 2.4 Configuration")]),e._v(" "),t("h4",{attrs:{id:"working-directory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#working-directory"}},[e._v("#")]),e._v(" Working Directory")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/resource-providers/standalone/working_directory/")]),e._v(" "),t("p",[e._v("Flink supports to configure a working directory (FLIP-198) for Flink processes (JobManager and TaskManager). The working directory is used by the processes to store information that can be recovered upon a process restart. The requirement for this to work is that the process is started with the same identity and has access to the volume on which the working directory is stored.")]),e._v(" "),t("h4",{attrs:{id:"dynamic-properties"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dynamic-properties"}},[e._v("#")]),e._v(" dynamic properties")]),e._v(" "),t("p",[e._v("The following scripts also allow configuration parameters to be set via dynamic properties:")]),e._v(" "),t("p",[e._v("jobmanager.sh\nstandalone-job.sh\ntaskmanager.sh\nhistoryserver.sh")]),e._v(" "),t("h4",{attrs:{id:"log-debug"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#log-debug"}},[e._v("#")]),e._v(" log debug:")]),e._v(" "),t("p",[e._v("conf/log4.properties rootLogger.level = DEBUG")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/advanced/logging/")]),e._v(" "),t("h4",{attrs:{id:"basics"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#basics"}},[e._v("#")]),e._v(" basics")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/config/")]),e._v(" "),t("ul",[t("li",[e._v("web.upload.dir")])]),e._v(" "),t("p",[e._v("JAR files are renamed when they are uploaded and stored in a directory that can be configured with the web.upload.dir configuration key.")]),e._v(" "),t("p",[e._v('If the web.upload.dir parameter is not set, the JAR files are stored in a dynamically generated directory under the jobmanager.web.tmpdir (default is System.getProperty("java.io.tmpdir")).')]),e._v(" "),t("h3",{attrs:{id:"_2-4-advanced"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-advanced"}},[e._v("#")]),e._v(" 2.4 Advanced")]),e._v(" "),t("h4",{attrs:{id:"history-server"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#history-server"}},[e._v("#")]),e._v(" History Server")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/advanced/historyserver/")]),e._v(" "),t("p",[e._v("Flink has a history server that can be used to query the statistics of completed jobs after the corresponding Flink cluster has been shut down.")]),e._v(" "),t("p",[e._v("Furthermore, it exposes a REST API that accepts HTTP requests and responds with JSON data.")]),e._v(" "),t("h4",{attrs:{id:"external-resource-framework"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#external-resource-framework"}},[e._v("#")]),e._v(" External Resource Framework")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/advanced/external_resources/")]),e._v(" "),t("p",[e._v("In addition to CPU and memory, many workloads also need some other resources, e.g. GPUs for deep learning. To support external resources, Flink provides an external resource framework. The framework supports requesting various types of resources from the underlying resource management systems (e.g., Kubernetes), and supplies information needed for using these resources to the operators. Different resource types can be supported. You can either leverage built-in plugins provided by Flink (currently only for GPU support), or implement your own plugins for custom resource types.")]),e._v(" "),t("h4",{attrs:{id:"custom-failure-enrichers"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#custom-failure-enrichers"}},[e._v("#")]),e._v(" Custom failure enrichers")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/advanced/failure_enrichers/")]),e._v(" "),t("p",[e._v("Flink provides a pluggable interface for users to register their custom logic and enrich failures with extra metadata labels (string key-value pairs). This enables users to implement their own failure enrichment plugins to categorize job failures, expose custom metrics, or make calls to external notification systems.")]),e._v(" "),t("p",[e._v("FailureEnrichers are triggered every time an exception is reported at runtime by the JobManager. Every FailureEnricher may asynchronously return labels associated with the failure that are then exposed via the JobManager’s REST API (e.g., a ’type:System’ label implying the failure is categorized as a system error).")]),e._v(" "),t("h2",{attrs:{id:"_3-api-libs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-api-libs"}},[e._v("#")]),e._v(" 3. API&Libs")]),e._v(" "),t("h3",{attrs:{id:"layered-apis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#layered-apis"}},[e._v("#")]),e._v(" Layered APIs")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://nightlies.apache.org/flink/flink-docs-release-1.15/fig/levels_of_abstraction.svg",alt:""}})]),e._v(" "),t("h4",{attrs:{id:"stateful-event-driven-applications-processfunctions-events-state-time"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#stateful-event-driven-applications-processfunctions-events-state-time"}},[e._v("#")]),e._v(" Stateful Event-Driven Applications - ProcessFunctions(events,state,time)")]),e._v(" "),t("p",[e._v("ProcessFunctions are the most expressive function interfaces that Flink offers. Flink provides ProcessFunctions to process individual events from one or two input streams or events that were grouped in a window. ProcessFunctions provide fine-grained control over time and state. A ProcessFunction can arbitrarily modify its state and register timers that will trigger a callback function in the future. Hence, ProcessFunctions can implement complex per-event business logic as required for many stateful event-driven applications.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('/**\n * Matches keyed START and END events and computes the difference between \n * both elements\' timestamps. The first String field is the key attribute, \n * the second String attribute marks START and END events.\n */\npublic static class StartEndDuration\n    extends KeyedProcessFunction<String, Tuple2<String, String>, Tuple2<String, Long>> {\n\n  private ValueState<Long> startTime;\n\n  @Override\n  public void open(Configuration conf) {\n    // obtain state handle\n    startTime = getRuntimeContext()\n      .getState(new ValueStateDescriptor<Long>("startTime", Long.class));\n  }\n\n  /** Called for each processed event. */\n  @Override\n  public void processElement(\n      Tuple2<String, String> in,\n      Context ctx,\n      Collector<Tuple2<String, Long>> out) throws Exception {\n\n    switch (in.f1) {\n      case "START":\n        // set the start time if we receive a start event.\n        startTime.update(ctx.timestamp());\n        // register a timer in four hours from the start event.\n        ctx.timerService()\n          .registerEventTimeTimer(ctx.timestamp() + 4 * 60 * 60 * 1000);\n        break;\n      case "END":\n        // emit the duration between start and end event\n        Long sTime = startTime.value();\n        if (sTime != null) {\n          out.collect(Tuple2.of(in.f0, ctx.timestamp() - sTime));\n          // clear the state\n          startTime.clear();\n        }\n      default:\n        // do nothing\n    }\n  }\n\n  /** Called when a timer fires. */\n  @Override\n  public void onTimer(\n      long timestamp,\n      OnTimerContext ctx,\n      Collector<Tuple2<String, Long>> out) {\n\n    // Timeout interval exceeded. Cleaning up the state.\n    startTime.clear();\n  }\n}\n')])])]),t("h4",{attrs:{id:"stream-batch-data-processing-datastream-api-streams-windows"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#stream-batch-data-processing-datastream-api-streams-windows"}},[e._v("#")]),e._v(" Stream-&Batch Data Processing - DataStream API(streams,windows)")]),e._v(" "),t("p",[e._v("The DataStream API provides primitives for many common stream processing operations, such as windowing, record-at-a-time transformations, and enriching events by querying an external data store. The DataStream API is available for Java and Scala and is based on functions, such as map(), reduce(), and aggregate(). Functions can be defined by extending interfaces or as Java or Scala lambda functions.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("// a stream of website clicks\nDataStream<Click> clicks = ...\n\nDataStream<Tuple2<String, Long>> result = clicks\n  // project clicks to userId and add a 1 for counting\n  .map(\n    // define function by implementing the MapFunction interface.\n    new MapFunction<Click, Tuple2<String, Long>>() {\n      @Override\n      public Tuple2<String, Long> map(Click click) {\n        return Tuple2.of(click.userId, 1L);\n      }\n    })\n  // key by userId (field 0)\n  .keyBy(0)\n  // define session window with 30 minute gap\n  .window(EventTimeSessionWindows.withGap(Time.minutes(30L)))\n  // count clicks per session. Define function as lambda function.\n  .reduce((a, b) -> Tuple2.of(a.f0, a.f1 + b.f1));\n")])])]),t("p",[e._v("try out:\nhttps://nightlies.apache.org/flink/flink-docs-release-1.15/docs/try-flink/datastream/")]),e._v(" "),t("h4",{attrs:{id:"high-level-analytics-api-sql-tableapi-dynamic-tables"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#high-level-analytics-api-sql-tableapi-dynamic-tables"}},[e._v("#")]),e._v(" High-level Analytics API - SQL/TableAPI(dynamic tables)")]),e._v(" "),t("p",[e._v("Flink features two relational APIs, the Table API and SQL. Both APIs are unified APIs for batch and stream processing, i.e., queries are executed with the same semantics on unbounded, real-time streams or bounded, recorded streams and produce the same results. The Table API and SQL leverage Apache Calcite for parsing, validation, and query optimization. They can be seamlessly integrated with the DataStream and DataSet APIs and support user-defined scalar, aggregate, and table-valued functions.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("SELECT userId, COUNT(*)\nFROM clicks\nGROUP BY SESSION(clicktime, INTERVAL '30' MINUTE), userId\n")])])]),t("p",[e._v("try out:\nhttps://nightlies.apache.org/flink/flink-docs-release-1.15/docs/try-flink/table_api/")]),e._v(" "),t("p",[e._v("SQL GATEWAY https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/sql-gateway/overview/#/starting-the-sql-gateway")]),e._v(" "),t("h3",{attrs:{id:"advanced-apis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#advanced-apis"}},[e._v("#")]),e._v(" Advanced APIs")]),e._v(" "),t("h4",{attrs:{id:"stateful-functions-a-platform-independent-stateful-serverless-stack"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#stateful-functions-a-platform-independent-stateful-serverless-stack"}},[e._v("#")]),e._v(" Stateful Functions: A Platform-Independent Stateful Serverless Stack")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-statefun-docs-stable/")]),e._v(" "),t("h4",{attrs:{id:"flink-ml-apache-flink-machine-learning-library"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flink-ml-apache-flink-machine-learning-library"}},[e._v("#")]),e._v(" Flink ML: Apache Flink Machine Learning Library")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-ml-docs-stable/\nhttps://nightlies.apache.org/flink/flink-ml-docs-release-2.0/docs/try-flink-ml/quick-start/")]),e._v(" "),t("h3",{attrs:{id:"libraries"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#libraries"}},[e._v("#")]),e._v(" Libraries")]),e._v(" "),t("h4",{attrs:{id:"complex-event-processing-cep"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#complex-event-processing-cep"}},[e._v("#")]),e._v(" Complex Event Processing (CEP):")]),e._v(" "),t("p",[e._v("Pattern detection is a very common use case for event stream processing. Flink’s CEP library provides an API to specify patterns of events (think of regular expressions or state machines). The CEP library is integrated with Flink’s DataStream API, such that patterns are evaluated on DataStreams. Applications for the CEP library include network intrusion detection, business process monitoring, and fraud detection.")]),e._v(" "),t("h4",{attrs:{id:"dataset-api"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dataset-api"}},[e._v("#")]),e._v(" DataSet API:")]),e._v(" "),t("p",[e._v("The DataSet API is Flink’s core API for batch processing applications. The primitives of the DataSet API include map, reduce, (outer) join, co-group, and iterate. All operations are backed by algorithms and data structures that operate on serialized data in memory and spill to disk if the data size exceed the memory budget. The data processing algorithms of Flink’s DataSet API are inspired by traditional database operators, such as hybrid hash-join or external merge-sort.")]),e._v(" "),t("h4",{attrs:{id:"gelly"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gelly"}},[e._v("#")]),e._v(" Gelly:")]),e._v(" "),t("p",[e._v("Gelly is a library for scalable graph processing and analysis. Gelly is implemented on top of and integrated with the DataSet API. Hence, it benefits from its scalable and robust operators. Gelly features built-in algorithms, such as label propagation, triangle enumeration, and page rank, but provides also a Graph API that eases the implementation of custom graph algorithms.")]),e._v(" "),t("h3",{attrs:{id:"exmaples"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#exmaples"}},[e._v("#")]),e._v(" Exmaples:")]),e._v(" "),t("p",[e._v("Flink State管理和使用 https://juejin.cn/post/7194847015677722681#heading-2")]),e._v(" "),t("h2",{attrs:{id:"_4-operations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-operations"}},[e._v("#")]),e._v(" 4. Operations")]),e._v(" "),t("h3",{attrs:{id:"run-your-applications-non-stop-24-7"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#run-your-applications-non-stop-24-7"}},[e._v("#")]),e._v(" Run Your Applications Non-Stop 24/7")]),e._v(" "),t("p",[e._v("Machine and process failures are ubiquitous in distributed systems. A distributed stream processors like Flink must recover from failures in order to be able to run streaming applications 24/7. Obviously, this does not only mean to restart an application after a failure but also to ensure that its internal state remains consistent, such that the application can continue processing as if the failure had never happened.")]),e._v(" "),t("ul",[t("li",[e._v("Consistent Checkpoints:\nFlink’s recovery mechanism is based on consistent checkpoints of an application’s state. In case of a failure, the application is restarted and its state is loaded from the latest checkpoint. In combination with resettable stream sources, this feature can guarantee exactly-once state consistency.")]),e._v(" "),t("li",[e._v("Efficient Checkpoints:\nCheckpointing the state of an application can be quite expensive if the application maintains terabytes of state. Flink’s can perform asynchronous and incremental checkpoints, in order to keep the impact of checkpoints on the application’s latency SLAs very small.")]),e._v(" "),t("li",[e._v("End-to-End Exactly-Once:\nFlink features transactional sinks for specific storage systems that guarantee that data is only written out exactly once, even in case of failures.")]),e._v(" "),t("li",[e._v("Integration with Cluster Managers:\nFlink is tightly integrated with cluster managers, such as Hadoop YARN, Mesos, or Kubernetes. When a process fails, a new process is automatically started to take over its work.")]),e._v(" "),t("li",[e._v("High-Availability Setup:\nFlink feature a high-availability mode that eliminates all single-points-of-failure. The HA-mode is based on Apache ZooKeeper, a battle-proven service for reliable distributed coordination.")])]),e._v(" "),t("h3",{attrs:{id:"update-migrate-suspend-and-resume-your-applications"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-migrate-suspend-and-resume-your-applications"}},[e._v("#")]),e._v(" Update, Migrate, Suspend, and Resume Your Applications")]),e._v(" "),t("p",[e._v("Streaming applications that power business-critical services need to be maintained. Bugs need to be fixed and improvements or new features need to be implemented. However, updating a stateful streaming application is not trivial. Often one cannot simply stop the applications and restart a fixed or improved version because one cannot afford to lose the state of the application.")]),e._v(" "),t("p",[e._v("Flink’s Savepoints are a unique and powerful feature that solves the issue of updating stateful applications and many other related challenges. A savepoint is a consistent snapshot of an application’s state and therefore very similar to a checkpoint. However in contrast to checkpoints, savepoints need to be manually triggered and are not automatically removed when an application is stopped. A savepoint can be used to start a state-compatible application and initialize its state. Savepoints enable the following features:")]),e._v(" "),t("ul",[t("li",[e._v("Application Evolution:\nSavepoints can be used to evolve applications. A fixed or improved version of an application can be restarted from a savepoint that was taken from a previous version of the application. It is also possible to start the application from an earlier point in time (given such a savepoint exists) to repair incorrect results produced by the flawed version.")]),e._v(" "),t("li",[e._v("Cluster Migration:\nUsing savepoints, applications can be migrated (or cloned) to different clusters.")]),e._v(" "),t("li",[e._v("Flink Version Updates:\nAn application can be migrated to run on a new Flink version using a savepoint.")]),e._v(" "),t("li",[e._v("Application Scaling:\nSavepoints can be used to increase or decrease the parallelism of an application.")]),e._v(" "),t("li",[e._v("A/B Tests and What-If Scenarios:\nThe performance or quality of two (or more) different versions of an application can be compared by starting all versions from the same savepoint.")]),e._v(" "),t("li",[e._v("Pause and Resume:\nAn application can be paused by taking a savepoint and stopping it. At any later point in time, the application can be resumed from the savepoint.")]),e._v(" "),t("li",[e._v("Archiving:\nSavepoints can be archived to be able to reset the state of an application to an earlier point in time.")])]),e._v(" "),t("h3",{attrs:{id:"monitor-and-control-your-applications"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#monitor-and-control-your-applications"}},[e._v("#")]),e._v(" Monitor and Control Your Applications")]),e._v(" "),t("p",[e._v("Just like any other service, continuously running streaming applications need to be supervised and integrated into the operations infrastructure, i.e., monitoring and logging services, of an organization. Monitoring helps to anticipate problems and react ahead of time. Logging enables root-cause analysis to investigate failures. Finally, easily accessible interfaces to control running applications are an important feature.")]),e._v(" "),t("p",[e._v("Flink integrates nicely with many common logging and monitoring services and provides a REST API to control applications and query information.")]),e._v(" "),t("ul",[t("li",[e._v("Web UI: Flink features a web UI to inspect, monitor, and debug running applications. It can also be used to submit executions for execution or cancel them.")]),e._v(" "),t("li",[e._v("Logging: Flink implements the popular slf4j logging interface and integrates with the logging frameworks log4j or logback.")]),e._v(" "),t("li",[e._v("Metrics: Flink features a sophisticated metrics system to collect and report system and user-defined metrics. Metrics can be exported to several reporters, including JMX, Ganglia, Graphite, Prometheus, StatsD, Datadog, and Slf4j.")]),e._v(" "),t("li",[e._v("REST API: Flink exposes a REST API to submit a new application, take a savepoint of a running application, or cancel an application. The REST API also exposes meta data and collected metrics of running or completed applications.")])]),e._v(" "),t("h3",{attrs:{id:"debug"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#debug"}},[e._v("#")]),e._v(" Debug")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/ops/debugging/debugging_classloading/",target:"_blank",rel:"noopener noreferrer"}},[e._v("User jars & Classpath"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("In Standalone mode, the following jars will be recognized as user-jars and included into user classpath:")]),e._v(" "),t("p",[e._v("Session Mode: The JAR file specified in startup command.\nApplication Mode: The JAR file specified in startup command and all JAR files in Flink’s usrlib folder.")]),e._v(" "),t("h2",{attrs:{id:"_5-integration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-integration"}},[e._v("#")]),e._v(" 5. Integration")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/rest_api/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rest API"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("dashboard: localhost:8081")]),e._v(" "),t("p",[e._v("jars:")]),e._v(" "),t("p",[e._v("http://localhost:8081/jars\nhttp://localhost:8081/jars/${jarId}/run\nhttp://localhost:8081/v1/config\nhttp://localhost:8081/v1/jobmanager/config\nhttp://localhost:8081/v1/jobmanager/logs")]),e._v(" "),t("h2",{attrs:{id:"_6-monitoring"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-monitoring"}},[e._v("#")]),e._v(" 6. Monitoring")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://flink.apache.org/2019/02/21/monitoring-apache-flink-applications-101/#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Monitoring Apache Flink Applications 101"),t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"customized"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#customized"}},[e._v("#")]),e._v(" Customized")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('package common;\n\nimport java.text.SimpleDateFormat;\nimport java.time.format.DateTimeFormatter;\n\nimport org.apache.flink.api.common.functions.RichMapFunction;\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.metrics.Counter;\nimport org.apache.flink.metrics.Gauge;\nimport org.apache.flink.metrics.MetricGroup;\nimport org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode;\n\npublic class MetricMapper extends RichMapFunction<ObjectNode, ObjectNode> {\n    private transient Counter totalcounter;\n    private transient Counter mycounter;\n    private transient long currenttimestamp;\n    private SimpleDateFormat  dateFormatter = new SimpleDateFormat ("yyyyMMdd HH:mm:ss.S");\n    @Override\n    public void open(Configuration config) {\n      MetricGroup metricGroup=getRuntimeContext().getMetricGroup();\n      this.totalcounter =  metricGroup.addGroup("nss").counter("totalcount");\n      this.mycounter =  metricGroup.addGroup("nss").counter("mycounter");\n      metricGroup.addGroup("nss").gauge("currenttime", new Gauge<Long>() {\n        @Override\n        public Long getValue() {\n          return currenttimestamp;\n        }\n      });\n    }\n  \n    @Override\n    public ObjectNode map(ObjectNode value) throws Exception {\n      if(value.has("MsgType")){\n        switch(value.get("MsgType").asText()){\n          case Constant.TYPE1:\n            this.ordercounter.inc();\n            this.totalcounter.inc();\n            if(value.has("time")){\n              this.currenttimestamp=dateFormatter.parse(value.get("time").asText()).getTime();\n            }\n            break;\n\n        }\n      }\n      return value;\n    }\n  }\n  \n')])])]),t("h3",{attrs:{id:"view-metric"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#view-metric"}},[e._v("#")]),e._v(" View Metric")]),e._v(" "),t("h4",{attrs:{id:"flink-webui"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flink-webui"}},[e._v("#")]),e._v(" flink webui")]),e._v(" "),t("p",[e._v("job => overview => click on the job=> scroll to the end => metric")]),e._v(" "),t("h4",{attrs:{id:"rest-monitoring-api"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rest-monitoring-api"}},[e._v("#")]),e._v(" Rest Monitoring API")]),e._v(" "),t("p",[e._v("In order to query the metric via Flink's REST interface you need to first figure some ids out:")]),e._v(" "),t("p",[e._v("flink_cluster: Address of your flink cluster\nport: Port of REST endpoint\njobId: Id of your job which can be figured out via http://flink_cluster:port/jobs\nvertexId: Id of the vertex to query. This can be figured out via http://flink_cluster:port/jobs/:jobId which gives you the job information with all vertexIds\nsubtaskindex: Index of the parallel subtask to query\nhttp://flink_cluster:port/jobs/:jobId/vertices/:vertexId/subtasks/:subtaskindex/metrics?get=MyCustomMetric")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('http://X.X.X.X:8081//jobs/44dd9736f9f84bd6a545f7fea059245e/#/\n\n{"jid":"44dd9736f9f84bd6a545f7fea059245e","name":"TEST Postgres (TEST20240607) > Flink > Redis","isStoppable":false,"state":"FINISHED","start-time":1717989261618,"end-time":1717989299500,"duration":37882,"now":1717990389537,"timestamps":{"INITIALIZING":1717989261618,"CANCELLING":0,"CANCELED":0,"FAILING":0,"FAILED":0,"RESTARTING":0,"SUSPENDED":0,"FINISHED":1717989299500,"RUNNING":1717989261639,"RECONCILING":0,"CREATED":1717989261630},\n"vertices":[{"id":"cbc357ccb763df2852fee8c4fc7d55f2","name":"Source: Custom Source -> Map -> Map -> Map -> Sink: Unnamed","parallelism":1,"status":"FINISHED","start-time":1717989261685,"end-time":1717989299499,"duration":37814,"tasks":{"DEPLOYING":0,"CANCELED":0,"RECONCILING":0,"FINISHED":1,"SCHEDULED":0,"RUNNING":0,"FAILED":0,"CREATED":0,"CANCELING":0},"metrics":{"read-bytes":0,"read-bytes-complete":true,"write-bytes":0,"write-bytes-complete":true,"read-records":0,"read-records-complete":true,"write-records":0,"write-records-complete":true}}],"status-counts":{"DEPLOYING":0,"CANCELED":0,"RECONCILING":0,"FINISHED":1,"SCHEDULED":0,"RUNNING":0,"FAILED":0,"CREATED":0,"CANCELING":0},"plan":{"jid":"44dd9736f9f84bd6a545f7fea059245e","name":"TEST Postgres (TEST20240607) > Flink > Redis","nodes":[{"id":"cbc357ccb763df2852fee8c4fc7d55f2","parallelism":1,"operator":"","operator_strategy":"","description":"Source: Custom Source -&gt; Map -&gt; Map -&gt; Map -&gt; Sink: Unnamed","optimizer_properties":{}}]}}\n\nhttp://X.X.X.X:8081//jobs/44dd9736f9f84bd6a545f7fea059245e/vertices/cbc357ccb763df2852fee8c4fc7d55f2/#/\n{"id":"cbc357ccb763df2852fee8c4fc7d55f2","name":"Source: Custom Source -> Map -> Map -> Map -> Sink: Unnamed","parallelism":1,"now":1717990526875,\n"subtasks":[{"subtask":0,"status":"FINISHED","attempt":0,"host":"sghc5-stag-nss-beta-v01","start-time":1717989261685,"end-time":1717989299499,"duration":37814,"metrics":{"read-bytes":0,"read-bytes-complete":true,"write-bytes":0,"write-bytes-complete":true,"read-records":0,"read-records-complete":true,"write-records":0,"write-records-complete":true},"taskmanager-id":"X.X.X.X:43268-200172","start_time":1717989261685}]}\n\nhttp://X.X.X.X:8081//jobs/44dd9736f9f84bd6a545f7fea059245e/vertices/cbc357ccb763df2852fee8c4fc7d55f2/subtasks/0/metrics#/\n[{"id":"Shuffle.Netty.Output.Buffers.outPoolUsage"},{"id":"checkpointStartDelayNanos"},{"id":"numBytesInLocal"},{"id":"Shuffle.Netty.Input.numBytesInRemotePerSecond"},{"id":"numBytesInRemotePerSecond"},{"id":"Source__Custom_Source.numRecordsInPerSecond"},{"id":"numBytesOut"},{"id":"Map.MyGauge"},{"id":"numBytesIn"},{"id":"Map.numRecordsInPerSecond"},{"id":"numBuffersOut"},{"id":"Shuffle.Netty.Input.numBuffersInLocal"},{"id":"numBuffersInRemotePerSecond"},{"id":"numBytesOutPerSecond"},{"id":"Map.myCounter"},{"id":"buffers.outputQueueLength"},{"id":"numBuffersOutPerSecond"},{"id":"Shuffle.Netty.Input.Buffers.inputExclusiveBuffersUsage"},{"id":"isBackPressured"},{"id":"Sink__Unnamed.numRecordsIn"},{"id":"numBytesInLocalPerSecond"},{"id":"Map.currentOutputWatermark"},{"id":"Source__Custom_Source.numSplitsProcessed"},{"id":"Sink__Unnamed.numRecordsOut"},{"id":"buffers.inPoolUsage"},{"id":"idleTimeMsPerSecond"},{"id":"Shuffle.Netty.Input.numBytesInLocalPerSecond"},{"id":"numBytesInRemote"},{"id":"Source__Custom_Source.numRecordsOut"},{"id":"Map.numRecordsOutPerSecond"},{"id":"Shuffle.Netty.Input.numBytesInLocal"},{"id":"Shuffle.Netty.Input.numBytesInRemote"},{"id":"Shuffle.Netty.Output.Buffers.outputQueueLength"},{"id":"Sink__Unnamed.currentInputWatermark"},{"id":"buffers.inputFloatingBuffersUsage"},{"id":"Shuffle.Netty.Input.Buffers.inPoolUsage"},{"id":"Map.currentInputWatermark"},{"id":"numBuffersInLocalPerSecond"},{"id":"numRecordsOut"},{"id":"numBuffersInLocal"},{"id":"Source__Custom_Source.currentOutputWatermark"},{"id":"numBuffersInRemote"},{"id":"Map.numRecordsOut"},{"id":"buffers.inputQueueLength"},{"id":"Source__Custom_Source.numRecordsOutPerSecond"},{"id":"Sink__Unnamed.numRecordsInPerSecond"},{"id":"numRecordsIn"},{"id":"Shuffle.Netty.Input.numBuffersInRemote"},{"id":"numBytesInPerSecond"},{"id":"Shuffle.Netty.Input.Buffers.inputQueueLength"},{"id":"Source__Custom_Source.numRecordsIn"},{"id":"buffers.inputExclusiveBuffersUsage"},{"id":"Map.numRecordsIn"},{"id":"Shuffle.Netty.Input.numBuffersInRemotePerSecond"},{"id":"numRecordsOutPerSecond"},{"id":"buffers.outPoolUsage"},{"id":"Sink__Unnamed.numRecordsOutPerSecond"},{"id":"Shuffle.Netty.Input.numBuffersInLocalPerSecond"},{"id":"numRecordsInPerSecond"},{"id":"Shuffle.Netty.Input.Buffers.inputFloatingBuffersUsage"},{"id":"Sink__Unnamed.currentOutputWatermark"}]\n\nhttp://X.X.X.X:8081//jobs/44dd9736f9f84bd6a545f7fea059245e/vertices/cbc357ccb763df2852fee8c4fc7d55f2/subtasks/0/metrics?get=Map.myCounter#/\n[{"id":"Map.myCounter","value":"63509"}]\n')])])]),t("h3",{attrs:{id:"metric-reporters"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#metric-reporters"}},[e._v("#")]),e._v(" Metric Reporters")]),e._v(" "),t("p",[e._v("https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/deployment/metric_reporters/#/")]),e._v(" "),t("h4",{attrs:{id:"prometheus"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prometheus"}},[e._v("#")]),e._v(" Prometheus")]),e._v(" "),t("p",[e._v("前面都很顺利，到这里就有问题了！\nhttps://github.com/mbode/flink-prometheus-example#/\nhttps://juejin.cn/post/7037379343248523295#/")]),e._v(" "),t("p",[e._v("首先是要配置flink的plugin metric-Prometheus reporter")]),e._v(" "),t("p",[e._v('有人说还需要把flink-metrics-prometheus-1.12.0.jar放到 flink/lib下面，貌似不必要\n$ find / -name "flink-metrics-prometheus*"\n/apex/ngs/svl/flink/flink-1.12.0/plugins/metrics-prometheus/flink-metrics-prometheus-1.12.0.jar')]),e._v(" "),t("p",[e._v("刚开始根据文档这么配置")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter\n\n$ grep -r "prometheus" ../log/*\n../log/flink-flink-standalonesession-0-sghc5-stag-nss-beta-v01.apex.com.log:INFO  [] - Loading configuration property: metrics.reporter.prom.class, org.apache.flink.metrics.prometheus.PrometheusReporter\n../log/flink-flink-standalonesession-0-sghc5-stag-nss-beta-v01.apex.com.log:2024-06-11 17:08:31,267 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.reporter.prom.class, org.apache.flink.metrics.prometheus.PrometheusReporter\n../log/flink-flink-standalonesession-0-sghc5-stag-nss-beta-v01.apex.com.log:2024-06-11 17:08:32,620 INFO  org.apache.flink.metrics.prometheus.PrometheusReporter       [] - Started PrometheusReporter HTTP server on port 9249.\n')])])]),t("p",[e._v("打开localhost:9249就会发现只有flink_jobmanager的信息，没有flink_taskmanager，\n应该改成如下配置，port给个区间，至少大于1")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("# JobManager and TaskManager expose /metrics REST API for Prometheus to scrape.\nmetrics.reporters: prom\nmetrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter\nmetrics.reporter.prom.port: 9250-9260\n")])])]),t("p",[e._v("查看监听端口就会发现至少有两个 9250 9251，\n一般 localhost:9250里面就是flink_jobmanager，localhost:9251就是flink_taskmanager和自定义的metric")]),e._v(" "),t("p",[e._v("接着配置Prometheus，就可以接入grafana了（注意一旦接入Prometheus，自定义的Metric在上面的localhost:9251中可能就消失了，至少低版本比如flink1.12是这样）")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("scrape_configs:\n- job_name: 'flink'\n  static_configs:\n  - targets: ['flink-host:9249]\n")])])]),t("p",[e._v("乐于助人贴：https://stackoverflow.com/questions/74386011/not-able-to-see-flink-custom-metrics-to-prometheus/78622484#/78622484")]),e._v(" "),t("h3",{attrs:{id:"backpressure-背压"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#backpressure-背压"}},[e._v("#")]),e._v(" backpressure 背压")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://flink.apache.org/2019/06/05/a-deep-dive-into-flinks-network-stack",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Deep-Dive into Flink's Network Stack"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://flink.apache.org/2021/07/07/how-to-identify-the-source-of-backpressure/#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("How to identify the source of backpressure?"),t("OutboundLink")],1)]),e._v(" "),t("h2",{attrs:{id:"_7-深度解析"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-深度解析"}},[e._v("#")]),e._v(" 7. 深度解析")]),e._v(" "),t("h3",{attrs:{id:"version-版本"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#version-版本"}},[e._v("#")]),e._v(" Version 版本")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://flink.apache.org/2020/12/10/apache-flink-1.12.0-release-announcement/#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Flink 1.12 release notes"),t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"运行模式和机制"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#运行模式和机制"}},[e._v("#")]),e._v(" 运行模式和机制")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.12/dev/datastream_execution_mode.html#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Execution Behavior"),t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"数据重分布分区和并发-maintain-partition"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据重分布分区和并发-maintain-partition"}},[e._v("#")]),e._v(" 数据重分布分区和并发 maintain partition")]),e._v(" "),t("p",[e._v("算子数据传递的两种方式:")]),e._v(" "),t("ul",[t("li",[e._v("One-to-one：数据不需要重新分布，上游SubTask生产的数据与下游SubTask受到的数据完全一致，数据不需要重分区，也就是数据不需要经过IO，比如下图中source->map的数据传递形式就是One-to-One方式。常见的map、fliter、flatMap等算子的SubTask的数据传递都是one-to-one的对应关系。类似于spark中的窄依赖。")]),e._v(" "),t("li",[e._v("Redistributing：数据需要通过shuffle过程重新分区，需要经过IO，比如上图中的map->keyBy。创建的keyBy、broadcast、rebalance、shuffle等算子的SubTask的数据传递都是Redistributing方式，但它们具体数据传递方式是不同的。类似于spark中的宽依赖。")])]),e._v(" "),t("p",[e._v("根据partitioner的分类来进行分析，主要分为四种大类型，即RoundRobinChannelSelector、StreamPartitioner、DataSkewChannelSelector、OutputEmitter四种\nhttps://cloud.tencent.com/developer/article/1863680#/")]),e._v(" "),t("p",[e._v("Flink 9种分区策略：")]),e._v(" "),t("ul",[t("li",[e._v("GlobalPartitioner 数据发到下游算子的第一个实例\n"),t("code",[e._v("dataStream.global()")])]),e._v(" "),t("li",[e._v("ShufflePartitioner 数据随机分发到下游算子\n"),t("code",[e._v("dataStream.shuffle()")])]),e._v(" "),t("li",[e._v("RebalancePartitioner 数据循环发送到下游的实例"),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("dataStream.setParallelism(2);\n\ndataStreamAfter.setParallelism(3);\n\ndataStream.rebalance()\n")])])])]),e._v(" "),t("li",[e._v("BroadcastPartitioner 输出到下游算子的每个实例中\n"),t("code",[e._v("dataStream.broadcast()")])]),e._v(" "),t("li",[e._v("ForwardPartitioner 上下游算子并行度一致\n"),t("code",[e._v("dataStream.forward()")]),e._v("\n对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常。\n既然如此有什么用？\n"),t("ul",[t("li",[e._v("Minimizing Shuffling Overhead\nWhen you have multiple operators in a Flink job connected by data streams, each operator may have a different degree of parallelism. Flink distributes data between operators based on keys or partitioning strategies. However, when you know that the downstream operator logically follows the upstream operator without any need for key-based shuffling or redistribution, you can use forward().")]),e._v(" "),t("li",[e._v("Efficiency in Data Movement\nBy using forward(), Flink avoids unnecessary data serialization, deserialization, and network overhead associated with key-based shuffling. This can lead to improved performance, especially in scenarios where data doesn't need to be redistributed across operators based on keys or partitions.")])])]),e._v(" "),t("li",[e._v("KeyGroupStreamPartitioner 按Key的Hash值输出到下游算子\nKey分区策略根据元素Key的Hash值输出到下游算子指定的实例。keyBy()算子底层正是使用的该分区策略，底层最终会调用KeyGroupStreamPartitioner的selectChannel()方法，计算每个Key对应的通道索引（通道编号，可理解为分区编号），根据通道索引将Key发送到下游相应的分区中。\n总的来说，Flink底层计算通道索引（分区编号）的流程如下：\n计算Key的HashCode值。\n将Key的HashCode值进行特殊的Hash处理，即MathUtils.murmurHash(keyHash)，返回一个非负哈希码。\n将非负哈希码除以最大并行度取余数，得到keyGroupId，即Key组索引。\n使用公式keyGroupId×parallelism/maxParallelism得到分区编号。parallelism为当前算子的并行度，即通道数量；maxParallelism为系统默认支持的最大并行度，即128。")]),e._v(" "),t("li",[e._v("RescalePartitioner 根据上下游算子的并行度，循环输出到下游算子\n"),t("code",[e._v("dataStream.rescale()")])]),e._v(" "),t("li",[e._v("BinaryHashPartitioner 对 BinaryRowData 这种数据进行hash分区\n该分区策略位于 Flink的Table API的org.apache.flink.table.runtime.partitioner包中，是一种针对BinaryRowData的哈希分区器。BinaryRowData是RowData的实现，可以显著减少Java对象的序列化／反序列化。RowData用于表示结构化数据类型，运行时通过Table API或SQL管道传递的所有顶级记录都是RowData的实例")]),e._v(" "),t("li",[e._v("CustomPartitionerWrapper 用户自定义分区器")])]),e._v(" "),t("p",[e._v("https://blog.csdn.net/qq_42596142/article/details/103727918\nhttps://blog.csdn.net/qq_37555071/article/details/122415430\nhttps://www.51cto.com/article/782165.html")]),e._v(" "),t("p",[e._v("测试代码")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('public class PartitionerTest {\n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(3);\n        DataStream<Integer> dataStream = env.fromElements(1, 2, 3, 4, 5, 6);\n        //1.分区策略前的操作\n        //输出dataStream每个元素及所属的子任务编号\n        dataStream.map(new RichMapFunction<Integer, Object>() {\n            @Override\n            public Object map(Integer value) throws Exception {\n                System.out.println(String.format("元素值: %s, 分区策略前，子任务编号: %s", value,\n                        getRuntimeContext().getIndexOfThisSubtask()));\n                return value;\n            }\n        });\n        //2.设置分区策略\n        //设置DataStream向下游发送数据时使用的策略\n        DataStream<Integer> dataStreamAfter = dataStream.broadcast();\n        //3.分区策略后的操作\n        dataStreamAfter.map(new RichMapFunction<Integer, Object>() {\n            @Override\n            public Object map(Integer value) throws Exception {\n                System.out.println(String.format("元素值: %s, 分区策略后，子任务编号: %s", value,\n                        getRuntimeContext().getIndexOfThisSubtask()));\n                return value;\n            }\n        }).print();\n        env.execute("PartitionerTest Job");\n    }\n}\n')])])]),t("h3",{attrs:{id:"others"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#others"}},[e._v("#")]),e._v(" others")]),e._v(" "),t("p",[e._v("深入了解 Apache Flink 的网络协议栈\nhttps://tianchi.aliyun.com/forum/post/61976#/")]),e._v(" "),t("p",[e._v("Flink 已经拥有了强大的 DataStream/DataSet API，可以基本满足流计算和批计算中的所有需求。为什么还需要 Table & SQL API 呢？\nhttps://flink-learning.org.cn/article/detail/5133eced98854eff56cc2eaa2150b1e4#/")]),e._v(" "),t("p",[e._v("Continuous Queries on Dynamic Tables\nhttps://flink.apache.org/2017/03/30/continuous-queries-on-dynamic-tables/#/")]),e._v(" "),t("h2",{attrs:{id:"troubleshooting"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#troubleshooting"}},[e._v("#")]),e._v(" Troubleshooting")]),e._v(" "),t("h3",{attrs:{id:"flink启动后无法正常关闭"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flink启动后无法正常关闭"}},[e._v("#")]),e._v(" flink启动后无法正常关闭")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("$ ./stop-cluster.sh \nNo taskexecutor daemon to stop on host xxx.\nNo standalonesession daemon to stop on host xxx.\n")])])]),t("p",[e._v("flink的进程默认存储在/tmp目录下，该目录为临时目录，会被系统清理，当存储在/tmp下的进程被清理后，执行stop-cluster.sh就无法找到对应的进程并进行停止了。")]),e._v(" "),t("p",[e._v('修改flink bin目录下的config.sh文件。\nDEFAULT_ENV_PID_DIR="/tmp"，将tmp修改为指定的不会被清理的目录即可。\njps 查询进程\nkill xxxx')]),e._v(" "),t("h3",{attrs:{id:"flink-task-manager-not-starting"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flink-task-manager-not-starting"}},[e._v("#")]),e._v(" flink task manager not starting")]),e._v(" "),t("p",[e._v("1.检查每个节点的日志，看是否是因为host或端口连不上，然后检查相应端口是否正常监听以及防火墙配置\n2.flink已经rename slave=》workers，注意文件改动")]),e._v(" "),t("h3",{attrs:{id:"could-not-start-actor-system-on-any-port-in-port-range-6123"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#could-not-start-actor-system-on-any-port-in-port-range-6123"}},[e._v("#")]),e._v(" Could not start actor system on any port in port range 6123")]),e._v(" "),t("p",[e._v("现象：使用普通用户停止flink，但是web ui仍然可以访问")]),e._v(" "),t("p",[e._v("原因：root用户启动了flink")]),e._v(" "),t("h3",{attrs:{id:"noresourceavailableexception"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#noresourceavailableexception"}},[e._v("#")]),e._v(" NoResourceAvailableException")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_372]\n\tat org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_372]\n\tat org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_372]\n\tat org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1132) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_372]\n\tat org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11.2.jar:1.11.2]\n\tat akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11.2.jar:1.11.2]\nCaused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException\n\tat java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_372]\n\t... 25 more\nCaused by: java.util.concurrent.TimeoutException\n\t... 23 more\n\n")])])]),t("h4",{attrs:{id:"verify-using-official-example"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#verify-using-official-example"}},[e._v("#")]),e._v(" verify using official example")]),e._v(" "),t("p",[t("code",[e._v("/bin/flink run -p 5 ./examples/batch/WordCount.jar")])]),e._v(" "),t("h3",{attrs:{id:"task-distribution-in-apache-flink"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#task-distribution-in-apache-flink"}},[e._v("#")]),e._v(" Task distribution in Apache Flink")]),e._v(" "),t("p",[e._v("https://stackoverflow.com/questions/34773379/task-distribution-in-apache-flink")]),e._v(" "),t("p",[e._v("cluster.evenly-spread-out-slots: true")]),e._v(" "),t("h3",{attrs:{id:"分区问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#分区问题"}},[e._v("#")]),e._v(" 分区问题")]),e._v(" "),t("h4",{attrs:{id:"rescale-有无状态"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rescale-有无状态"}},[e._v("#")]),e._v(" rescale 有无状态")]),e._v(" "),t("p",[e._v("https://flink.apache.org/2017/07/04/a-deep-dive-into-rescalable-state-in-apache-flink/#/")]),e._v(" "),t("h4",{attrs:{id:"kafka-source-issue"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#kafka-source-issue"}},[e._v("#")]),e._v(" kafka source issue")]),e._v(" "),t("p",[e._v("https://stackoverflow.com/questions/70096166/parallelism-in-flink-kafka-source-causes-nothing-to-execute#/")]),e._v(" "),t("h4",{attrs:{id:"sink-can-apache-flink-write-to-files-that-are-named-based-on-a-key"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sink-can-apache-flink-write-to-files-that-are-named-based-on-a-key"}},[e._v("#")]),e._v(" sink Can Apache Flink write to files that are named based on a key?")]),e._v(" "),t("p",[e._v("That is not possible ouf-of-the-box. However, you can implement an own output format")]),e._v(" "),t("p",[e._v("https://stackoverflow.com/questions/39276290/can-apache-flink-write-to-files-that-are-named-based-on-a-key#/")]),e._v(" "),t("p",[e._v("Apache Flink can write to files with names based on a key. This functionality is typically achieved using Flink's KeyedProcessFunction or KeyedCoProcessFunction in conjunction with the ProcessFunction API.")]),e._v(" "),t("p",[e._v("Here's a general approach you can take:")]),e._v(" "),t("p",[e._v("KeyedStream: Start with a KeyedStream using the keyBy() method to partition your stream by a specific key.\nProcessFunction: Use a ProcessFunction or KeyedProcessFunction to process each element in the stream. Within this function, you can implement the logic to write to files based on the key.\nFile Output: Utilize Flink's OutputFileConfig or custom file sink implementations to control the file writing process. You can specify the file name based on the key within the ProcessFunction.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('DataStream<MyEvent> stream = ... // your event stream\nDataStream<MyEvent> keyedStream = stream.keyBy(event -> event.getKey());\n\nkeyedStream.process(new KeyedProcessFunction<KeyType, MyEvent, Void>() {\n    @Override\n    public void processElement(MyEvent event, Context ctx, Collector<Void> out) throws Exception {\n        // Write event to file named based on event.getKey()\n        String fileName = "file_" + event.getKey() + ".txt";\n        String content = event.toString(); // Convert event to string or desired format\n        // Write content to file with fileName\n        // Example: Files.write(Paths.get(fileName), content.getBytes(), StandardOpenOption.CREATE, StandardOpenOption.APPEND);\n    }\n});\n\n')])])]),t("p",[e._v("OR")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('import org.apache.flink.api.common.serialization.SimpleStringEncoder;\nimport org.apache.flink.core.fs.FileSystem;\nimport org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\nimport org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer;\nimport org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.StringElementBucketAssigner;\nimport org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy;\nimport org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.RollingPolicy;\n\npublic class KeyedFileSink extends RichSinkFunction<MyEvent> {\n\n    private String basePath;\n\n    public KeyedFileSink(String basePath) {\n        this.basePath = basePath;\n    }\n\n    @Override\n    public void invoke(MyEvent event, Context context) throws Exception {\n        String key = event.getKey();\n        String fileName = basePath + "/file_" + key + ".txt";\n        String content = event.toString(); // Convert event to string or desired format\n\n        // Write content to file with fileName\n        // Example: Files.write(Paths.get(fileName), content.getBytes(), StandardOpenOption.CREATE, StandardOpenOption.APPEND);\n    }\n}\n\n.....\nDataStream<MyEvent> stream = ... // your event stream\nstream.addSink(new KeyedFileSink("/path/to/output"));\n\n')])])]),t("h4",{attrs:{id:"partition-the-whole-datastream-in-flink-at-the-start-of-source-and-maintain-the-partition-till-sink"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#partition-the-whole-datastream-in-flink-at-the-start-of-source-and-maintain-the-partition-till-sink"}},[e._v("#")]),e._v(" Partition the whole dataStream in flink at the start of source and maintain the partition till sink")]),e._v(" "),t("p",[e._v("https://stackoverflow.com/questions/62303722/partition-the-whole-datastream-in-flink-at-the-start-of-source-and-maintain-the#/")]),e._v(" "),t("h4",{attrs:{id:"数据库并行读取-only-one-slot-is-actively-processing-data-and-the-others-quickly-finish-with-0-bytes"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据库并行读取-only-one-slot-is-actively-processing-data-and-the-others-quickly-finish-with-0-bytes"}},[e._v("#")]),e._v(" 数据库并行读取 only one slot is actively processing data and the others quickly finish with 0 bytes")]),e._v(" "),t("p",[e._v("代码片段")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('DataStream<Row> streamSource = env.createInput(createInputFormat(url,username,password,tablename));\nDataStream<String> streamJsonString = streamSource.map(row -> (String) row.getField(0));\nDataStream<ObjectNode> streamJson = streamJsonString.map(string -> (ObjectNode) JsonUtil.fromJson(string));\n\nstreamJson.addSink(new RedisAutoKeyExpirationSink<>(new SampleExtractor(), config));\n\nenv.setParallelism(3);\n\npublic static JDBCInputFormat createInputFormat(String url, String username, String password, String tablename) {\n\n        return JDBCInputFormat.buildJDBCInputFormat()\n                .setDrivername("org.postgresql.Driver")\n                .setDBUrl(url)\n                .setUsername(username)\n                .setPassword(password)\n                .setQuery(String.format("SELECT CAST(info AS TEXT) FROM %s ORDER BY id;", tablename))\n                .setRowTypeInfo(new RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO))\n                .setFetchSize(100)\n                .finish();\n    }\n')])])]),t("p",[e._v("在flink dashboard发现运行时图像显示并行度虽然是3，但是2个快速结束（0 bytes）实际只有一个在跑")]),e._v(" "),t("p",[e._v("If you set the parallelism of your Flink job to 5 and notice that only one slot is actively processing data while the other four slots quickly finish with 0 bytes, there could be a few reasons for this behavior:")]),e._v(" "),t("ul",[t("li",[e._v("Data Distribution: The data fetched from the database may not be evenly distributed among the parallel instances of the JDBC source. If the data distribution is skewed, one instance may fetch significantly more data than the others, leading to uneven processing.")]),e._v(" "),t("li",[e._v("Parallelism Mismatch: The parallelism of the JDBC source may not match the parallelism of downstream operators. Even if you set the parallelism of your Flink job to 5, if the JDBC source has a lower parallelism or if it fetches data serially, only one instance of the source will be actively fetching data while the others remain idle.")]),e._v(" "),t("li",[e._v("Resource Constraints: If the actively processing slot is starved of resources (CPU, memory, etc.), it may not be able to process data as efficiently as expected, causing other slots to finish quickly with 0 bytes.")])]),e._v(" "),t("p",[e._v("显然是第一种情况，仔细琢磨了下，上面代码实际上用的是datastream，jdbc读取出的数据转成JDBCInputFormat作为输入，"),t("br"),e._v("\n有意思的是查看"),t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/datastream/overview/#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("datastream api文档"),t("OutboundLink")],1),e._v("，JDBC是作为sink，MongoDB才有原生的source支持（在官方例子中可以看到可以直接设置partition分区类型）\n"),t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/datastream/mongodb/",target:"_blank",rel:"noopener noreferrer"}},[e._v("JDBC (sink)"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/datastream/mongodb/",target:"_blank",rel:"noopener noreferrer"}},[e._v("MongoDB (source/sink)"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("所以大概是datastream对jdbc支持的一般，所以虽然设置并行度为3，只有一个线程连接数据库读取数据，其他两个线程空转，")]),e._v(" "),t("p",[e._v("可能的解决办法：\n对接数据库采用flink tableAPI的默认BinaryHashPartitioner分区策略 https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tableapi/#/ ，然后下游转成datastream重分区\nConverting between DataStream and Table\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/data_stream_api/#/")]),e._v(" "),t("p",[e._v("partitionby()\nhttps://blog.csdn.net/dinghua_xuexi/article/details/107759503#/\nhttps://zhuanlan.zhihu.com/p/139101137#/\nhttps://www.cnblogs.com/Springmoon-venn/p/16540664.html#/")]),e._v(" "),t("p",[e._v("解决方法二：\n使用DataStream也行，不过出了设置并行度还需要设置NumericBetweenParametersProvider\nhttps://blog.csdn.net/SVDJASFHIAU/article/details/119416169#/")]),e._v(" "),t("p",[e._v("解决方法三：\n又发现了 "),t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-cdc-docs-stable/#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("cdc connector"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/overview/#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("里面明确标明了哪些支持 Parallel Read"),t("OutboundLink")],1),e._v("，所以cdc是flink官方提供的一种更强大的连接方式")]),e._v(" "),t("h3",{attrs:{id:"千万级数据断崖式变慢"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#千万级数据断崖式变慢"}},[e._v("#")]),e._v(" 千万级数据断崖式变慢")]),e._v(" "),t("p",[t("strong",[e._v("现象描述：")])]),e._v(" "),t("p",[e._v("仍然是前面提到的代码片段，taskmanger jvm memory 5G，taskmanager内存，只不过：")]),e._v(" "),t("ol",[t("li",[e._v("为了监控增加了自定义的 MetricMapper 统计数据量和处理进度")]),e._v(" "),t("li",[e._v("并行度设置为1")])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("DataStream<Row> streamSource = env.createInput(createInputFormat(url,username,password,tablename));\nDataStream<String> streamJsonString = streamSource.map(row -> (String) row.getField(0));\nDataStream<ObjectNode> streamJson = streamJsonString.map(new MetricMapper()).map(string -> (ObjectNode) JsonUtil.fromJson(string));\n.....\n")])])]),t("p",[t("strong",[e._v("肉眼观察：")]),e._v("\n300万数据量以下很快跑完,500万跑到快结束的时候迅速变慢,更多的数据跑到后面会挂掉，注意不是一下挂掉，而是跑着跑着挂掉")]),e._v(" "),t("p",[t("strong",[e._v("metric指标：")])]),e._v(" "),t("p",[e._v("flink_taskmanager_job_task_operator_numRecordsOutPerSecond迅速变小；\nflink_taskmanager_Status_JVM_Memory_Heap_Max-flink_taskmanager_Status_JVM_Memory_Heap_Used 负责跑job的taskmanager只剩下几十M的容量，然后开始疯狂的gc，而且是从年轻代的minor gc升级为老年代的full gc")]),e._v(" "),t("p",[e._v("delta(flink_taskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Time[30s])\ndelta(flink_taskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Count[30s])")]),e._v(" "),t("p",[t("strong",[e._v("接着尝试监控flink jvm 发生full gc前 生成内存快照文件：")]),e._v(" "),t("a",{attrs:{href:"https://nightlies.apache.org/flink/flink-docs-release-1.12/deployment/config.html#jvm-and-logging-options",target:"_blank",rel:"noopener noreferrer"}},[e._v("修改flink.conf"),t("OutboundLink")],1),e._v(":")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("#(Java options to start the JVM of the TaskManager with.)\nenv.java.opts.taskmanager: -XX:+HeapDumpBeforeFullGC -XX:HeapDumpPath=保存dump文件的文件绝对路径 \t\n")])])]),t("p",[t("strong",[e._v("visualvm分析")]),e._v("\n发现 Dominators by Retained Size(The retained size for an object is the quantity of memory this objects preserves from garbage collection):\n下面这个对象实例占用了95%的内存，并且可以清晰的可以看到其中 resultset 从数据库拉回的所有数据的大小和size\norg.apache.flink.streaming.api.functions.source.InputFormatSourceFunction#1 [GC root - Java frame]")]),e._v(" "),t("p",[t("strong",[e._v("flink分析")]),e._v("\n既然已经定位到了问题所在")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('DataStream<Row> streamSource = env.createInput(createInputFormat(url,username,password,tablename));\n=>\npublic static JDBCInputFormat createInputFormat(String url, String username, String password, String tablename) {\n\n        return JDBCInputFormat.buildJDBCInputFormat()\n                .setDrivername("org.postgresql.Driver")\n                .setDBUrl(url)\n                .setUsername(username)\n                .setPassword(password)\n                .setQuery(String.format("SELECT CAST(info AS TEXT) FROM %s ORDER BY id;", tablename))\n                .setRowTypeInfo(new RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO))\n                .setFetchSize(100)\n                .finish();\n    }\nprivate <OUT> DataStreamSource<OUT> createInput(InputFormat<OUT, ?> inputFormat,\n\t\t\t\t\t\t\t\t\t\t\t\t\tTypeInformation<OUT> typeInfo,\n\t\t\t\t\t\t\t\t\t\t\t\t\tString sourceName) {\n\n\t\tInputFormatSourceFunction<OUT> function = new InputFormatSourceFunction<>(inputFormat, typeInfo);\n\t\treturn addSource(function, sourceName, typeInfo);\n\t}\n')])])]),t("p",[e._v("所以"),t("s",[e._v("flink的这个source")]),e._v("jdbc不断的从db捞数据，一次100条，但是永不释放，全存在了resultset里面，这样早晚也得爆啊，为什么flink这么设计？肯定是我们用错了，仔细的对应查找了flink的data stream api，结果就是没找到这个InputFormatSourceFunction，版本flink1.12，终于在dataset api下面才找到，并且看到warning：")]),e._v(" "),t("blockquote",[t("p",[e._v("Starting with Flink 1.12 the DataSet API has been soft deprecated.")])]),e._v(" "),t("blockquote",[t("p",[e._v("We recommend that you use the Table API and SQL to run efficient batch pipelines in a fully unified API. Table API is well integrated with common batch connectors and catalogs.")])]),e._v(" "),t("blockquote",[t("p",[e._v("Alternatively, you can also use the DataStream API with BATCH execution mode. The linked section also outlines cases where it makes sense to use the DataSet API but those cases will become rarer as development progresses and the DataSet API will eventually be removed. Please also see FLIP-131 for background information on this decision.")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158866741#/",target:"_blank",rel:"noopener noreferrer"}},[e._v("原因是 too many api？"),t("OutboundLink")],1),e._v(",不过感觉这里跟我们的问题相关：")]),e._v(" "),t("ul",[t("li",[e._v("DataSet API: All-or-nothing output: a job either produces data or it doesn't")]),e._v(" "),t("li",[e._v('DataStream API: "Incremental" output, based on watermarks or checkpoints')])]),e._v(" "),t("h4",{attrs:{id:"解决方案"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解决方案"}},[e._v("#")]),e._v(" 解决方案")]),e._v(" "),t("p",[e._v("总之现在的问题变成：")]),e._v(" "),t("h5",{attrs:{id:"_1-为什么-dataset-api读取数据库的数据一直会在resultset里面hold住所有的数据-不管下游是否处理了都一点不释放"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-为什么-dataset-api读取数据库的数据一直会在resultset里面hold住所有的数据-不管下游是否处理了都一点不释放"}},[e._v("#")]),e._v(" 1.为什么 dataset api读取数据库的数据一直会在resultset里面hold住所有的数据，不管下游是否处理了都一点不释放？")]),e._v(" "),t("p",[e._v("经过测试发现个有意思的现象，如果把setQuery的where条件每次改变，比如自己维护一个id的offset（startid,endid)，这样反而能够正常跑完数据，猜测jdbc每次执行的query不同，所以会垃圾回收前一个query对应的resultset")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('import org.apache.flink.api.common.functions.RichMapFunction;\nimport org.apache.flink.api.java.ExecutionEnvironment;\nimport org.apache.flink.api.java.io.jdbc.JDBCInputFormat;\nimport org.apache.flink.api.java.tuple.Tuple2;\nimport org.apache.flink.api.java.typeutils.RowTypeInfo;\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.types.Row;\n\npublic class FlinkJDBCExample {\n\n    public static void main(String[] args) throws Exception {\n        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n        // Configure JDBCInputFormat\n        String driverName = "org.postgresql.Driver";\n        String dbUrl = "jdbc:postgresql://localhost:5432/mydatabase";\n        String queryTemplate = "SELECT id, name FROM my_table WHERE id >= ? AND id < ?";\n        String username = "username";\n        String password = "password";\n        int fetchSize = 100; // Set fetch size\n        int batchSize = 100; // Number of records per batch\n        int startId = 0; // Initial start id\n\n        JDBCInputFormat jdbcInputFormat = JDBCInputFormat.buildJDBCInputFormat()\n                .setDrivername(driverName)\n                .setDBUrl(dbUrl)\n                .setQuery(queryTemplate)\n                .setUsername(username)\n                .setPassword(password)\n                .setRowTypeInfo(new RowTypeInfo(org.apache.flink.api.common.typeinfo.Types.INT, org.apache.flink.api.common.typeinfo.Types.STRING))\n                .setFetchSize(fetchSize)\n                .setParametersProvider(new DynamicParameterProvider(batchSize, startId))\n                .finish();\n\n        // Create input DataSet using JDBCInputFormat\n        env.createInput(jdbcInputFormat)\n                .map(new RichMapFunction<Row, Tuple2<Integer, String>>() {\n                    private int batchSize;\n                    private int startId;\n\n                    @Override\n                    public void open(Configuration parameters) throws Exception {\n                        super.open(parameters);\n                        this.batchSize = getRuntimeContext().getExecutionConfig().getGlobalJobParameters().getInt("batchSize", 100);\n                        this.startId = getRuntimeContext().getExecutionConfig().getGlobalJobParameters().getInt("startId", 0);\n                    }\n\n                    @Override\n                    public Tuple2<Integer, String> map(Row row) throws Exception {\n                        // Map Row to Tuple2\n                        return Tuple2.of((Integer) row.getField(0), (String) row.getField(1));\n                    }\n                })\n                .print(); // Example: Print the result\n\n        // Execute the Flink job\n        env.execute("Flink JDBC Example");\n    }\n\n    public static class DynamicParameterProvider implements JDBCInputFormat.ParameterProvider {\n        private final int batchSize;\n        private int startId;\n\n        public DynamicParameterProvider(int batchSize, int startId) {\n            this.batchSize = batchSize;\n            this.startId = startId;\n        }\n\n        @Override\n        public Object[][] getParameters() {\n            int endId = startId + batchSize;\n            Object[][] params = new Object[][] { { startId }, { endId } };\n            startId = endId; // Update startId for the next batch\n            return params;\n        }\n    }\n}\n\n')])])]),t("p",[e._v("但是注意，这么写会引入另一个潜在问题：当我们提供了ParameterValuesProvider,会触发jdbc的并行模式，导致从数据库分批次拿的数据不是顺序执行！如果数据前后有依赖会造成问题")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('/**\n\t * Connects to the source database and executes the query in a <b>parallel\n\t * fashion</b> if\n\t * this {@link InputFormat} is built using a parameterized query (i.e. using\n\t * a {@link PreparedStatement})\n\t * and a proper {@link ParameterValuesProvider}, in a <b>non-parallel\n\t * fashion</b> otherwise.\n\t *\n\t * @param inputSplit which is ignored if this InputFormat is executed as a\n\t *        non-parallel source,\n\t *        a "hook" to the query parameters otherwise (using its\n\t *        <i>splitNumber</i>)\n\t * @throws IOException if there\'s an error during the execution of the query\n\t */\n\t@Override\n\tpublic void open(InputSplit inputSplit) throws IOException {\n')])])]),t("p",[e._v("调用栈：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:123)\nat org.apache.flink.api.java.DataSet.collect(DataSet.java:456)\nat org.apache.flink.api.java.DataSet.output(DataSet.java:789)\nat org.apache.flink.connector.jdbc.JdbcInputFormat.open(JdbcInputFormat.java:101)\nat org.apache.flink.api.common.functions.RichFunction.open(RichFunction.java:95)\nat org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:117)\nat org.apache.flink.runtime.taskmanager.Task.run(Task.java:705)\n")])])]),t("h5",{attrs:{id:"_2-那么datastream-api和table-api-呢"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-那么datastream-api和table-api-呢"}},[e._v("#")]),e._v(" 2.那么datastream api和table api 呢")]),e._v(" "),t("p",[e._v("When executing a SQL query, conventional database systems and query engines read and process a data set, which is completely available, and produce a fixed sized result. In contrast, data streams continuously provide new records such that data arrives over time. Hence, streaming queries have to continuously process the arriving data and never “complete”.")]),e._v(" "),t("p",[e._v("API演进：\n"),t("strong",[e._v("The JdbcRowInputFormat in Apache Flink was introduced in version 1.11.0：")]),e._v("\nthe JdbcRowInputFormat to read data from PostgreSQL in a streaming fashion, which avoids loading the entire dataset into memory at once.")]),e._v(" "),t("p",[t("strong",[e._v("However, JdbcRowInputFormat  was removed in subsequent versions like Apache Flink 1.12.0：")]),e._v("\nSimplification and Consolidation: Flink's ecosystem and API have undergone continuous improvements to streamline and simplify usage patterns. In many cases, having multiple ways to achieve similar functionality can lead to confusion and maintenance overhead. Thus, the decision might have been made to consolidate and standardize the approach to reading from JDBC sources.")]),e._v(" "),t("p",[e._v("API Evolution: As Flink evolves, APIs and functionalities sometimes undergo changes to better align with the overall architecture and user needs. The removal of JdbcRowInputFormat could be part of such an evolution, where newer or more integrated solutions (like the JDBC connector via Table API or SQL) are preferred.")]),e._v(" "),t("p",[e._v("Community and Usage Patterns: Feedback from the Flink community and users often plays a crucial role in shaping the direction of the framework. If a feature is found to be underutilized or if better alternatives are available, it may be deprecated or removed to focus development efforts on more impactful features.")]),e._v(" "),t("p",[e._v("Extension and Compatibility: In later versions of Flink (post 1.11.0), JDBC connectivity is still supported but through separate extensions or connectors (like flink-jdbc-connector). This modular approach allows users to include only the dependencies they need, reducing the overall footprint and complexity.")]),e._v(" "),t("h6",{attrs:{id:"_2-1-flink-server-version-1-11-可以使用-jdbcrowinputformat-但是1-12后被废弃"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-flink-server-version-1-11-可以使用-jdbcrowinputformat-但是1-12后被废弃"}},[e._v("#")]),e._v(" 2.1. flink server version 1.11.* 可以使用 JdbcRowInputFormat，但是1.12后被废弃")]),e._v(" "),t("h6",{attrs:{id:"_2-2-flink-server-version-1-12-可以使用-sourcefunction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-flink-server-version-1-12-可以使用-sourcefunction"}},[e._v("#")]),e._v(" 2.2. flink server version 1.12.* 可以使用 SourceFunction")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('public class FlinkJdbcExample {\n\n    public static void main(String[] args) throws Exception {\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        //env.setRuntimeMode(RuntimeMode.BATCH);\n\n        // Add JDBC source function\n        env.addSource(new JdbcSourceFunction())\n           .print();\n\n        // Execute the program\n        env.execute("Flink JDBC Example");\n    }\n\n    public static class JdbcSourceFunction implements SourceFunction<Row> {\n    private volatile boolean isRunning = true;\n    private transient Connection connection;\n    private transient PreparedStatement preparedStatement;\n\n    @Override\n    public void run(SourceContext<Row> ctx) throws Exception {\n        connection = DriverManager.getConnection("jdbc:postgresql://localhost:5432/your_database", "your_username", "your_password");\n        preparedStatement = connection.prepareStatement("SELECT * FROM your_table");\n        preparedStatement.setFetchSize(fetchSize); // Set fetchSize\n\n        while (isRunning) {\n            ResultSet resultSet = preparedStatement.executeQuery();\n            while (resultSet.next()) {\n                // Convert ResultSet row to Flink Row object\n                Row row = Row.of(resultSet.getInt("id"), resultSet.getString("name"));\n                ctx.collect(row);\n            }\n            Thread.sleep(1000); // Example: Sleep for 1 second before querying again\n        }\n    }\n\n    @Override\n    public void cancel() {\n        isRunning = false;\n        try {\n            if (preparedStatement != null) {\n                preparedStatement.close();\n            }\n            if (connection != null) {\n                connection.close();\n            }\n        } catch (SQLException e) {\n            // Log or handle the exception\n        }\n    }\n}\n\nhow it work:\n\nIn Apache Flink, when using the JdbcSourceFunction or similar custom implementations to read from JDBC sources, the number of records fetched in one batch and the timing of querying the next batch are influenced by several factors and settings:\n\n1. Batch Size\nThe batch size, or the number of records fetched in one JDBC query execution, is typically determined by the database driver and configuration settings. This is not explicitly controlled by Flink\'s JdbcSourceFunction, but rather by how the JDBC driver interacts with the database.\n\nDefault Settings: The default batch size is controlled by the JDBC driver\'s implementation and can vary. Some drivers may fetch all results at once (especially for smaller result sets), while others may use a cursor to fetch a certain number of rows at a time.\n\nDatabase Configuration: You can configure the batch fetching behavior in some JDBC drivers using properties like fetchSize or through specific driver configurations. This can influence how many rows are fetched per round trip to the database.\n\n2. Timing of Querying Next Batch\nThe timing of when the JdbcSourceFunction queries the database for the next batch of data depends on your implementation:\n\nThread Sleep: In the example provided (Thread.sleep(1000);), after fetching a batch of records and emitting them into the Flink stream, the function sleeps for 1 second before executing the query again. This introduces a delay between successive queries to the database.\n\nBackpressure and Internal Strategy: Flink\'s runtime manages backpressure internally. When downstream operators (like transformations or sinks) cannot keep up with the rate of incoming data, Flink\'s streaming runtime applies backpressure. This means it slows down or pauses upstream sources (such as JdbcSourceFunction) to balance the flow of data through the pipeline.\n\nEffect on Querying Next Batch: When backpressure is applied, Flink\'s runtime will pause the execution of the JdbcSourceFunction (specifically, it won\'t call run() again) until downstream operators indicate readiness to process more data. This helps prevent overwhelming downstream operators and ensures efficient resource usage.\n')])])]),t("p",[e._v("注意：\n1.流处理该程序会一直执行不会终止，如果需要处理完数据就终止则必须设置条件主动终止")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('while (running) {\n                ResultSet resultSet = statement.executeQuery(query);\n\n                boolean hasRecords = false;\n                while (resultSet.next()) {\n                    hasRecords = true;\n                    MyDataType record = new MyDataType(\n                            resultSet.getString("column1"),\n                            resultSet.getInt("column2")\n                    );\n                    ctx.collect(record);\n                }\n\n                // If no records were found, exit the loop\n                if (!hasRecords) {\n                    break; // Exit after processing all records\n                }\n')])])]),t("ol",{attrs:{start:"2"}},[t("li",[t("p",[e._v("注意 虽然可以使用 env.setRuntimeMode(RuntimeMode.BATCH); 切换到batch模式，Flink will eventually terminate the job if no records are emitted and if there are no further tasks to process, but this behavior is not guaranteed. It may lead to an indefinite execution state if the loop doesn't exit correctly.")])]),e._v(" "),t("li",[t("p",[e._v("笔者还犯了个错误，由于flink server版本是1.11，而client端这边误用了1.12，虽然setFetchSize可以编译通过，但是1.11并不支持，所以提交任务之后，因为fetchsize失效，jdbc直接一次性将所有数据都load进来，导致oom")])])]),e._v(" "),t("p",[e._v("当然也可以喂给 table api")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('tableEnv.registerFunction("myFunction", new MyCustomTableFunction());\nTable resultTable = tableEnv.sqlQuery("SELECT * FROM MyTable, LATERAL TABLE(myFunction(...)) AS T(...) WHERE ...");\n')])])]),t("p",[e._v("或者干脆直接使用 table api")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('tableEnv.executeSql("CREATE TEMPORARY TABLE jdbc_table (\\n" +\n        "  id INT,\\n" +\n        "  name STRING\\n" +\n        ") WITH (\\n" +\n        "  \'connector\' = \'jdbc\',\\n" +\n        "  \'url\' = \'jdbc:mysql://localhost:3306/database\',\\n" +\n        "  \'table-name\' = \'your_table\',\\n" +\n        "  \'username\' = \'user\',\\n" +\n        "  \'password\' = \'password\'\\n" +\n        ")");\ntableEnv.sqlQuery("SELECT * FROM jdbc_table").toAppendStream(Row.class).print();\n')])])]),t("p",[e._v("然后再转成 datastream")]),e._v(" "),t("h6",{attrs:{id:"_2-3-flink-server-version-1-12-可以使用flink-jdbc-connector"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-flink-server-version-1-12-可以使用flink-jdbc-connector"}},[e._v("#")]),e._v(" 2.3. flink server version 1.12.* 可以使用flink-jdbc-connector")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('import org.apache.flink.table.api.TableSchema;\nimport org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\nimport org.apache.flink.table.sources.DefinedRowtimeAttributes;\nimport org.apache.flink.table.sources.RowtimeAttributeDescriptor;\nimport org.apache.flink.table.sources.TableSource;\nimport org.apache.flink.types.Row;\n\nimport java.util.Collections;\nimport java.util.List;\n\npublic class PostgreSQLTableSource implements TableSource<Row>, DefinedRowtimeAttributes {\n\n    private final String url = "jdbc:postgresql://localhost:5432/your_database";\n    private final String username = "your_username";\n    private final String password = "your_password";\n    private final String tableName = "your_table";\n\n    @Override\n    public DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {\n        JDBCOptions jdbcOptions = JDBCOptions.builder()\n                .setDBUrl(url)\n                .setTableName(tableName)\n                .setDriverName("org.postgresql.Driver")\n                .setUsername(username)\n                .setPassword(password)\n                .build();\n\n        return JDBCInputFormat.buildJDBCInputFormat()\n                .setOptions(jdbcOptions)\n                .setRowTypeInfo(...) // Set your RowTypeInfo according to table schema\n                .finish()\n                .createInput(execEnv);\n    }\n\n    @Override\n    public TableSchema getTableSchema() {\n        // Define your table schema based on the PostgreSQL table structure\n        return TableSchema.builder()\n                .field("field1", DataTypes.STRING())\n                .field("field2", DataTypes.INT())\n                .build();\n    }\n\n    @Override\n    public String explainSource() {\n        return "PostgreSQL Table Source";\n    }\n\n    @Override\n    public List<RowtimeAttributeDescriptor> getRowtimeAttributeDescriptors() {\n        // Return descriptors if your table has rowtime attributes\n        return Collections.emptyList();\n    }\n}\n\ntableEnv.registerTableSource("source_table", new PostgreSQLTableSource());\ntableEnv.sqlQuery("SELECT * FROM source_table").toAppendStream(Row.class).print();\n\n')])])]),t("h6",{attrs:{id:"_2-4-flink-server-version-1-11-强行使用sourcefunction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-flink-server-version-1-11-强行使用sourcefunction"}},[e._v("#")]),e._v(" 2.4 flink server version 1.11.* 强行使用sourcefunction")]),e._v(" "),t("p",[e._v("因为setFetchSize是1.12引入的，所以1.11需要自己控制每个批次要拿的size")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n    DataStream<String> streamJsonString = env.addSource(new SourceFunction<String>(){\n        private volatile boolean running = true;\n        private PGPoolConnection pgPoolConnection = new PGPoolConnection(config);\n        private Connection conn;\n        private PreparedStatement stmt;\n        private ResultSet rs;\n        private int windowSize = 1000000;\n        private int total = 0;\n        private int processedCount = 0;\n        private String tablename = config.getPostgresTable();\n\n        @Override\n        public void run(SourceContext<String> ctx) throws Exception {\n            pgPoolConnection.open();\n            conn = pgPoolConnection.getDataSource().getConnection();\n            stmt = conn.prepareStatement(String.format("SELECT count(*) AS total FROM %s", tablename));\n            stmt.setFetchSize(1);\n            rs = stmt.executeQuery();\n            if (rs.next()) {\n                total = rs.getInt("total");\n                System.out.println("Total:: " + total);\n            }\n            this.close();\n\n            while (running) {\n                pgPoolConnection.open();\n                conn = pgPoolConnection.getDataSource().getConnection();\n                stmt = conn.prepareStatement(String.format("SELECT CAST(info AS TEXT) AS info FROM %s ORDER BY id ASC LIMIT ? OFFSET ?", tablename));\n                stmt.setInt(1, windowSize);\n                stmt.setInt(2, processedCount);\n                rs = stmt.executeQuery();\n                while (running && rs.next()) {\n                    ctx.collect(rs.getString("info"));\n                    processedCount++;\n                }\n                System.out.println("PCount:: " + processedCount + (processedCount >= total));\n                if (processedCount >= total) {\n                    running = false;\n                }\n                this.close();\n            }\n            //FOR STREAMING MODE, CODE NEVER COME HERE UNTIL PROGRAM TERMINATED\n            this.terminated();\n        }\n\n        @Override\n        public void cancel() {\n            isRunning = false;\n            this.finish();\n        }\n\n        private void close() {\n            try {\n                if (rs != null) {\n                    rs.close();\n                }\n                if (stmt != null) {\n                    stmt.close();\n                }\n                if (conn != null) {\n                    conn.close();\n                }\n            } catch (Exception e) {\n                e.printStackTrace();\n            } finally {\n                rs = null;\n                stmt = null;\n                conn = null;\n            }\n        }\n\n        private void terminated() {\n            this.close();\n            try {\n                pgPoolConnection.close();\n            } catch (Exception e) {\n                e.printStackTrace();\n            } finally {\n                pgPoolConnection = null;\n            }\n        }\n    });\n')])])]),t("p",[t("a",{attrs:{href:"https://stackoverflow.com/questions/48151881/how-to-run-apache-flink-streaming-job-continuously-on-flink-server/78696018#/78696018",target:"_blank",rel:"noopener noreferrer"}},[e._v("我在stackoverflow的回答 How to run apache flink streaming job continuously on Flink server"),t("OutboundLink")],1)]),e._v(" "),t("hr"),e._v(" "),t("p",[e._v("flink自定义函数加线程锁 https://juejin.cn/s/flink%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E5%8A%A0%E7%BA%BF%E7%A8%8B%E9%94%81")]),e._v(" "),t("p",[e._v("使用Flink前必知的10个『陷阱』\nhttps://dbaplus.cn/news-73-3769-1.html")]),e._v(" "),t("p",[e._v("https://stackoverflow.com/questions/63668191/flink-workflow-parallelism-with-custom-source#/")]),e._v(" "),t("hr"),e._v(" "),t("p",[e._v("Refer:")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://www.infoq.cn/article/ZmL7TCcEchvANY-9jG1H",target:"_blank",rel:"noopener noreferrer"}},[e._v("Flink Slot 详解与 Job Execution Graph 优化"),t("OutboundLink")],1),e._v(" "),t("disqus")],1),e._v(" "),t("p",[t("a",{attrs:{href:"https://cloud.tencent.com/developer/article/2170959",target:"_blank",rel:"noopener noreferrer"}},[e._v("Flink中: 你的Function是如何被执行的"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://mp.weixin.qq.com/s/GDKlZtLqx6xR4wxBnteRnQ",target:"_blank",rel:"noopener noreferrer"}},[e._v("如何实现 Flink 读写数据到 Redis ？"),t("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=r.exports}}]);